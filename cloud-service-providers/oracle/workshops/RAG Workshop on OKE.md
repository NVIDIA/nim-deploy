# Deploy RAG Application Using NVIDIA NIM and NeMo Retriever on Oracle Kubernetes Engine (OKE) Workshop

## Table of Contents

- [Introduction](#introduction)
- [What You Will Learn](#what-you-will-learn)
- [Learn the Components](#learn-the-components)
- [Setup and Requirements](#setup-and-requirements)
- [Task 1. Create OKE Cluster](#task-1-create-oke-cluster)
- [Task 2. Configure Cluster Access](#task-2-configure-cluster-access)
- [Task 3. Configure NVIDIA NGC API Key](#task-3-configure-nvidia-ngc-api-key)
- [Task 4. Deploy RAG Blueprint](#task-4-deploy-rag-blueprint)
- [Task 5. Monitor Deployment](#task-5-monitor-deployment)
- [Task 6. Access the RAG Playground](#task-6-access-the-rag-playground)
- [Congratulations!](#congratulations)
- [Troubleshooting](#troubleshooting)
- [Cleanup](#cleanup)
- [Learn More](#learn-more)

## Introduction

This workshop will guide you through deploying a Retrieval Augmented Generation (RAG) application on Oracle Kubernetes Engine (OKE). You'll leverage the power of NVIDIA Inference Microservices (NIMs) and NeMo Retriever to build a robust document question-answering system. This system uses Milvus as the vector store to manage embeddings and generate accurate responses to user queries.

RAG is a cutting-edge approach in natural language processing that boosts the capabilities of large language models (LLM) by incorporating retrieval mechanisms, allowing models to access and utilize external data sources for more accurate and contextually relevant responses.

This workshop is ideal for developers and data scientists interested in:

- **Building RAG applications**: Learn how to construct a complete RAG pipeline using NVIDIA's pre-built microservices and open-source tools.
- **Optimizing LLM inference**: Explore how to deploy and utilize TensorRT optimized LLMs for efficient inference within a microservice architecture.
- **Leveraging vector databases**: Understand how to use Milvus to store and query embeddings for semantic search in a RAG workflow.

## What You Will Learn

By the end of this workshop, you will have hands-on experience with:

1. **Deploying a RAG pipeline on OKE**: Learn to deploy a complete RAG pipeline, including LLM, embedding, and retriever microservices, onto your OKE cluster using NVIDIA NIMs.
2. **Integrating with Milvus vector database**: Understand how to connect your RAG pipeline to a Milvus vector store for efficient storage and retrieval of embeddings.
3. **Testing document ingestion and Q&A**: Upload documents and ask questions through the RAG Playground interface.
4. **Managing your RAG deployment**: Explore techniques for managing and monitoring your RAG pipeline using Kubernetes.

## Learn the Components

### GPUs in Oracle Kubernetes Engine (OKE)

GPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. OKE provides a range of GPU shapes for node configuration, including bare metal instances with NVIDIA H100 and A100 GPUs.

| Shape | GPUs | GPU Memory |
|-------|------|------------|
| BM.GPU.H100.8 | 8x H100 | 640 GB |
| BM.GPU.A100-v2.8 | 8x A100 | 640 GB |
| BM.GPU4.8 | 8x A100 | 320 GB |

### NVIDIA NIMs

[NVIDIA NIMs](https://www.nvidia.com/en-us/ai/) are a set of easy-to-use inference microservices for accelerating the deployment of foundation models on any cloud or data center. NIMs provide:

- Pre-optimized containers for popular AI models
- TensorRT acceleration for maximum performance
- OpenAI-compatible APIs for easy integration
- Enterprise-grade security and support

### NVIDIA NeMo Retriever Microservice

[NVIDIA NeMo Retriever](https://developer.nvidia.com/nemo-microservices), part of NVIDIA NeMo, is a collection of generative AI microservices that enable organizations to seamlessly connect custom models to diverse business data and deliver highly accurate responses. It includes:

- **Embedding NIM**: Converts text to vector embeddings for semantic search
- **Reranking NIM**: Improves search accuracy by reranking retrieved results

### Vector Database (Milvus)

The RAG application leverages [Milvus](https://milvus.io/) as the vector store. Milvus stores embeddings generated from your documents and enables fast similarity search when users ask questions. The retrieved context is then injected into the LLM prompt for more accurate responses.

### RAG Playground

The RAG Playground provides a user interface for:

- Uploading documents (PDFs with text, tables, and charts)
- Asking questions about uploaded content
- Viewing responses generated by the RAG pipeline

## Setup and Requirements

### What You Need

To complete this workshop, you need:

- **OCI Account** with access to GPU instances (H100 or A100)
- **OCI CLI** installed and configured
- **kubectl** command-line tool
- **Helm 3.x** package manager
- **NVIDIA NGC Account** for an NGC API Key - [Sign up here](https://ngc.nvidia.com/setup/api-key)
- Sufficient OCI quota for GPU bare metal instances

### GPU Requirements

| Configuration | H100 80GB | A100 80GB |
|---------------|-----------|-----------|
| Text Only (this workshop) | 4 | 5 |
| Full Multimodal | 8 | 9 |

> **Note**: This workshop uses the Text Only configuration (4 GPUs on H100, 5 GPUs on A100) which supports text extraction from PDFs. The Full Multimodal configuration adds support for tables, charts, and images.

### IAM Policy Requirements

Ensure your user/group has the following OCI permissions:

```
Allow group <GROUP_NAME> to manage instance-family in compartment <COMPARTMENT_NAME>
Allow group <GROUP_NAME> to manage cluster-family in compartment <COMPARTMENT_NAME>
Allow group <GROUP_NAME> to manage virtual-network-family in compartment <COMPARTMENT_NAME>
```

## Task 1. Create OKE Cluster

In this task, you'll create an OKE cluster with GPU nodes using the OCI Console Quick Create feature.

1. Navigate to **OCI Console** → **Developer Services** → **Kubernetes Clusters (OKE)**

2. Click **Create cluster** → Select **Quick create** → Click **Submit**

3. Configure the cluster with the following settings:
   - **Name**: `rag-workshop`
   - **Kubernetes API endpoint**: Select **Public endpoint**
   - **Node type**: Select **Managed**
   - **Shape**: Select `BM.GPU.H100.8` or `BM.GPU.A100-v2.8`
   - **Number of nodes**: `1`
   - **Boot volume size**: `500` GB

4. Click **Create cluster**

5. Wait for the cluster to reach **Active** state (approximately 10-15 minutes)

   You can monitor the cluster creation progress in the OCI Console. The cluster will go through several states: Creating → Active.

## Task 2. Configure Cluster Access

Once your cluster is active, configure kubectl to access it.

1. **Get your cluster OCID** from the OCI Console (click on the cluster name and copy the OCID)

2. **Set environment variables**:

   ```bash
   export CLUSTER_ID="<your-cluster-ocid>"
   export REGION="<your-region>"  # e.g., us-ashburn-1
   ```

3. **Generate kubeconfig**:

   ```bash
   oci ce cluster create-kubeconfig --cluster-id $CLUSTER_ID --region $REGION \
     --file $HOME/.kube/config --token-version 2.0.0 --kube-endpoint PUBLIC_ENDPOINT
   ```

4. **Verify cluster access**:

   ```bash
   kubectl get nodes
   ```

   You should see output similar to:

   ```
   NAME            STATUS   ROLES   AGE   VERSION
   10.0.10.xxx     Ready    node    10m   v1.28.2
   ```

5. **Verify GPU availability**:

   ```bash
   kubectl describe nodes | grep -A5 "Allocatable:" | grep gpu
   ```

   Expected output:

   ```
     nvidia.com/gpu:     8
   ```

## Task 3. Configure NVIDIA NGC API Key

The NVIDIA NGC API key is required to pull container images and model artifacts from NGC.

1. **Export your NGC API key**:

   ```bash
   export NGC_API_KEY="<your-ngc-api-key>"
   ```

   > **Note**: If you don't have an NGC API key, create one at [ngc.nvidia.com/setup/api-key](https://ngc.nvidia.com/setup/api-key)

2. **Remove GPU taints** (allows non-GPU pods to schedule):

   ```bash
   kubectl taint nodes --all nvidia.com/gpu:NoSchedule- 2>/dev/null || true
   ```

   Expected output:

   ```
   node/10.0.10.xxx untainted
   ```

3. **Add NVIDIA Blueprint Helm repository**:

   ```bash
   helm repo add nvidia-blueprint https://helm.ngc.nvidia.com/nvidia/blueprint \
     --username='$oauthtoken' --password=$NGC_API_KEY
   helm repo update
   ```

   Expected output:

   ```
   "nvidia-blueprint" has been added to your repositories
   Update Complete. ⎈Happy Helming!⎈
   ```

## Task 4. Deploy RAG Blueprint

The NVIDIA RAG Blueprint bundles all the necessary components (LLM, Embedding, Reranking, Milvus, RAG Server, Frontend) into a single Helm chart for easy deployment.

1. **Deploy the RAG Blueprint**:

   ```bash
   helm install rag nvidia-blueprint/nvidia-blueprint-rag \
     --namespace rag --create-namespace \
     --set imagePullSecret.password=$NGC_API_KEY \
     --set ngcApiSecret.password=$NGC_API_KEY \
     --set nv-ingest.milvus.image.all.repository=docker.io/milvusdb/milvus \
     --set nv-ingest.milvus.image.tools.repository=docker.io/milvusdb/milvus-config-tool \
     --set nv-ingest.milvus.minio.image.repository=docker.io/minio/minio \
     --set nv-ingest.nemoretriever-graphic-elements-v1.deployed=false \
     --set nv-ingest.nemoretriever-table-structure-v1.deployed=false \
     --set nv-ingest.paddleocr-nim.deployed=false \
     --set frontend.service.type=LoadBalancer
   ```

   > **Note**: The `docker.io/` prefix is required on OKE because CRI-O enforces fully qualified image names.

   Expected output:

   ```
   NAME: rag
   LAST DEPLOYED: Mon Feb  3 10:00:00 2026
   NAMESPACE: rag
   STATUS: deployed
   REVISION: 1
   ```

2. **Understand what was deployed**:

   The Helm chart deploys the following components:

   | Component | Purpose | GPUs |
   |-----------|---------|------|
   | `rag-nim-llm-0` | Nemotron Super 49B LLM for text generation | 1 (H100) / 2 (A100) |
   | `rag-nvidia-nim-*-embedqa-*` | Embedding model for semantic search | 1 |
   | `rag-nvidia-nim-*-rerankqa-*` | Reranker for improved accuracy | 1 |
   | `rag-nemoretriever-page-elements-*` | Page layout analysis | 1 |
   | `milvus-standalone-*` | Vector database | 0 |
   | `rag-server-*` | RAG API server | 0 |
   | `rag-frontend-*` | RAG Playground UI | 0 |

## Task 5. Monitor Deployment

The deployment takes 10-15 minutes as the LLM model needs to be downloaded.

1. **Watch pod status**:

   ```bash
   kubectl get pods -n rag -w
   ```

   Initial state (pods starting):

   ```
   NAME                                           READY   STATUS              RESTARTS   AGE
   rag-nim-llm-0                                  0/1     Init:0/1            0          2m
   milvus-standalone-7d8bb68445-xxxxx             0/1     ContainerCreating   0          2m
   rag-frontend-7b9c8d7f56-xxxxx                  0/1     ContainerCreating   0          2m
   ```

   Press `Ctrl+C` to exit the watch.

2. **Check LLM model download progress** (this is the longest step):

   ```bash
   kubectl logs -n rag rag-nim-llm-0 --tail=20
   ```

   You'll see download progress:

   ```
   Downloading model files...
   Progress: 45%
   ```

   When ready:

   ```
   INFO: Started server process
   INFO: Uvicorn running on http://0.0.0.0:8000
   ```

3. **Verify all pods are running** (after 10-15 minutes):

   ```bash
   kubectl get pods -n rag
   ```

   Expected output (all pods should be `Running` with `1/1` or `2/2` READY):

   ```
   NAME                                           READY   STATUS    RESTARTS   AGE
   rag-nim-llm-0                                  1/1     Running   0          15m
   milvus-standalone-7d8bb68445-xxxxx             1/1     Running   0          15m
   rag-frontend-7b9c8d7f56-xxxxx                  1/1     Running   0          15m
   rag-server-xxxxx                               1/1     Running   0          15m
   ingestor-server-xxxxx                          1/1     Running   0          15m
   rag-nvidia-nim-nv-embedqa-xxxxx                1/1     Running   0          15m
   rag-nvidia-nim-nv-rerankqa-xxxxx               1/1     Running   0          15m
   rag-nemoretriever-page-elements-xxxxx          1/1     Running   0          15m
   ```

## Task 6. Access the RAG Playground

The RAG Playground provides a web interface to test the RAG pipeline.

1. **Get the frontend LoadBalancer IP**:

   ```bash
   kubectl get svc rag-frontend -n rag -w
   ```

   Wait until `EXTERNAL-IP` changes from `<pending>` to an IP address (1-2 minutes):

   ```
   NAME           TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)          AGE
   rag-frontend   LoadBalancer   10.96.xxx.xx   129.xxx.xxx.xx   3000:31234/TCP   5m
   ```

   Press `Ctrl+C` to exit.

2. **Get the RAG Playground URL**:

   ```bash
   echo "RAG Playground: http://$(kubectl get svc rag-frontend -n rag -o jsonpath='{.status.loadBalancer.ingress[0].ip}'):3000"
   ```

3. **Open the RAG Playground** in your browser using the URL from the previous command.

4. **Test the RAG Pipeline**:

   a. **Upload a document**: Click **Upload** and select a PDF document
   
   b. **Wait for processing**: The document will be ingested and converted to embeddings
   
   c. **Ask a question**: Type a question about the document content
   
   d. **View the response**: The RAG pipeline will retrieve relevant context and generate an answer

## Congratulations!

You've successfully deployed a RAG text Q&A application on OKE using NVIDIA Inference Microservices!

**What you accomplished**:

- Created an OKE cluster with GPU nodes
- Deployed the NVIDIA RAG Blueprint with all components
- Accessed the RAG Playground to test document Q&A

**For testing**, you can use the [NVIDIA CUDA C Programming Guide](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf) as a sample document to upload and test the RAG functionality.

**Next steps**:

- Try uploading different document types
- Explore the RAG API endpoints at `http://<EXTERNAL-IP>:8081/docs`
- Scale the deployment for increased performance

## Troubleshooting

### Pods Stuck in Pending

Check for GPU taints that may be blocking scheduling:

```bash
kubectl describe nodes | grep -i taint
```

Expected output (should show no taints):

```
Taints:             <none>
```

If taints exist, remove them:

```bash
kubectl taint nodes --all nvidia.com/gpu:NoSchedule- 2>/dev/null || true
```

Check GPU availability:

```bash
kubectl describe nodes | grep -A5 "Allocated resources:" | grep gpu
```

### ImageInspectError on Milvus/MinIO

This happens when images don't have the `docker.io/` prefix. Fix with:

```bash
helm upgrade rag nvidia-blueprint/nvidia-blueprint-rag -n rag --reuse-values \
  --set nv-ingest.milvus.image.all.repository=docker.io/milvusdb/milvus \
  --set nv-ingest.milvus.image.tools.repository=docker.io/milvusdb/milvus-config-tool \
  --set nv-ingest.milvus.minio.image.repository=docker.io/minio/minio
```

### Ingestor-server CrashLoopBackOff

Usually waiting for Milvus/MinIO to start. Check logs:

```bash
kubectl logs -n rag -l app=ingestor-server --tail=20
```

It will recover automatically once dependencies are ready. Wait 2-3 minutes.

### LLM Pod Not Ready

Check model download progress:

```bash
kubectl logs -n rag rag-nim-llm-0 --tail=20
```

The LLM downloads a ~100GB model. This can take 10-15 minutes depending on network speed.

### NGC Authentication Errors (ImagePullBackOff)

Verify your API key is set:

```bash
echo $NGC_API_KEY
```

Check the secret exists:

```bash
kubectl get secret -n rag | grep ngc
```

If missing, redeploy with the correct API key:

```bash
helm upgrade rag nvidia-blueprint/nvidia-blueprint-rag -n rag --reuse-values \
  --set imagePullSecret.password=$NGC_API_KEY \
  --set ngcApiSecret.password=$NGC_API_KEY
```

### LoadBalancer Stuck in Pending

If the external IP stays `<pending>` for more than 5 minutes:

```bash
kubectl describe svc rag-frontend -n rag
```

Check for events indicating issues with the OCI load balancer provisioning.

---

## Cleanup

To avoid incurring further costs, clean up the resources when you're done.

1. **Delete the Helm release**:

   ```bash
   helm uninstall rag --namespace rag
   ```

2. **Delete persistent volume claims**:

   ```bash
   kubectl delete pvc -n rag --all
   ```

3. **Wait for volumes to detach**:

   ```bash
   echo "Waiting 60s for OCI block volumes to detach..."
   sleep 60
   ```

4. **Delete the namespace**:

   ```bash
   kubectl delete namespace rag
   ```

5. **Delete the OKE cluster** (optional - via OCI Console):
   
   Navigate to **OCI Console** → **Developer Services** → **Kubernetes Clusters** → Select your cluster → **Delete**

## Learn More

- [NVIDIA RAG Blueprint](https://github.com/NVIDIA-AI-Blueprints/rag)
- [NVIDIA NIMs](https://www.nvidia.com/en-us/ai/)
- [NVIDIA NeMo Retriever](https://developer.nvidia.com/nemo-microservices)
- [Oracle Kubernetes Engine (OKE)](https://www.oracle.com/cloud/cloud-native/container-engine-kubernetes/)
- [Milvus Vector Database](https://milvus.io/)
