{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying NIMs on AzureML using AzureML CLI\n",
    "\n",
    "**Prerequisites:**\n",
    "- AzureML account with minimum 24 vCPUs of NC Series A100 GPU provisioned.\n",
    "- Host machine (CPU only) to download NIM container and model. Needs to have atleast 50 GB + **your LLM model size** GB amount of disk space to store the container and model.\n",
    "- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)\n",
    "- [Azure ML extension](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-cli?view=azureml-api-2&tabs=public)\n",
    "- [NGC API Key](https://catalog.ngc.nvidia.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the configuration variables\n",
    "\n",
    "Example:\n",
    "subscription_id = \"xxxxxxxxxxxxxxxxxxxxxx\"  \n",
    "resource_group = \"nim-demo\"  \n",
    "workspace = \"nim-test\"  \n",
    "location = \"westeurope\"  # e.g., \"southcentralus\", \"westeurope\"  \n",
    "ngc_container = \"nvcr.io/nim/microsoft/phi-3-mini-4k-instruct:1.2.3\"  \n",
    "ngc_api_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"  \n",
    "email_address = \"vikalluru@nvidia.com\"  \n",
    "endpoint_name = \"phi3-nim-endpoint-aml-1\"  \n",
    "deployment_name = \"phi3-nim-deployment-aml-1\"  \n",
    "instance_type = \"Standard_NC24ads_A100_v4\"  \n",
    "acr_registry_name = \"nimdemocr\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AzureML Workspace and corresponding container registry related information\n",
    "subscription_id = \"<your-azure-subscription-id>\"\n",
    "resource_group = \"<your-resource-group>\"\n",
    "workspace = \"<your-azureml-workspace-name>\"\n",
    "location = \"<your-azureml-region>\"  # e.g., \"southcentralus\", \"westeurope\"\n",
    "ngc_api_key = \"<your-ngc-api-key>\"\n",
    "email_address = \"<your-email-address>\"\n",
    "ngc_container = \"nvcr.io/nim/microsoft/phi-3-mini-4k-instruct:1.2.3\"\n",
    "endpoint_name = \"phi3-nim-endpoint-aml-1\"\n",
    "deployment_name = \"phi3-nim-deployment-aml-1\"\n",
    "instance_type = \"Standard_NC24ads_A100_v4\"\n",
    "acr_registry_name = \"<your-azureml-registry-name>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(command, description=\"\", return_output=False):\n",
    "    try:\n",
    "        if return_output:\n",
    "            return subprocess.run(\n",
    "                command, \n",
    "                check=True, \n",
    "                capture_output=True,  # capture_output should be set to True directly\n",
    "                text=True\n",
    "            ).stdout.strip()            \n",
    "        else:\n",
    "            subprocess.run(command, check=True)\n",
    "            print(f\"\\033[92mSuccess: {description}\\033[0m \\n Command: {' '.join(command)}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\033[91mError: {description}\\033[0m \\n Command: {' '.join(command)}\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\"az\", \"login\", \"--use-device-code\"], \"Azure login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\n",
    "        \"az\", \"ml\", \"workspace\", \"create\",\n",
    "        \"--name\", workspace,\n",
    "        \"--resource-group\", resource_group,\n",
    "        \"--location\", location\n",
    "], \"AzureML workspace creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\n",
    "    \"az\", \"ml\", \"workspace\", \"update\",\n",
    "    \"--name\", workspace,\n",
    "    \"--resource-group\", resource_group,\n",
    "    \"--container-registry\", f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ContainerRegistry/registries/{acr_registry_name}\"\n",
    "    ,\"-u\",\n",
    "], \"Link Azure Container Registry to Azure ML Workspace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store NGC API KEY in Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 1) Add NGC API Key to workspace key vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_keyvault_uri = run_command(\n",
    "    [\"az\", \"ml\", \"workspace\", \"show\", \"--name\", workspace, \"--resource-group\", resource_group, \"--query\", \"key_vault\", \"-o\", \"tsv\"], \n",
    "    \"Fetching workspace keyvault URI\", \n",
    "    True\n",
    ")\n",
    "\n",
    "keyvault_name = workspace_keyvault_uri.split(\"/\")[-1]\n",
    "\n",
    "print(\"Workspace keyvault URI: \", workspace_keyvault_uri)\n",
    "print(\"Keyvault name: \", keyvault_name)\n",
    "\n",
    "# Assign role to allow access to the Key Vault\n",
    "run_command([\n",
    "    \"az\", \"role\", \"assignment\", \"create\",\n",
    "    \"--role\", \"Key Vault Secrets Officer\",\n",
    "    \"--assignee\", email_address,\n",
    "    \"--scope\", workspace_keyvault_uri\n",
    "], \"Role assignment to access key vault\")\n",
    "\n",
    "# Set a secret in the Key Vault\n",
    "run_command([\n",
    "    \"az\", \"keyvault\", \"secret\", \"set\",\n",
    "    \"--vault-name\", keyvault_name,\n",
    "    \"--name\", \"NGC-KEY\",\n",
    "    \"--value\", ngc_api_key\n",
    "], \"Add NGC secret to key vault\")\n",
    "\n",
    "# Show the secret in the Key Vault (for verification, if needed)\n",
    "run_command([\n",
    "    \"az\", \"keyvault\", \"secret\", \"show\",\n",
    "    \"--vault-name\", keyvault_name,\n",
    "    \"--name\", \"NGC-KEY\"\n",
    "], \"Verify NGC secret in key vault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 2) Add NGC API KEY as an AzureML workspace connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign role permission to read secrets from workspace connections\n",
    "run_command([\n",
    "    \"az\", \"role\", \"assignment\", \"create\",\n",
    "    \"--assignee\", email_address,\n",
    "    \"--role\", \"Azure Machine Learning Workspace Connection Secrets Reader\",\n",
    "    \"--scope\", f\"/subscriptions/{subscription_id}/resourcegroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace}\"\n",
    "], \"AzureML secrets reader role assignment\")\n",
    "\n",
    "# Get a personal access token for the workspace\n",
    "token = run_command([\"az\", \"account\", \"get-access-token\", \"--query\", \"accessToken\", \"-o\", \"tsv\"], \"Getting access token for workspace\")\n",
    "\n",
    "# Define URLs\n",
    "url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace}/connections/ngc?api-version=2023-08-01-preview\"\n",
    "verify_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace}/connections/ngc/listsecrets?api-version=2023-08-01-preview\"\n",
    "\n",
    "# Add a workspace connection to store NGC API key\n",
    "print(\"Adding NGC API key to workspace\")\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"properties\": {\n",
    "        \"authType\": \"CustomKeys\",\n",
    "        \"category\": \"CustomKeys\",\n",
    "        \"credentials\": {\n",
    "            \"keys\": {\n",
    "                \"NGC_API_KEY\": ngc_api_key\n",
    "            }\n",
    "        },\n",
    "        \"expiryTime\": None,\n",
    "        \"target\": \"_\",\n",
    "        \"isSharedToAll\": False,\n",
    "        \"sharedUserList\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.put(url, headers=headers, json=data)\n",
    "if response.status_code == 200:\n",
    "    print(\"NGC API key added to workspace successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to add NGC API key to workspace: {response.text}\")\n",
    "\n",
    "# Verify if the key got added\n",
    "print(\"Verifying if the NGC API key was added\")\n",
    "verify_response = requests.post(verify_url, headers=headers, json={})\n",
    "if verify_response.status_code == 200:\n",
    "    verify_result = verify_response.json()\n",
    "    ngc_api_key_value = verify_result.get(\"properties\", {}).get(\"credentials\", {}).get(\"keys\", {}).get(\"NGC_API_KEY\")\n",
    "    \n",
    "    if ngc_api_key_value == ngc_api_key:\n",
    "        print(\"The NGC_API_KEY value matches the provided key.\")\n",
    "    else:\n",
    "        print(\"The NGC_API_KEY value does not match the provided key.\")\n",
    "else:\n",
    "    print(f\"Failed to verify NGC API key: {verify_response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull NIM container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\"docker\", \"login\", \"nvcr.io\", \"-u\", \"$oauthtoken\", \"-p\", ngc_api_key], \"Docker login\")\n",
    "run_command([\"docker\", \"pull\", ngc_container], \"Pulling NIM container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Cache NIM model for airgapped deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all compatible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cache_path = \"<path-to-cache-model-repo>\"\n",
    "model_cache_path = \"/mnt/models\"\n",
    "\n",
    "# Run the Docker container and list model profiles inside it\n",
    "docker_run_command = [\n",
    "    \"docker\", \"run\", \"--rm\", \"--name=nim_list_profiles\",\n",
    "    \"-e\", \"LOG_LEVEL=info\",\n",
    "    \"-e\", f\"NGC_API_KEY={ngc_api_key}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    \"-v\", f\"{model_cache_path}:/model-repo\",\n",
    "    \"-u\", \"root\",\n",
    "    f\"{ngc_container}\",\n",
    "    \"bash\", \"-i\", \"-c\", \"list-model-profiles\"\n",
    "]\n",
    "\n",
    "# Execute the command to start Docker and list model profiles\n",
    "run_command(docker_run_command, \"Run Docker container to list model profiles\")\n",
    "\n",
    "# Parse compatible model profile IDs from output (assuming the format provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select model profile and download the required NIM model\n",
    "\n",
    "Select a profile from the compatible profiles from the previous output.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "===========================================\n",
    "== NVIDIA Inference Microservice LLM NIM ==\n",
    "===========================================\n",
    "...\n",
    "...\n",
    "...\n",
    "SYSTEM INFO\n",
    "- Free GPUs:\n",
    "  -  [20b5:10de] (0) NVIDIA A100 80GB PCIe [current utilization: 0%]\n",
    "MODEL PROFILES\n",
    "- Compatible with system and runnable:\n",
    "  - cc2e0f9cb33ad6f9d31f64c0c1188342b00f427569a62a46397dfa33a2db7695 (vllm-bf16-tp1)\n",
    "  - With LoRA support:\n",
    "- Incompatible with system:\n",
    "  - 94efad505d9248f0453e13c8a24420d5da4a909dd060c22904bc9fad923823b9 (tensorrt_llm-h100-fp8-tp1-throughput)\n",
    "...\n",
    "```\n",
    "selected_profile = \"cc2e0f9cb33ad6f9d31f64c0c1188342b00f427569a62a46397dfa33a2db7695\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a compatible profile ID for the model store creation\n",
    "selected_profile = \"cc2e0f9cb33ad6f9d31f64c0c1188342b00f427569a62a46397dfa33a2db7695\"\n",
    "print(f\"Selected compatible profile ID: {selected_profile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache_path=\"/home/azureuser/phi3-mini-4k-nim\"\n",
    "# Run the command to create model store for the chosen profile\n",
    "docker_create_model_command = [\n",
    "    \"docker\", \"run\", \"--rm\", \"--name=nim_model_store\",\n",
    "    \"-e\", \"LOG_LEVEL=info\",\n",
    "    \"-e\", f\"NGC_API_KEY={ngc_api_key}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    \"-v\", f\"{model_cache_path}:/model-repo\",\n",
    "    \"-u\", \"root\",\n",
    "    f\"{ngc_container}\",\n",
    "    \"bash\", \"-c\", f\"create-model-store --profile {selected_profile} --model-store /model-repo\"\n",
    "]\n",
    "\n",
    "print(\"Running model store creation command in Docker...\")\n",
    "run_command(docker_create_model_command, \"Create model store in Docker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the model was stored successfully\n",
    "run_command([\"ls\",model_cache_path], \"Model cache verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push downloaded model to your AzureML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache_path = \"/home/azureuser/phi3-mini-4k-nim\"\n",
    "cached_model_name = \"phi3-mini-4k-nim\"\n",
    "model_version = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\n",
    "    \"az\", \"ml\", \"model\", \"create\",\n",
    "    \"--name\", cached_model_name,\n",
    "    \"--version\", model_version,\n",
    "    \"--path\", model_cache_path,\n",
    "    \"--resource-group\", resource_group,\n",
    "    \"--workspace-name\", workspace\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push container to Azure Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nim_acr_name = ngc_container.replace(\"nvcr.io\", f\"{acr_registry_name}.azurecr.io\")\n",
    "print(\"NIM image name as saved in ACR: \", nim_acr_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile content for NIM container\n",
    "dockerfile_content = f\"\"\"FROM {ngc_container}\n",
    "EXPOSE 8000\n",
    "USER root\n",
    "CMD bash -c \"echo 'Displaying the NGC API Key:' && echo $NGC_API_KEY && \\\n",
    "             echo 'Displaying the NIM Model Name:' && echo $NIM_MODEL_NAME && \\\n",
    "             echo 'Listing the contents of /model-repo:' && ls /model-repo && \\\n",
    "             /opt/nim/start-server.sh\"\n",
    "\"\"\"\n",
    "\n",
    "# Write Dockerfile to disk\n",
    "with open(\"Dockerfile\", \"w\") as dockerfile:\n",
    "    dockerfile.write(dockerfile_content)\n",
    "print(\"NIM Dockerfile has been created.\")\n",
    "\n",
    "# Login to Azure Container Registry\n",
    "print(\"Logging into Azure Container Registry\")\n",
    "run_command([\"az\", \"acr\", \"login\", \"-n\", acr_registry_name])\n",
    "\n",
    "# Build and tag the Docker image\n",
    "print(\"Building the new Docker image and tagging it\")\n",
    "run_command([\"docker\", \"build\", \"-t\", nim_acr_name, \"-f\", \"Dockerfile\", \".\"], \"Building azure NIM image\")\n",
    "\n",
    "# Clean up Dockerfile after build\n",
    "os.remove(\"Dockerfile\")\n",
    "\n",
    "# Push the image to ACR\n",
    "print(\"Pushing the image to ACR\")\n",
    "run_command([\"docker\", \"push\", nim_acr_name], \"Pushing NIM image to Azure container registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AzureML managed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the endpoint YAML configuration\n",
    "endpoint_yaml_content = f\"\"\"$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\n",
    "name: {endpoint_name}\n",
    "auth_mode: key\n",
    "properties:\n",
    "    enforce_access_to_default_secret_stores: enabled  # default: disabled\n",
    "\"\"\"\n",
    "\n",
    "# Write endpoint YAML configuration to file\n",
    "with open(\"actual_endpoint_aml.yml\", \"w\") as endpoint_yaml:\n",
    "    endpoint_yaml.write(endpoint_yaml_content)\n",
    "print(\"Endpoint YAML configuration created.\")\n",
    "\n",
    "# Deploy the endpoint using the Azure CLI\n",
    "print(f\"Creating Online Endpoint {endpoint_name}\")\n",
    "output = run_command([\n",
    "    \"az\", \"ml\", \"online-endpoint\", \"create\", \"-f\", \"actual_endpoint_aml.yml\",\n",
    "    \"--resource-group\", resource_group, \"--workspace-name\", workspace\n",
    "], return_output=True)\n",
    "\n",
    "endpointidentityid = \"\"\n",
    "if output:\n",
    "    endpoint_data = json.loads(output)\n",
    "    endpointidentityid = endpoint_data.get(\"identity\", {}).get(\"principal_id\")\n",
    "    if endpointidentityid:\n",
    "        print(f\"Principal ID: {endpointidentityid}\")\n",
    "    else:\n",
    "        print(\"Principal ID not found.\")\n",
    "else:\n",
    "    print(\"No output received.\")\n",
    "\n",
    "# Clean up the generated YAML file\n",
    "os.remove(\"actual_endpoint_aml.yml\")\n",
    "print(\"Cleaned up temporary endpoint YAML file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide access to endpoint to read secrets from key vault\n",
    "endpointidentityid = \"<provide-your-endpoint-id>\"\n",
    "run_command([\n",
    "    \"az\", \"role\", \"assignment\", \"create\",\n",
    "    \"--assignee\", endpointidentityid,\n",
    "    \"--role\", \"Key Vault Secrets User\",\n",
    "    \"--scope\", f\"/subscriptions/{subscription_id}/resourcegroups/{resource_group}/providers/Microsoft.KeyVault/vaults/{keyvault_name}\"\n",
    "], \"Providing permissions for endpoint to access NGC key from key vault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NIM deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Normal deployment yaml file\n",
    "\n",
    "Run this cell to generate a deployment YAML file for deploying NIMs in a non-airgapped environment, utilizing the internet to fetch the NIM profile and models. The deployment instance requires the NGC API KEY to retrieve these models, which is provided by injecting it from our workspace’s Azure Key Vault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyvault_uri = \"${{keyvault:<provide-your-NGC-key-URI>}}\"\n",
    "# Ex: keyvault_uri = \"${{keyvault:https://keyvaultname.vault.azure.net/secrets/NGC-KEY/acbfcec9c19e4a2ab6b64197382a5ecb}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_yaml_content = f\"\"\"$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: {deployment_name}\n",
    "endpoint_name: {endpoint_name}\n",
    "environment: \n",
    "  name: nim-env\n",
    "  image: {nim_acr_name}\n",
    "  inference_config:\n",
    "      liveness_route:\n",
    "          path: /v1/health/ready\n",
    "          port: 8000\n",
    "      readiness_route:\n",
    "          path: /v1/health/ready\n",
    "          port: 8000\n",
    "      scoring_route:\n",
    "          path: /\n",
    "          port: 8000\n",
    "instance_type: {instance_type}\n",
    "instance_count: 1\n",
    "environment_variables:\n",
    "    NGC_API_KEY: {keyvault_uri}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Airgapped deployment yaml file\n",
    "\n",
    "Run this cell to generate a deployment YAML file for deploying NIMs in an airgapped environment. This deployment requires mounting a cached NIM model and setting the NIM container’s NIM_MODEL_NAME environment variable, which instructs NIM to avoid downloading from the internet and instead utilize the model hosted on the instance. The NIM model is sourced by mounting the previously pushed model from our AzureML workspace’s model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_yaml_content = f\"\"\"$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: {deployment_name}\n",
    "endpoint_name: {endpoint_name}\n",
    "model: azureml:{cached_model_name}:{model_version}\n",
    "model_mount_path: /model-repo\n",
    "environment: \n",
    "  name: nim-env\n",
    "  image: {nim_acr_name}\n",
    "  inference_config:\n",
    "      liveness_route:\n",
    "          path: /v1/health/ready\n",
    "          port: 8000\n",
    "      readiness_route:\n",
    "          path: /v1/health/ready\n",
    "          port: 8000\n",
    "      scoring_route:\n",
    "          path: /\n",
    "          port: 8000\n",
    "instance_type: {instance_type}\n",
    "instance_count: 1\n",
    "environment_variables:\n",
    "    NIM_MODEL_NAME: /model-repo/phi3-mini-4k-nim/\n",
    "    NGC_API_KEY: {keyvault_uri}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display content of deployment YAML file\n",
    "import yaml\n",
    "yaml_content = yaml.safe_load(deployment_yaml_content)\n",
    "print(yaml.dump(yaml_content, sort_keys=False, default_flow_style=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write deployment YAML configuration to file\n",
    "with open(\"actual_deployment_aml.yml\", \"w\") as deployment_yaml:\n",
    "    deployment_yaml.write(deployment_yaml_content)\n",
    "print(\"Deployment YAML configuration created.\")\n",
    "\n",
    "# Display the modified YAML file for confirmation\n",
    "with open(\"actual_deployment_aml.yml\", \"r\") as file:\n",
    "    print(file.read())\n",
    "\n",
    "# Create the online deployment using the Azure CLI\n",
    "print(f\"Creating Online Deployment {deployment_name}\")\n",
    "run_command([\n",
    "    \"az\", \"ml\", \"online-deployment\", \"create\", \"-f\", \"actual_deployment_aml.yml\",\n",
    "    \"--resource-group\", resource_group, \"--workspace-name\", workspace\n",
    "])\n",
    "\n",
    "# Clean up the generated YAML file\n",
    "os.remove(\"actual_deployment_aml.yml\")\n",
    "print(\"Cleaned up temporary deployment YAML file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your connection\n",
    "\n",
    "Verify your deployment using the code below. Modify the code to add your endpoint URL, Endpoint authorization token and AzureML deployment name obtained from \"Consume\" tab under your AzureML endpoint page as shown below.\n",
    "\n",
    "![Endpoint details](./endpoint_details.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at your deployment logs for available serving endpoints and example CURL request\n",
    "\n",
    "![Serving endpoints](./serving_endpoints.png)\n",
    "\n",
    "![Example request](./example_request.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://phi3-nim-endpoint-aml-1.westeurope.inference.ml.azure.com' # modify this URL\n",
    "token = '4lMW5uj9wQbbhxhAFBxFvY9Jhgn1UIoi'\n",
    "model_name = \"microsoft/phi-3-mini-4k-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url = urljoin(base_url, \"v1/chat/completions\")\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Authorization': f'Bearer {token}', # modify this token\n",
    "    'Content-Type': 'application/json',\n",
    "    'azureml-model-deployment': deployment_name\n",
    "}\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a polite and respectful chatbot helping people plan a vacation.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"What should I do for a 4 day vacation in Spain?\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": model_name,\n",
    "    \"max_tokens\": 16,\n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1,\n",
    "    \"stream\": False,\n",
    "    \"stop\": \"\\n\",\n",
    "    \"frequency_penalty\": 0.0\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "# Pretty print the JSON response\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Launch a Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Function to handle the chat with LLM\n",
    "def chat_with_llm(user_message, history):\n",
    "    # Format messages for the LLM with initial system prompt\n",
    "    formatted_messages = [\n",
    "        {\"content\": \"You are a polite and respectful AI assistant.\", \"role\": \"system\"}\n",
    "    ]\n",
    "    \n",
    "    # Add previous messages from the history\n",
    "    for user_msg, assistant_msg in history:\n",
    "        formatted_messages.append({\"content\": user_msg, \"role\": \"user\"})\n",
    "        formatted_messages.append({\"content\": assistant_msg, \"role\": \"assistant\"})\n",
    "\n",
    "    # Add the user's latest message\n",
    "    formatted_messages.append({\"content\": user_message, \"role\": \"user\"})\n",
    "\n",
    "    data = {\n",
    "        \"messages\": formatted_messages,\n",
    "        \"model\": model_name,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,  # Set to True if streaming responses are supported by your API\n",
    "        \"stop\": \"\\n\",\n",
    "        \"frequency_penalty\": 0.1\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        assistant_reply = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response\")\n",
    "    else:\n",
    "        assistant_reply = f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "    # Return the assistant's reply as a string\n",
    "    return assistant_reply\n",
    "\n",
    "# Create a ChatInterface using Gradio\n",
    "chat_interface = gr.ChatInterface(\n",
    "    fn=chat_with_llm,\n",
    "    title=\"Multi-Turn LLM Chatbot with gr.ChatInterface\",\n",
    "    description=\"A chatbot interface that interacts with your LLM endpoint for multi-turn conversations.\",\n",
    "    examples=[[\"What should I do for a 4-day vacation in Spain?\"]]\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "chat_interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
