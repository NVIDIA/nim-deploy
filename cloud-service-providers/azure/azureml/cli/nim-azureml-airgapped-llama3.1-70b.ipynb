{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a Demo Notebook of Deploying Air Gapped NIM LLama 3.1-70b with TensorRT profile on AzureML using AzureML CLI\n",
    "\n",
    "**Prerequisites:**\n",
    "- AzureML account with minimum 1-A100 GPU VM with 48 vCPUs (NC48ads_A100_v4) provisioned.\n",
    "- Host machine (CPU only) to download NIM container and model. Needs to have atleast 140 GB  amount of disk space to store the container and model.\n",
    "- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)\n",
    "- [Azure ML extension](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-configure-cli?view=azureml-api-2&tabs=public)\n",
    "- [NGC API Key](https://catalog.ngc.nvidia.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill the configuration variables\n",
    "\n",
    "subscription_id=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "resource_group=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "workspace=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "location=\"eastus\" # e.g., \"southcentralus\", \"westeurope\"\n",
    "keyvault_name=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "email_address = \"youremail@nvidia.com\"  \n",
    "ngc_api_key=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "instance_type=\"Standard_NC48ads_A100_v4\"\n",
    "acr_registry_name=\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "ngc_container = \"nvcr.io/nim/meta/llama-3.1-70b-instruct:1.3.3\"\n",
    "endpoint_name = \"llama31-70b-nim-trt\"\n",
    "deployment_name = \"llama31-70b-nim-trt-tp1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command(command, description=\"\", return_output=False):\n",
    "    try:\n",
    "        if return_output:\n",
    "            return subprocess.run(\n",
    "                command, \n",
    "                check=True, \n",
    "                capture_output=True,  # capture_output should be set to True directly\n",
    "                text=True\n",
    "            ).stdout.strip()            \n",
    "        else:\n",
    "            subprocess.run(command, check=True)\n",
    "            print(f\"\\033[92mSuccess: {description}\\033[0m \\n Command: {' '.join(command)}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"\\033[91mError: {description}\\033[0m \\n Command: {' '.join(command)}\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\"az\", \"login\", \"--use-device-code\"], \"Azure login\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\n",
    "        \"az\", \"ml\", \"workspace\", \"create\",\n",
    "        \"--name\", workspace,\n",
    "        \"--resource-group\", resource_group,\n",
    "        \"--location\", location\n",
    "], \"AzureML workspace creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\n",
    "    \"az\", \"ml\", \"workspace\", \"update\",\n",
    "    \"--name\", workspace,\n",
    "    \"--resource-group\", resource_group,\n",
    "    \"--container-registry\", f\"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.ContainerRegistry/registries/{acr_registry_name}\"\n",
    "    ,\"-u\",\n",
    "], \"Link Azure Container Registry to Azure ML Workspace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store NGC API KEY in Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 1) Add NGC API Key to workspace key vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_keyvault_uri = run_command(\n",
    "    [\"az\", \"ml\", \"workspace\", \"show\", \"--name\", workspace, \"--resource-group\", resource_group, \"--query\", \"key_vault\", \"-o\", \"tsv\"], \n",
    "    \"Fetching workspace keyvault URI\", \n",
    "    True\n",
    ")\n",
    "\n",
    "keyvault_name = workspace_keyvault_uri.split(\"/\")[-1]\n",
    "\n",
    "print(\"Workspace keyvault URI: \", workspace_keyvault_uri)\n",
    "print(\"Keyvault name: \", keyvault_name)\n",
    "\n",
    "# Assign role to allow access to the Key Vault\n",
    "run_command([\n",
    "    \"az\", \"role\", \"assignment\", \"create\",\n",
    "    \"--role\", \"Key Vault Secrets Officer\",\n",
    "    \"--assignee\", email_address,\n",
    "    \"--scope\", workspace_keyvault_uri\n",
    "], \"Role assignment to access key vault\")\n",
    "\n",
    "# Set a secret in the Key Vault\n",
    "run_command([\n",
    "    \"az\", \"keyvault\", \"secret\", \"set\",\n",
    "    \"--vault-name\", keyvault_name,\n",
    "    \"--name\", \"NGC-KEY\",\n",
    "    \"--value\", ngc_api_key\n",
    "], \"Add NGC secret to key vault\")\n",
    "\n",
    "# Show the secret in the Key Vault (for verification, if needed)\n",
    "run_command([\n",
    "    \"az\", \"keyvault\", \"secret\", \"show\",\n",
    "    \"--vault-name\", keyvault_name,\n",
    "    \"--name\", \"NGC-KEY\"\n",
    "], \"Verify NGC secret in key vault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option 2) Add NGC API KEY as an AzureML workspace connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign role permission to read secrets from workspace connections\n",
    "run_command([\n",
    "    \"az\", \"role\", \"assignment\", \"create\",\n",
    "    \"--assignee\", email_address,\n",
    "    \"--role\", \"Azure Machine Learning Workspace Connection Secrets Reader\",\n",
    "    \"--scope\", f\"/subscriptions/{subscription_id}/resourcegroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace}\"\n",
    "], \"AzureML secrets reader role assignment\")\n",
    "\n",
    "# Get a personal access token for the workspace\n",
    "token = run_command([\"az\", \"account\", \"get-access-token\", \"--query\", \"accessToken\", \"-o\", \"tsv\"], \"Getting access token for workspace\")\n",
    "\n",
    "# Define URLs\n",
    "url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace}/connections/ngc?api-version=2023-08-01-preview\"\n",
    "verify_url = f\"https://management.azure.com/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace}/connections/ngc/listsecrets?api-version=2023-08-01-preview\"\n",
    "\n",
    "# Add a workspace connection to store NGC API key\n",
    "print(\"Adding NGC API key to workspace\")\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"properties\": {\n",
    "        \"authType\": \"CustomKeys\",\n",
    "        \"category\": \"CustomKeys\",\n",
    "        \"credentials\": {\n",
    "            \"keys\": {\n",
    "                \"NGC_API_KEY\": ngc_api_key\n",
    "            }\n",
    "        },\n",
    "        \"expiryTime\": None,\n",
    "        \"target\": \"_\",\n",
    "        \"isSharedToAll\": False,\n",
    "        \"sharedUserList\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.put(url, headers=headers, json=data)\n",
    "if response.status_code == 200:\n",
    "    print(\"NGC API key added to workspace successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to add NGC API key to workspace: {response.text}\")\n",
    "\n",
    "# Verify if the key got added\n",
    "print(\"Verifying if the NGC API key was added\")\n",
    "verify_response = requests.post(verify_url, headers=headers, json={})\n",
    "if verify_response.status_code == 200:\n",
    "    verify_result = verify_response.json()\n",
    "    ngc_api_key_value = verify_result.get(\"properties\", {}).get(\"credentials\", {}).get(\"keys\", {}).get(\"NGC_API_KEY\")\n",
    "    \n",
    "    if ngc_api_key_value == ngc_api_key:\n",
    "        print(\"The NGC_API_KEY value matches the provided key.\")\n",
    "    else:\n",
    "        print(\"The NGC_API_KEY value does not match the provided key.\")\n",
    "else:\n",
    "    print(f\"Failed to verify NGC API key: {verify_response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull NIM container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\"docker\", \"login\", \"nvcr.io\", \"-u\", \"$oauthtoken\", \"-p\", ngc_api_key], \"Docker login\")\n",
    "run_command([\"docker\", \"pull\", ngc_container], \"Pulling NIM container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache NIM model for airgapped deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all compatible models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_cache_path = \"<path-to-cache-model-repo>\"\n",
    "model_cache_path=\"/home/azureuser/llama-3.1-70b-nim\"\n",
    "\n",
    "# Run the Docker container and list model profiles inside it\n",
    "docker_run_command = [\n",
    "    \"docker\", \"run\", \"--rm\", \"--name=nim_list_profiles\",\n",
    "    \"-e\", \"LOG_LEVEL=info\",\n",
    "    \"-e\", \"NIM_MANIFEST_ALLOW_UNSAFE\",\n",
    "    \"-e\", f\"NGC_API_KEY={ngc_api_key}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    \"-v\", f\"{model_cache_path}:/model-repo\",\n",
    "    \"-u\", \"root\",\n",
    "    f\"{ngc_container}\",\n",
    "    \"bash\", \"-i\", \"-c\", \"list-model-profiles\"\n",
    "]\n",
    "\n",
    "# Execute the command to start Docker and list model profiles\n",
    "run_command(docker_run_command, \"Run Docker container to list model profiles\")\n",
    "\n",
    "# Parse compatible model profile IDs from output (assuming the format provided)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select model profile and download the required NIM model\n",
    "\n",
    "Select a profile from the compatible profiles from the previous output.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "MODEL PROFILES\n",
    "- Compatible with system and runnable:\n",
    "  - 395082aa40085d35f004dd3056d7583aea330417ed509b4315099a66cfc72bdd (vllm-bf16-tp2)\n",
    "  - With LoRA support:\n",
    "- Compilable to TRT-LLM using just-in-time compilation of HF models to TRTLLM engines:\n",
    "  - b7b6fa584441d9536091ce5cf80ccc31765780b8a46540da4e7bada5c5108ed9 (tensorrt_llm-trtllm_buildable-bf16-tp2)\n",
    "  - With LoRA support:\n",
    "- Incompatible with system:\n",
    "  - a29dc20fff4ad67746205295ccb4af9e010f8f31207235c75e27786fb834e574 (tensorrt_llm-h100-fp8-tp8-pp1-latency)\n",
    "  - 852c2c07610526d83d0fa80c656c5ee32c54f91df2626b9e7f7dfb575e25dabf (tensorrt_llm-h100_nvl-fp8-tp4-pp1-latency)\n",
    "  \n",
    "...\n",
    "```\n",
    "selected_profile = \"b7b6fa584441d9536091ce5cf80ccc31765780b8a46540da4e7bada5c5108ed9\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select TRT-LLM using just-in-time compilation of HF models to TRTLLM engines: \n",
    "profile: b7b6fa584441d9536091ce5cf80ccc31765780b8a46540da4e7bada5c5108ed9 (tensorrt_llm-trtllm_buildable-bf16-tp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a compatible profile ID for the model store creation\n",
    "# 8b09858c4fc22c360e5e6dda70d67751671c9f5a1182059ebaa91b4babce884c (tensorrt_llm-a100-fp16-tp1-throughput)\n",
    "selected_profile = \"b7b6fa584441d9536091ce5cf80ccc31765780b8a46540da4e7bada5c5108ed9\"\n",
    "print(f\"Selected compatible profile ID: {selected_profile}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\tmkdir /home/azureuser/llama-3.1-70b-nim\n",
    "\texport LOCAL_NIM_CACHE=/home/azureuser/llama-3.1-70b-nim\n",
    "\texport NIM_MODEL_PROFILE=trt-engine\n",
    "\t\n",
    "\tdocker run -it --rm --gpus all --shm-size=16GB -e NIM_CUSTOM_MODEL_NAME=70b-trt-engine -e NGC_API_KEY=xxxxxx  -e NIM_LOW_MEMORY_MODE=1 -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" -u $(id -u) -p 8000:8000  nvcr.io/nim/meta/llama-3.1-70b-instruct:1.3.3\n",
    "\n",
    "\tdocker run -it --rm  --gpus all     --shm-size=16GB  -e NIM_MODEL_PROFILE  -e NGC_API_KEY=xxxxxx -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\"     -u $(id -u)     -p 8000:8000 nvcr.io/nim/meta/llama-3.1-70b-instruct:1.3.3 bash -i -c list-model-profiles\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache_path=\"/home/azureuser/llama-3.1-70b-nim\"\n",
    "nim_custom_model_name=\"70b-trt-engine\"\n",
    "# Run the command to create model store for the chosen profile\n",
    "docker_create_model_command = [\n",
    "    \"docker\", \"run\", \"--rm\", \"--name=nim_model_cache_custom_name\",\n",
    "    \"-e\", \"LOG_LEVEL=info\",\n",
    "    \"-e\", \"NIM_LOW_MEMORY_MODE=1\",\n",
    "    \"-e\", f\"NIM_CUSTOM_MODEL_NAME={nim_custom_model_name}\",    \n",
    "    \"-e\", f\"NGC_API_KEY={ngc_api_key}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    \"-v\", f\"{model_cache_path}:/model-repo\",\n",
    "    \"-u\", \"root\",\n",
    "    f\"{ngc_container}\"\n",
    "]\n",
    "\n",
    "print(\"Cache lama-3.1-70b-instruct:1.3.3 to model_cache_path command in Docker...\")\n",
    "run_command(docker_create_model_command, \"Cache Model command in Docker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cache_path=\"/home/azureuser/llama-3.1-70b-nim/local_cache/70b-trt-engine/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the model was stored successfully\n",
    "run_command([\"ls\",model_cache_path], \"Model cache verification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Docker container and list model profiles inside it to make sure the new custom model profile is created\n",
    "docker_run_command = [\n",
    "    \"docker\", \"run\", \"--rm\", \"--name=nim_list_profiles\",\n",
    "    \"-e\", \"LOG_LEVEL=info\",\n",
    "    \"-e\", \"NIM_MODEL_PROFILE=trt-engine\",\n",
    "    \"-e\", f\"NGC_API_KEY={ngc_api_key}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    \"-v\", f\"{model_cache_path}:/model-repo\",\n",
    "    \"-u\", \"root\",\n",
    "    f\"{ngc_container}\",\n",
    "    \"bash\", \"-i\", \"-c\", \"list-model-profiles\"\n",
    "]\n",
    "\n",
    "# Execute the command to start Docker and list model profiles\n",
    "run_command(docker_run_command, \"Run Docker container to list model profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you run the command above, you should get the newly cached profile listed in list-model-profile command (70b-trt-engine): \n",
    "```\n",
    "942435195ceb8d93a1035c94844200e79e21c1b3744c5b4324ca40b71b164e75 (70b-trt-engine)\n",
    "```\n",
    "\n",
    "```\n",
    "===========================================\n",
    "\t== NVIDIA Inference Microservice LLM NIM ==\n",
    "\t===========================================\n",
    "\n",
    "\tNVIDIA Inference Microservice LLM NIM Version 1.3.3\n",
    "\tModel: meta/llama-3.1-70b-instruct\n",
    "\n",
    "\t......\n",
    "\tProfile 942435195ceb8d93a1035c94844200e79e21c1b3744c5b4324ca40b71b164e75 is not fully defined with checksums\n",
    "\tSYSTEM INFO\n",
    "\t- Free GPUs:\n",
    "\t  -  [20b5:10de] (0) NVIDIA A100 80GB PCIe [current utilization: 0%]\n",
    "\t  -  [20b5:10de] (1) NVIDIA A100 80GB PCIe [current utilization: 0%]\n",
    "\tProfile 0462612f0f2de63b2d423bc3863030835c0fbdbc13b531868670cc416e030029 is not fully defined with checksums\n",
    "\tProfile 09af04392c0375ae5493ca5e6ea0134890ac28f75efd244a57f414f86e97b133 is not fully defined with checksums\n",
    "\tProfile 0b0193d56ec0bba1840ea429993c776f9168a1ca4699e81f4db48319dd7e5c3a is not fully defined with checksums\n",
    "\tProfile 162948dba7374caeb8f7886f7c62a105fd198cfc2dd533aa1cdb34eaea872af0 is not fully defined with checksums\n",
    "\t.......\n",
    "\tMODEL PROFILES\n",
    "\t- Compatible with system and runnable:\n",
    "\t  - 942435195ceb8d93a1035c94844200e79e21c1b3744c5b4324ca40b71b164e75 (70b-trt-engine)\n",
    "\t  - 395082aa40085d35f004dd3056d7583aea330417ed509b4315099a66cfc72bdd (vllm-bf16-tp2)\n",
    "\t  - With LoRA support:\n",
    "\t- Compilable to TRT-LLM using just-in-time compilation of HF models to TRTLLM engines:\n",
    "\t  - b7b6fa584441d9536091ce5cf80ccc31765780b8a46540da4e7bada5c5108ed9 (tensorrt_llm-trtllm_buildable-bf16-tp2)\n",
    "\t  - With LoRA support:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push downloaded model to your AzureML workspace. Make sure to point to the local_cache/70b-trt-engine/  path this is where .engine files will be stored\n",
    "```\n",
    "\t az ml model create --name llama-3-1-70b-nim-tensorrt-bf16-tp2 --version 1 --path /home/azureuser/llama-3.1-70b-nim/local_cache/70b-trt-engine/ --resource-group rg-azeltov-aml --workspace-name aml_east\n",
    "```\n",
    "\n",
    "```\n",
    "azureuser@nc48a100:~/dev$ ll /home/azureuser/llama-3.1-70b-nim/local_cache/70b-trt-engine/trtllm_engine/\n",
    "total 140145228\n",
    "drwxr-xr-x 2 azureuser ubuntu        4096 Jan 15 00:55 ./\n",
    "drwxr-xr-x 3 azureuser ubuntu        4096 Jan 15 01:03 ../\n",
    "-rw-r--r-- 1 azureuser ubuntu        3640 Jan 15 00:59 config.json\n",
    "-rw-r--r-- 1 azureuser ubuntu 71754332004 Jan 15 01:02 rank0.engine\n",
    "-rw-r--r-- 1 azureuser ubuntu 71754331260 Jan 15 00:59 rank1.engine\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cached_model_name = \"llama-3-1-70b-nim-tensorrt-bf16-tp2\"\n",
    "model_version = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command([\n",
    "    \"az\", \"ml\", \"model\", \"create\",\n",
    "    \"--name\", cached_model_name,\n",
    "    \"--version\", model_version,\n",
    "    \"--path\", model_cache_path,\n",
    "    \"--resource-group\", resource_group,\n",
    "    \"--workspace-name\", workspace\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push container to Azure Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nim_acr_name = ngc_container.replace(\"nvcr.io\", f\"{acr_registry_name}.azurecr.io\")\n",
    "print(\"NIM image name as saved in ACR: \", nim_acr_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dockerfile content for NIM container\n",
    "dockerfile_content = f\"\"\"FROM {ngc_container}\n",
    "EXPOSE 8000\n",
    "USER root\n",
    "CMD bash -c \"echo 'Displaying the NGC API Key:' && echo $NGC_API_KEY && \\\n",
    "             echo 'Displaying the NIM Model Name:' && echo $NIM_MODEL_NAME && \\\n",
    "             echo 'Listing the contents of /model-repo:' && ls -al /model-repo && \\\n",
    "             /opt/nim/start-server.sh\"\n",
    "\"\"\"\n",
    "\n",
    "# Write Dockerfile to disk\n",
    "with open(\"Dockerfile\", \"w\") as dockerfile:\n",
    "    dockerfile.write(dockerfile_content)\n",
    "print(\"NIM Dockerfile has been created.\")\n",
    "\n",
    "# Login to Azure Container Registry\n",
    "print(\"Logging into Azure Container Registry\")\n",
    "run_command([\"az\", \"acr\", \"login\", \"-n\", acr_registry_name])\n",
    "\n",
    "# Build and tag the Docker image\n",
    "print(\"Building the new Docker image and tagging it\")\n",
    "run_command([\"docker\", \"build\", \"-t\", nim_acr_name, \"-f\", \"Dockerfile\", \".\"], \"Building azure NIM image\")\n",
    "\n",
    "# Clean up Dockerfile after build\n",
    "#os.remove(\"Dockerfile\")\n",
    "\n",
    "# Push the image to ACR\n",
    "print(\"Pushing the image to ACR\")\n",
    "run_command([\"docker\", \"push\", nim_acr_name], \"Pushing NIM image to Azure container registry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AzureML managed endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the endpoint YAML configuration\n",
    "endpoint_yaml_content = f\"\"\"$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json\n",
    "name: {endpoint_name}\n",
    "auth_mode: key\n",
    "properties:\n",
    "    enforce_access_to_default_secret_stores: enabled  # default: disabled\n",
    "\"\"\"\n",
    "\n",
    "# Write endpoint YAML configuration to file\n",
    "with open(\"actual_endpoint_aml.yml\", \"w\") as endpoint_yaml:\n",
    "    endpoint_yaml.write(endpoint_yaml_content)\n",
    "print(\"Endpoint YAML configuration created.\")\n",
    "\n",
    "# Deploy the endpoint using the Azure CLI\n",
    "print(f\"Creating Online Endpoint {endpoint_name}\")\n",
    "output = run_command([\n",
    "    \"az\", \"ml\", \"online-endpoint\", \"create\", \"-f\", \"actual_endpoint_aml.yml\",\n",
    "    \"--resource-group\", resource_group, \"--workspace-name\", workspace\n",
    "], return_output=True)\n",
    "\n",
    "endpointidentityid = \"\"\n",
    "if output:\n",
    "    endpoint_data = json.loads(output)\n",
    "    endpointidentityid = endpoint_data.get(\"identity\", {}).get(\"principal_id\")\n",
    "    if endpointidentityid:\n",
    "        print(f\"Principal ID: {endpointidentityid}\")\n",
    "    else:\n",
    "        print(\"Principal ID not found.\")\n",
    "else:\n",
    "    print(\"No output received.\")\n",
    "\n",
    "# Clean up the generated YAML file\n",
    "#os.remove(\"actual_endpoint_aml.yml\")\n",
    "print(\"Cleaned up temporary endpoint YAML file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide access to endpoint to read secrets from key vault\n",
    "endpointidentityid = \"5ebd626e-907e-463c-ab5a-cdd89702dcfa\"\n",
    "run_command([\n",
    "    \"az\", \"role\", \"assignment\", \"create\",\n",
    "    \"--assignee\", endpointidentityid,\n",
    "    \"--role\", \"Key Vault Secrets User\",\n",
    "    \"--scope\", f\"/subscriptions/{subscription_id}/resourcegroups/{resource_group}/providers/Microsoft.KeyVault/vaults/{keyvault_name}\"\n",
    "], \"Providing permissions for endpoint to access NGC key from key vault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NIM deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airgapped deployment yaml file\n",
    "\n",
    "Run this cell to generate a deployment YAML file for deploying NIMs in an airgapped environment. This deployment requires mounting a cached NIM model and setting the NIM container’s NIM_MODEL_NAME environment variable, which instructs NIM to avoid downloading from the internet and instead utilize the model hosted on the instance. The NIM model is sourced by mounting the previously pushed model from our AzureML workspace’s model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_yaml_content = f\"\"\"$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json\n",
    "name: {deployment_name}\n",
    "endpoint_name: {endpoint_name}\n",
    "model: azureml:{cached_model_name}:{model_version}\n",
    "model_mount_path: /model-repo\n",
    "environment: \n",
    "  name: llama31-70b-nim-trt-tp2-env\n",
    "  image: {nim_acr_name}\n",
    "  inference_config:\n",
    "      liveness_route:\n",
    "          path: /v1/health/ready\n",
    "          port: 8000\n",
    "      readiness_route:\n",
    "          path: /v1/health/ready\n",
    "          port: 8000\n",
    "      scoring_route:\n",
    "          path: /\n",
    "          port: 8000\n",
    "instance_type: {instance_type}\n",
    "instance_count: 1\n",
    "request_settings:\n",
    "    max_concurrent_requests_per_instance: 256\n",
    "    request_timeout_ms: 180000\n",
    "environment_variables:\n",
    "    NIM_MODEL_NAME: /model-repo/70b-trt-engine/\n",
    "    NGC_API_KEY: {ngc_api_key}\n",
    "    shm-size: 16GB\n",
    "    gpus: 0,1\n",
    "    OMPI_ALLOW_RUN_AS_ROOT: 1\n",
    "    OMPI_ALLOW_RUN_AS_ROOT_CONFIRM: 1\n",
    "    # NIM_LOW_MEMORY_MODE: 1\n",
    "    NIM_MANIFEST_ALLOW_UNSAFE: 1\n",
    "    NIM_MODEL_PROFILE: trt-engine    \n",
    "liveness_probe:\n",
    "    timeout: 300\n",
    "    period: 300\n",
    "    failure_threshold: 100\n",
    "readiness_probe:\n",
    "    timeout: 300\n",
    "    period: 300\n",
    "    failure_threshold: 100    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display content of deployment YAML file\n",
    "import yaml\n",
    "yaml_content = yaml.safe_load(deployment_yaml_content)\n",
    "print(yaml.dump(yaml_content, sort_keys=False, default_flow_style=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write deployment YAML configuration to file\n",
    "with open(\"actual_deployment_aml.yml\", \"w\") as deployment_yaml:\n",
    "    deployment_yaml.write(deployment_yaml_content)\n",
    "print(\"Deployment YAML configuration created.\")\n",
    "\n",
    "# Display the modified YAML file for confirmation\n",
    "with open(\"actual_deployment_aml.yml\", \"r\") as file:\n",
    "    print(file.read())\n",
    "\n",
    "# Create the online deployment using the Azure CLI\n",
    "print(f\"Creating Online Deployment {deployment_name}\")\n",
    "run_command([\n",
    "    \"az\", \"ml\", \"online-deployment\", \"create\", \"-f\", \"actual_deployment_aml.yml\",\n",
    "    \"--resource-group\", resource_group, \"--workspace-name\", workspace\n",
    "])\n",
    "\n",
    "# Clean up the generated YAML file\n",
    "#os.remove(\"actual_deployment_aml.yml\")\n",
    "print(\"Cleaned up temporary deployment YAML file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your connection\n",
    "\n",
    "Verify your deployment using the code below. Modify the code to add your endpoint URL, Endpoint authorization token and AzureML deployment name obtained from \"Consume\" tab under your AzureML endpoint page as shown below.\n",
    "\n",
    "![Endpoint details](./endpoint_details.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at your deployment logs for available serving endpoints and example CURL request\n",
    "\n",
    "![Serving endpoints](./serving_endpoints.png)\n",
    "\n",
    "![Example request](./example_request.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://llama31-70b-nim-trt.eastus.inference.ml.azure.com/' # modify this URL\n",
    "token = 'xxxxxxxxxxxxxxxx'\n",
    "model_name = \"meta/llama-3.1-70b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url = urljoin(base_url, \"v1/chat/completions\")\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Authorization': f'Bearer {token}', # modify this token\n",
    "    'Content-Type': 'application/json',\n",
    "    'azureml-model-deployment': deployment_name\n",
    "}\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a polite and respectful chatbot helping people plan a vacation.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"What should I do for a 4 day vacation in Spain?\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": model_name,\n",
    "    \"max_tokens\": 500,\n",
    "    \"top_p\": 1,\n",
    "    \"n\": 1,\n",
    "    \"stream\": False,\n",
    "    #\"stop\": \"\\n\",\n",
    "    \"frequency_penalty\": 0.0\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "# Pretty print the JSON response\n",
    "print(json.dumps(response.json(), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Launch a Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Function to handle the chat with LLM\n",
    "def chat_with_llm(user_message, history):\n",
    "    # Format messages for the LLM with initial system prompt\n",
    "    formatted_messages = [\n",
    "        {\"content\": \"You are a polite and respectful AI assistant.\", \"role\": \"system\"}\n",
    "    ]\n",
    "    \n",
    "    # Add previous messages from the history\n",
    "    for user_msg, assistant_msg in history:\n",
    "        formatted_messages.append({\"content\": user_msg, \"role\": \"user\"})\n",
    "        formatted_messages.append({\"content\": assistant_msg, \"role\": \"assistant\"})\n",
    "\n",
    "    # Add the user's latest message\n",
    "    formatted_messages.append({\"content\": user_message, \"role\": \"user\"})\n",
    "\n",
    "    data = {\n",
    "        \"messages\": formatted_messages,\n",
    "        \"model\": model_name,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"top_p\": 1,\n",
    "        \"n\": 1,\n",
    "        \"stream\": False,  # Set to True if streaming responses are supported by your API\n",
    "        \"stop\": \"\\n\",\n",
    "        \"frequency_penalty\": 0.1\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_data = response.json()\n",
    "        assistant_reply = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"No response\")\n",
    "    else:\n",
    "        assistant_reply = f\"Error: {response.status_code} - {response.text}\"\n",
    "\n",
    "    # Return the assistant's reply as a string\n",
    "    return assistant_reply\n",
    "\n",
    "# Create a ChatInterface using Gradio\n",
    "chat_interface = gr.ChatInterface(\n",
    "    fn=chat_with_llm,\n",
    "    title=\"Multi-Turn LLM Chatbot with gr.ChatInterface\",\n",
    "    description=\"A chatbot interface that interacts with your LLM endpoint for multi-turn conversations.\",\n",
    "    examples=[[\"What should I do for a 4-day vacation in Spain?\"]]\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "chat_interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
