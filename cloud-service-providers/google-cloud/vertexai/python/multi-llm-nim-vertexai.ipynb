{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dda6d03",
   "metadata": {},
   "source": [
    "## Deploy multi-LLM NIM (Hugging Face) to GCP Vertex AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a452f11",
   "metadata": {},
   "source": [
    "This notebook demonstrates deploying the multi-LLM compatible NVIDIA NIM on Vertex AI using a Hugging Face model.\n",
    "\n",
    "We will:\n",
    "- Pull multi-LLM NIM from NGC and push to Artifact Registry\n",
    "- Optionally run locally for validation\n",
    "- Upload the container as a Vertex AI Model\n",
    "- Create a Vertex AI Endpoint and deploy the Model\n",
    "- Send inference requests to the Endpoint\n",
    "\n",
    "Reference: [Get Started with NVIDIA NIM for LLMs](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8815c61",
   "metadata": {},
   "source": [
    "### Install and import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0393fe",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f23a76",
   "metadata": {},
   "source": [
    "Restart kernel after installs so new packages are available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de39b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc35381a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform_v1beta1 as aip_beta\n",
    "from google.cloud.aiplatform import Endpoint, Model\n",
    "from google.api_core.exceptions import InvalidArgument\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa1ef3",
   "metadata": {},
   "source": [
    "### Authenticate to Google Cloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db42dc0",
   "metadata": {},
   "source": [
    "Please run the following commands **in a separate Terminal window.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth login\n",
    "gcloud auth application-default login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12d15b5",
   "metadata": {},
   "source": [
    "### Set up variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"\" # e.g. \"us-central1\"\n",
    "project_id = \"\" # your GCP project id\n",
    "public_repository = \"\"  # optional: name for Artifact Registry that will get created below\n",
    "NGC_API_KEY = \"\" # Your NGC API Key\n",
    "HF_TOKEN = \"\" # add token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fe96a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from subprocess import getoutput\n",
    "account_email = getoutput('gcloud config get-value account')\n",
    "account_name = account_email.split('@')[0] if '@' in account_email else \"user\"\n",
    "private_repository = account_name\n",
    "bucket_url = f\"gs://{account_name}\"\n",
    "\n",
    "# NIM (multi-LLM) + Hugging Face model\n",
    "nim_image_ngc = \"nvcr.io/nim/nvidia/llm-nim:1.13.0\"\n",
    "image_name = \"llm-nim\"\n",
    "image_tag = \"1.13.0\"\n",
    "\n",
    "NIM_MODEL_NAME = \"hf://meta-llama/Meta-Llama-3-8B\"\n",
    "NIM_SERVED_MODEL_NAME = \"meta/llama3-8b-instruct\"\n",
    "\n",
    "# Artifact Registry targets (repo/image:tag)\n",
    "private_nim_image = f\"{region}-docker.pkg.dev/{project_id}/{private_repository}/{image_name}:{image_tag}\"\n",
    "public_nim_image = (\n",
    "    f\"{region}-docker.pkg.dev/{project_id}/{public_repository}/{image_name}:{image_tag}\"\n",
    "    if public_repository else None\n",
    ")\n",
    "\n",
    "# Vertex AI names\n",
    "va_model_name = \"nim-multi-llm\"\n",
    "endpoint_name = va_model_name + \"-endpoint\"\n",
    "\n",
    "# Local cache for optional local run\n",
    "local_nim_cache = \"~/.cache/nim\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c0bc3",
   "metadata": {},
   "source": [
    "### GCP Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16f8a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_bash_cmd(cmd):\n",
    "    import subprocess\n",
    "\n",
    "    if isinstance(cmd, str):\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, text=True)\n",
    "    elif isinstance(cmd, list):\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False, text=True)\n",
    "        \n",
    "    output, error = process.communicate()\n",
    "    if error:\n",
    "        raise Exception(error)\n",
    "    else:\n",
    "        print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e58bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bash_cmd = f\"\"\"\n",
    "    export region={region}\n",
    "    gcloud config set ai_platform/region {region}\n",
    "    gcloud config set project {project_id}\n",
    "    gcloud auth configure-docker {region}-docker.pkg.dev\n",
    "    \"\"\"\n",
    "run_bash_cmd(bash_cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b815d57",
   "metadata": {},
   "source": [
    "Grant required IAM roles to the service account\n",
    "\n",
    "- Vertex AI Users `(roles/aiplatform.user)`\n",
    "- Artifact Registry Repository Administrator `(roles/artifactregistry.repoAdmin)`\n",
    "- Storage Admin `(roles/storage.admin)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c84e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from subprocess import getoutput\n",
    "\n",
    "project_number = getoutput(f\"gcloud projects describe {project_id} --format='value(projectNumber)'\").strip()\n",
    "service_account = f\"serviceAccount:{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "run_bash_cmd(f\"gcloud projects add-iam-policy-binding {project_id} --member={service_account} --role=roles/aiplatform.user\")\n",
    "run_bash_cmd(f\"gcloud projects add-iam-policy-binding {project_id} --member={service_account} --role=roles/artifactregistry.repoAdmin\")\n",
    "run_bash_cmd(f\"gcloud projects add-iam-policy-binding {project_id} --member={service_account} --role=roles/storage.admin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e3158",
   "metadata": {},
   "source": [
    "If Cloud Storage bucket or Artifact Registry repositories don't exist, create them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b2bda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create bucket and AR repos if needed\n",
    "run_bash_cmd(f\"gsutil mb -l {region} -p {project_id} {bucket_url}\")\n",
    "run_bash_cmd(f\"gcloud artifacts repositories create {private_repository} --repository-format=docker --location={region}\")\n",
    "if public_repository:\n",
    "    run_bash_cmd(f\"gcloud artifacts repositories create {public_repository} --repository-format=docker --location={region}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9343612e",
   "metadata": {},
   "source": [
    "(Optional) Grant Artifact Registry read access to a user/group/service account for public repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: set member to grant AR read access to\n",
    "member = None  # e.g. 'user:test-user@gmail.com' or 'serviceAccount:xyz@project.iam.gserviceaccount.com'\n",
    "if public_repository and member:\n",
    "    run_bash_cmd(f\"gcloud artifacts repositories add-iam-policy-binding {public_repository} --location={region} --member={member} --role=roles/artifactregistry.reader\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13d167",
   "metadata": {},
   "source": [
    "### Pull multi-LLM NIM from NGC and push to Artifact Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1080d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "container_name = \"LLM-NIM\"\n",
    "local_cache_dir = str(Path(local_nim_cache).expanduser())\n",
    "\n",
    "bash_cmd = f\"\"\"\n",
    "    export NGC_API_KEY={NGC_API_KEY}\n",
    "    echo \"export NGC_API_KEY={NGC_API_KEY}\" >> ~/.bashrc\n",
    "    echo \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin\n",
    "    mkdir -p \"{local_cache_dir}\"\n",
    "    chmod -R a+w \"{local_cache_dir}\"\n",
    "    echo \"Local NIM cache created at {local_cache_dir}\"\n",
    "    docker pull {nim_image_ngc}\n",
    "    \"\"\"\n",
    "run_bash_cmd(bash_cmd)\n",
    "\n",
    "# Tag and push to private Artifact Registry\n",
    "run_bash_cmd(f\"docker tag {nim_image_ngc} {private_nim_image}\")\n",
    "run_bash_cmd(f\"docker push {private_nim_image}\")\n",
    "\n",
    "# Optional: also push to public AR\n",
    "if public_repository:\n",
    "    run_bash_cmd(f\"docker tag {private_nim_image} {public_nim_image}\")\n",
    "    run_bash_cmd(f\"docker push {public_nim_image}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afed0bb",
   "metadata": {},
   "source": [
    "### Optional: Run multi-LLM NIM locally (validate model startup) **in terminal**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578dce5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run in terminal\n",
    "export HF_TOKEN=\"\"\n",
    "export CONTAINER_NAME=\"LLM-NIM\"\n",
    "export IMG_NAME=\"nvcr.io/nim/nvidia/llm-nim:1.13.0\"\n",
    "export NIM_MODEL_NAME = \"hf://meta-llama/Meta-Llama-3-8B\"\n",
    "export NIM_SERVED_MODEL_NAME = \"meta/llama3-8b-instruct\"\n",
    "export LOCAL_NIM_CACHE=\"$HOME/.cache/nim\"\n",
    "\n",
    "# Create cache directory\n",
    "mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "chmod -R a+w \"$LOCAL_NIM_CACHE\"\n",
    "\n",
    "# Run Docker container\n",
    "docker run -it --rm --name=\"$CONTAINER_NAME\" \\\n",
    "  --gpus all \\\n",
    "  --shm-size=\"16GB\" \\\n",
    "  -e HF_TOKEN=\"$HF_TOKEN\" \\\n",
    "  -e NIM_MODEL_NAME=\"$NIM_MODEL_NAME\" \\\n",
    "  -e NIM_SERVED_MODEL_NAME=\"$NIM_SERVED_MODEL_NAME\" \\\n",
    "  -v \"$LOCAL_NIM_CACHE:/opt/nim/.cache\" \\\n",
    "  -u \"$(id -u)\" \\\n",
    "  -p 8000:8000 \\\n",
    "  \"$IMG_NAME\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77162b",
   "metadata": {},
   "source": [
    "### Upload multi-LLM NIM as a Vertex AI Model resource\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd671d8-94f4-451b-9f20-13bed6bb59bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core.future.polling import DEFAULT_POLLING\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "DEFAULT_POLLING._timeout = 360000\n",
    "\n",
    "# Init SDK\n",
    "aiplatform.init(project=project_id, location=region, staging_bucket=bucket_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce72a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multi-LLM NIM needs the Hugging Face token and model selection via env vars\n",
    "serving_env = {\n",
    "    \"PORT\": \"8000\",\n",
    "    \"shm-size\": \"16GB\",\n",
    "    \"HF_TOKEN\": HF_TOKEN or \"\",\n",
    "    \"NIM_MODEL_NAME\": NIM_MODEL_NAME,\n",
    "    \"NIM_SERVED_MODEL_NAME\": NIM_SERVED_MODEL_NAME,\n",
    "}\n",
    "\n",
    "models = aiplatform.Model.list(filter=f'displayName=\"{va_model_name}\"')\n",
    "\n",
    "if models:\n",
    "    model = models[0]\n",
    "else:\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=va_model_name,\n",
    "        serving_container_image_uri=private_nim_image,\n",
    "        serving_container_predict_route=\"/v1/chat/completions\",\n",
    "        serving_container_health_route=\"/v1/health/ready\",\n",
    "        serving_container_environment_variables=serving_env,\n",
    "        serving_container_shared_memory_size_mb=16000,\n",
    "        serving_container_ports=[8000],\n",
    "        sync=True,\n",
    "    )\n",
    "model.wait()\n",
    "\n",
    "print(\"Model:\")\n",
    "print(f\"\\tDisplay name: {model.display_name}\")\n",
    "print(f\"\\tResource name: {model.resource_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd49da6",
   "metadata": {},
   "source": [
    "### Create Vertex AI Endpoint and deploy the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecedbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import Endpoint\n",
    "\n",
    "endpoints = Endpoint.list(filter=f'displayName=\"{endpoint_name}\"')\n",
    "if endpoints:\n",
    "    endpoint = endpoints[0]\n",
    "else:\n",
    "    print(f\"Endpoint {endpoint_name} doesn't exist, creating...\")\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=endpoint_name)\n",
    "print(\"Endpoint:\")\n",
    "print(f\"\\tDisplay name: {endpoint.display_name}\")\n",
    "print(f\"\\tResource name: {endpoint.resource_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e56fa-d986-4d49-be45-3ec4a1af53d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=va_model_name,\n",
    "    traffic_percentage=100,\n",
    "    machine_type=\"g2-standard-24\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    accelerator_count=2,\n",
    "    enable_access_logging=True,\n",
    "    sync=True,\n",
    ")\n",
    "print(f\"Model {model.display_name} deployed at endpoint {endpoint.display_name}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b8143",
   "metadata": {},
   "source": [
    "### Endpoint inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e683af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare payloads for chat completions\n",
    "import json\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU computing.\"}\n",
    "]\n",
    "\n",
    "llama3_chat_template = (\n",
    "    \"{% for message in messages %}\"\n",
    "    \"{% if message['role'] == 'system' %}\"\n",
    "    \"{{'<|begin_of_text|>' + message['content']}}\"\n",
    "    \"{% elif message['role'] == 'user' %}\"\n",
    "    \"{{'<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>'}}\"\n",
    "    \"{% elif message['role'] == 'assistant' %}\"\n",
    "    \"{{'<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>'}}\"\n",
    "    \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{'<|start_header_id|>assistant<|end_header_id|>\\n\\n'}}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "\n",
    "payload = {\n",
    "  \"model\": NIM_SERVED_MODEL_NAME,\n",
    "  \"messages\": messages,\n",
    "  \"temperature\": 0.2,\n",
    "  \"max_tokens\": 512,\n",
    "  \"top_p\": 0.8,\n",
    "  \"chat_template\": llama3_chat_template\n",
    "}\n",
    "\n",
    "with open(\"request.json\", \"w\") as outfile:\n",
    "    json.dump(payload, outfile)\n",
    "\n",
    "payload_s = {\n",
    "  \"model\": NIM_SERVED_MODEL_NAME,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 512,\n",
    "  \"stream\": True\n",
    "}\n",
    "\n",
    "with open(\"request_stream.json\", \"w\") as outfile:\n",
    "    json.dump(payload_s, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f46651-e824-4401-a6c3-13a209635bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7f7e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Python SDK rawPredict\n",
    "from pprint import pprint\n",
    "from google.api import httpbody_pb2\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "http_body = httpbody_pb2.HttpBody(\n",
    "    data=json.dumps(payload).encode(\"utf-8\"),\n",
    "    content_type=\"application/json\",\n",
    ")\n",
    "\n",
    "req = aiplatform_v1.RawPredictRequest(\n",
    "    http_body=http_body, endpoint=endpoint.resource_name\n",
    ")\n",
    "\n",
    "API_ENDPOINT = f\"{region}-aiplatform.googleapis.com\"\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "pred_client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "response = pred_client.raw_predict(req)\n",
    "print(\"Response:\")\n",
    "try:\n",
    "    print(json.loads(response.data))\n",
    "except Exception:\n",
    "    print(response.data.decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42167f50",
   "metadata": {},
   "source": [
    "### Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_endpoint = True\n",
    "delete_model = True\n",
    "delete_image = True\n",
    "delete_art_repo = False\n",
    "delete_bucket = False\n",
    "\n",
    "# Undeploy model and delete endpoint\n",
    "try:\n",
    "    if delete_endpoint:\n",
    "        endpoint.undeploy_all(sync=True)\n",
    "        endpoint.delete()\n",
    "        print(f\"Deleted endpoint {endpoint.display_name}\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the model resource\n",
    "try:\n",
    "    if delete_model:\n",
    "        model.delete()\n",
    "        print(f\"Deleted model {model.display_name}\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the container image from Artifact Registry\n",
    "if delete_image:\n",
    "    run_bash_cmd(f\"gcloud artifacts docker images delete --quiet --delete-tags {private_nim_image}\")\n",
    "\n",
    "# Optionally delete repositories and bucket\n",
    "if delete_art_repo:\n",
    "    run_bash_cmd(f\"gcloud artifacts repositories delete {private_repository} --location={region} -q\")\n",
    "\n",
    "if delete_bucket:\n",
    "    run_bash_cmd(f\"gsutil rm -rf {bucket_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b700dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_endpoint = True\n",
    "delete_model = True\n",
    "delete_image = True\n",
    "delete_art_repo = False\n",
    "delete_bucket = False\n",
    "\n",
    "# Undeploy model and delete endpoint\n",
    "try:\n",
    "    if delete_endpoint:\n",
    "        endpoint.undeploy_all(sync=True)\n",
    "        endpoint.delete()\n",
    "        print(f\"Deleted endpoint {endpoint.display_name}\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the model resource\n",
    "try:\n",
    "    if delete_model:\n",
    "        model.delete()\n",
    "        print(f\"Deleted model {model.display_name}\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the container image from Artifact Registry\n",
    "if delete_image:\n",
    "    run_bash_cmd(f\"gcloud artifacts docker images delete --quiet --delete-tags {private_nim_image}\")\n",
    "\n",
    "# Optionally delete repositories and bucket\n",
    "if delete_art_repo:\n",
    "    run_bash_cmd(f\"gcloud artifacts repositories delete {private_repository} --location={region} -q\")\n",
    "\n",
    "if delete_bucket:\n",
    "    run_bash_cmd(f\"gsutil rm -rf {bucket_url}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
