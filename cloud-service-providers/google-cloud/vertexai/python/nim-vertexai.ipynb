{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab89d38c-6977-4c6f-9a0f-79676a820fa1",
   "metadata": {},
   "source": [
    "## Deploy NVIDIA NIM to GCP Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc36ce1-5671-470b-83b9-92a7450eefa9",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "NVIDIA NIM is a set of easy-to-use microservices designed to accelerate the deployment of generative AI models across the cloud, data center, and workstations. NIMs are categorized by model family and a per model basis. For example, NVIDIA NIM for large language models (LLMs) brings the power of state-of-the-art LLMs to enterprise applications, providing unmatched natural language processing and understanding capabilities.\n",
    "\n",
    "In this notebook, you learn to how to run NVIDIA NIM container on Google Cloud Vertex AI, make inference to get customized responses, and deploy model to Vertex AI endpoint.\n",
    "\n",
    "This tutorial uses the following NVIDIA NIM and Vertex AI services:\n",
    "\n",
    "- NVIDIA NIM Container\n",
    "- Vertex AI Model resource\n",
    "- Vertex AI Model Registry\n",
    "- Vertex AI Endpoint resource\n",
    "- Vertex AI Prediction\n",
    "- Vertex AI Artifact Registry\n",
    "- Vertex AI Cloud Storage\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Pull NVIDIA NIM container from NGC.\n",
    "- Push NVIDIA NIM container to Artifact Registry.\n",
    "- Run NIM container to make inference within interface.\n",
    "- Upload NIM container as a Vertex AI Model resource.\n",
    "- Create a Vertex AI Endpoint resource.\n",
    "- Deploy the Model resource to the Endpoint resource.\n",
    "- Generate prediction responses from Endpoint resource.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a8a7f-d92b-443e-984b-0902742cf5aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install and Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ONDCfQqTYsu7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7700,
     "status": "ok",
     "timestamp": 1721147733783,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "ONDCfQqTYsu7",
    "outputId": "191cf11d-20fc-4142-edf1-7c12ea178438",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765a47d-d7e9-4f00-acda-5c5aedfc9698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11544c12-8d8b-47a2-a32a-8129affab5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that the environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oHkv14VRimlG",
   "metadata": {
    "executionInfo": {
     "elapsed": 2680,
     "status": "ok",
     "timestamp": 1721243802293,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "oHkv14VRimlG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import google.cloud.aiplatform_v1beta1 as aip_beta\n",
    "\n",
    "from google.cloud.aiplatform import Endpoint, Model\n",
    "from google.api_core.exceptions import InvalidArgument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84772f30-4f4c-4356-93be-4d60b17d0437",
   "metadata": {},
   "source": [
    "### Authenticate to Google Cloud\n",
    "Depending on Jupyter environment, please follow instructions below to authenticate to Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c6ebe-e545-4505-97fd-cfb4ae123f13",
   "metadata": {},
   "source": [
    "* Vertex AI Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20441a-e684-4bf5-b6da-496f9664ede9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud auth login\n",
    "! gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d20a9f9-e9cb-4318-9973-3066bbfe10e2",
   "metadata": {},
   "source": [
    "* Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ieTra-OYt66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1721238618341,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "5ieTra-OYt66",
    "outputId": "8bc00099-f31f-423c-f7ae-3154c73b9697",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed8ff4-2deb-45af-ad9f-ccbd7d801a8f",
   "metadata": {},
   "source": [
    "### Set Up\n",
    "\n",
    "The example provided is `llama3-8b-instruct` NIM, on Vertex AI Workbench Notebook `g2-standard-24` instance with NVIDIA L4 GPU.\n",
    "\n",
    "IAM role requirements:\n",
    "* Artifact Registry Repository Administrator `(roles/artifactregistry.repoAdmin)` \n",
    "* Storage Admin `(roles/storage.admin)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vn6KU7wfjew5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1721238645640,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "vn6KU7wfjew5",
    "outputId": "ca54be92-1d89-43cf-96e8-25d28f7577b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get account name\n",
    "import requests\n",
    "gcloud_token = !gcloud auth print-access-token\n",
    "gcloud_tokeninfo = requests.get('https://www.googleapis.com/oauth2/v3/tokeninfo?access_token=' + gcloud_token[0]).json()\n",
    "account_email = gcloud_tokeninfo['email']\n",
    "account_name = gcloud_tokeninfo['email'].split('@')[0]\n",
    "print(account_email)\n",
    "print(account_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g7chyrzCF9yoWk9IFyfdOe4W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1721251321866,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "g7chyrzCF9yoWk9IFyfdOe4W",
    "outputId": "e2090fb2-bca8-46c9-c79b-d2369a53377f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NIM: llama3-8b-instruct\n",
    "region = \"us-central1\" # please set here\n",
    "project_id = None # please set here\n",
    "public_repository = None # please set here\n",
    "private_repository = account_name\n",
    "bucket_url = f\"gs://{account_name}\"\n",
    "\n",
    "nim_model = \"nim:llama3-8b-instruct-1.0.0\"\n",
    "# NIM in NGC\n",
    "ngc_nim_image = \"nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\"\n",
    "# NIM in Artifact Registry\n",
    "public_nim_image = f\"{region}-docker.pkg.dev/{project_id}/{public_repository}/{nim_model}\"\n",
    "private_nim_image = f\"{region}-docker.pkg.dev/{project_id}/{private_repository}/{nim_model}\"\n",
    "\n",
    "va_model_name = \"nim-llama3-8b-instruct\"\n",
    "\n",
    "selected_profile = \"vllm-fp16-tp2\"\n",
    "machine_type = \"g2-standard-24\"\n",
    "accelerator_type = \"NVIDIA_L4\"\n",
    "accelerator_count =2\n",
    "\n",
    "endpoint_name = va_model_name+\"_endpoint\"\n",
    "payload_model = \"meta/llama3-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dSwhC52fduXZ",
   "metadata": {
    "id": "dSwhC52fduXZ"
   },
   "source": [
    "If Cloud Storage Bucket or Artifact Registry repository doesn't already exist: Run the following cell to create your bucket or repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-ti93Pb8dkdW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2220,
     "status": "ok",
     "timestamp": 1721250406564,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "-ti93Pb8dkdW",
    "outputId": "1810a050-6d92-40a2-b914-4ee353b5f1fb"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l {region} -p  {project_id} {bucket_url}\n",
    "! gcloud artifacts repositories create {public_repository} --repository-format=docker --location={region}\n",
    "! gcloud artifacts repositories add-iam-policy-binding {public_repository} --location={region} --member=allUsers --role=roles/artifactregistry.repoAdmin\n",
    "! gcloud artifacts repositories create {private_repository} --repository-format=docker --location={region}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3d2c3-92ad-49c5-85a5-229602c25e06",
   "metadata": {},
   "source": [
    "Initialize Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uYMNxGozkZyj",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1721251252748,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "uYMNxGozkZyj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=project_id, location=region, staging_bucket=bucket_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c98fc3-d8a2-4abd-bd86-b1e120f07c4a",
   "metadata": {},
   "source": [
    "GCP Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fad6a-e213-4e7e-baab-84f78b17fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bash_cmd(cmd):\n",
    "    import subprocess\n",
    "\n",
    "    if isinstance(cmd, str):\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True, text=True)\n",
    "    elif isinstance(cmd, list):\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=False, text=True)\n",
    "        \n",
    "    output, error = process.communicate()\n",
    "    if error:\n",
    "        raise Exception(error)\n",
    "    else:\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d570257-69fd-48e3-a558-8d861db11818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bash_cmd = f\"\"\"\n",
    "    export region={region}\n",
    "    gcloud config set ai_platform/region {region}\n",
    "    gcloud config set project {project_id}\n",
    "    gcloud auth configure-docker {region}-docker.pkg.dev\n",
    "    \"\"\"\n",
    "run_bash_cmd(bash_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb66464-949b-4998-b9fc-620acba7fd8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NIM Container\n",
    "\n",
    "* **NGC_API_KEY**\n",
    "\n",
    "To access NIM container from NGC catalog, `NGC_API_KEY` is required.\n",
    "\n",
    "The credentail will be used in Vertex AI as an environment variable during model uploading, and will show on Model Registry Version Details UI. **Attention: the credential will be visible for all Vertex AI users in the same project.**\n",
    "\n",
    "Please upload a json file to Cloud Storage Bucket to use `read_key()` function below, format  `\"{NGC_API_KEY\": Your Key}\"`.\n",
    "\n",
    "Reference: [NGC User Guide](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html)\n",
    "\n",
    "* **Artifact Registry**\n",
    "\n",
    "We will first pull NIM image from NGC, then push to a public AR repository. This allows all accounts in the project able to access NIM.\n",
    "\n",
    "Then we pull NIM image from the public AR and push to a private AR repository. This allows modification of NIM image without affecting the origin. (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1LxZSHxbeiV4",
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1721238635666,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "1LxZSHxbeiV4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import json\n",
    "\n",
    "def read_key(bucket_name, blob_name, key_name):\n",
    "    \"\"\"Write and read a blob from GCS using file-like IO\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your new GCS object\n",
    "    # blob_name = \"storage-object-name\"\n",
    "    \n",
    "    # The ID of your NGC key\n",
    "    # key_name = \"NGC_API_KEY\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "\n",
    "    with blob.open(\"r\") as f:\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "    return data[key_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X0s70ghC-gY_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1721238662144,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "X0s70ghC-gY_",
    "outputId": "7ef6b24a-ca64-44b9-ecdb-8c3c5af7c47f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set NGC API KEY\n",
    "import json\n",
    "json_file = None # please set here\n",
    "NGC_API_KEY = read_key(account_name, json_file, \"NGC_API_KEY\")\n",
    "\n",
    "assert NGC_API_KEY is not None, \"NGC API KEY is not set. Please set the NGC_API_KEY variable. It's required for running NIM.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b460cda-e7f7-4e14-83c5-4707e2b3770c",
   "metadata": {},
   "source": [
    "Pull NIM from NGC and Push to GCP AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kI-V4mFQF0DW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3277,
     "status": "ok",
     "timestamp": 1721243311334,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "kI-V4mFQF0DW",
    "outputId": "ce78f943-0702-482c-bc83-a53ef531920e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Login to NGC\n",
    "from pathlib import Path\n",
    "container_name=\"llama3-8B-Instruct\"\n",
    "local_nim_cache=str(Path(\".cache/nim\").absolute())\n",
    "\n",
    "bash_cmd = f\"\"\"\n",
    "    sudo apt-get install -y nvidia-docker2\n",
    "    export NGC_API_KEY={NGC_API_KEY}\n",
    "    echo \"export NGC_API_KEY={NGC_API_KEY}\" >> ~/.bashrc\n",
    "    echo \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin\n",
    "\n",
    "    export LOCAL_NIM_CACHE={local_nim_cache}\n",
    "    mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "    echo \"Local NIM cache created\"\n",
    "    \"\"\"\n",
    "\n",
    "run_bash_cmd(bash_cmd)\n",
    "\n",
    "# Pull NIM image from NGC and run container\n",
    "docker_cmd = [\n",
    "    \"docker\", \"run\", \"-d\", \"--rm\",\n",
    "    f\"--name={container_name}\",\n",
    "    \"--gpus\", \"all\",\n",
    "    \"-e\", f\"{NGC_API_KEY}\",\n",
    "    \"-v\", f\"{local_nim_cache}:/opt/nim/.cache\",\n",
    "    \"-p\", \"8000:8000\",\n",
    "    ngc_nim_image\n",
    "]\n",
    "\n",
    "print(f\"NIM image {ngc_nim_image} pulled from NGC successfully, running container is\")\n",
    "run_bash_cmd(docker_cmd)\n",
    "\n",
    "# Push NIM image to public AR repository\n",
    "bash_cmd = f\"\"\"\n",
    "    docker tag {ngc_nim_image} {public_nim_image}\n",
    "\n",
    "    docker push {public_nim_image}\n",
    "    \"\"\"\n",
    "\n",
    "run_bash_cmd(bash_cmd)\n",
    "print(f\"NIM image {ngc_nim_image} pushed to Artifact Registry {public_nim_image} successfully\")\n",
    "\n",
    "# Optional\n",
    "# Push NIM image to private AR repository\n",
    "bash_cmd = f\"\"\"\n",
    "    docker tag {public_nim_image} {private_nim_image}\n",
    "\n",
    "    docker push {private_nim_image}\n",
    "    \"\"\"\n",
    "\n",
    "run_bash_cmd(bash_cmd)\n",
    "print(f\"NIM image {public_nim_image} pushed to Artifact Registry {private_nim_image} successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3a6114-4105-4c86-88e9-246d6b43ed09",
   "metadata": {},
   "source": [
    "### Run NIM Container Within Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2932919-6f68-449c-8184-571036a6798e",
   "metadata": {},
   "source": [
    "Run NIM container in **Terminal** or another notebook, keep the container active, then inference with Python OpenAI API or CLI command to get model responses in the Notebook interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3c57f-6388-491b-b42e-a773a32017aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run NIM container\n",
    "! docker run -it --rm --name={container_name} \\\n",
    "  --runtime=nvidia \\\n",
    "  --gpus all \\\n",
    "  --shm-size=16GB \\\n",
    "  -e NGC_API_KEY={NGC_API_KEY} \\\n",
    "  -e NIM_MODEL_PROFILE={selected_profile} \\\n",
    "  -v {local_nim_cache}\":/opt/nim/.cache\" \\\n",
    "  -u $(id -u) \\\n",
    "  -p 8000:8000 \\\n",
    "  {private_nim_image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce051b0f-5d8d-4183-a601-b51781459db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0672d-5c48-4a6b-a47c-9f97c6f30908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker ps \n",
    "! echo \"\"\n",
    "CONTAINER_ID = !docker ps | awk 'NR>1 {print $1}'\n",
    "CONTAINER_ID = CONTAINER_ID[0]\n",
    "! echo 'Running Container is' $CONTAINER_ID\n",
    "! echo 'IP Address'\n",
    "# ! docker inspect $CONTAINER_ID\n",
    "IPAddress= !docker exec $CONTAINER_ID sh -c \"hostname --ip-address\" \n",
    "IPAddress=IPAddress[0]\n",
    "! echo $IPAddress\n",
    "! echo \"\"\n",
    "! echo \"NIM Model and Profile\"\n",
    "! docker inspect $CONTAINER_ID |grep -i model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca20056-db4e-44aa-902a-ce76da8f38fe",
   "metadata": {},
   "source": [
    "#### Make Inference within Interface\n",
    "After running NIM container and keeping it active, we could make inference to model and get response. NIM on Vertex AI Workbench supports both OpenAI Python API and CLI.\n",
    "\n",
    "With the `completions` endpoint, `prompt` could be set as input strings to give instructions to the model, it could also be in the form of `messages` with roles and contents for multi-turn conversation. Other model parameters could adjust output length, temperature, etc. \n",
    "\n",
    "*Note: May need to change IP address of URL when make request (e.g. http://172.18.0.2:8000/v1/completions)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d68c90-d684-4c18-8265-bf56b3e28925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! curl -X 'POST' \\\n",
    "        'http://0.0.0.0:8000/v1/completions' \\\n",
    "        -H 'accept: application/json' \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{ \"model\": \"meta/llama3-8b-instruct\", \\\n",
    "              \"prompt\": \"Once upon a time\",\"max_tokens\": 100}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63ae01-1f27-4205-be64-e1d05f81662a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "prompt = \"Once upon a time\"\n",
    "response = client.completions.create(\n",
    "    model=payload_model,\n",
    "    prompt=prompt,\n",
    "    max_tokens=100,\n",
    "    stream=False\n",
    ")\n",
    "completion = response.choices[0].text\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329905b-ca4f-46ea-8de0-83c433bbc499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! curl -X 'POST' \\\n",
    "    'http://0.0.0.0:8000/v1/chat/completions' \\\n",
    "    -H 'accept: application/json' \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -d '{\"model\": \"meta/llama3-8b-instruct\", \\\n",
    "        \"messages\": [ \\\n",
    "            {\"role\":\"user\", \\\n",
    "            \"content\":\"Hello! How are you?\"}, \\\n",
    "            {\"role\":\"assistant\", \\\n",
    "            \"content\":\"Hi! I am quite well, how can I help you today?\"}, \\\n",
    "            {\"role\":\"user\", \\\n",
    "            \"content\":\"Write a short limerick about the wonders of GPU computing.\"} \\\n",
    "            ], \\\n",
    "        \"max_tokens\": 512 \\\n",
    "        }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022cd9e-dc98-4cb7-ae9c-b5b1136d1546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8000/v1\", api_key=\"not-used\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU computing.\"}\n",
    "]\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=payload_model,\n",
    "    messages=messages,\n",
    "    max_tokens=512,\n",
    "    stream=False\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message\n",
    "print(assistant_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817da080-24b6-46a5-a6c5-4e1ee9aa72cd",
   "metadata": {},
   "source": [
    "Stop NIM container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45275c-e271-4c45-ae3c-e5904547a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker stop $CONTAINER_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c88a7-94b3-407d-ada1-4c049efb92f4",
   "metadata": {},
   "source": [
    "### Endpoint Deployment\n",
    "\n",
    "Then we could proceed to endpoint deloyment, this will allow the model endpoint available on Vertex AI Online Prediction.\n",
    "\n",
    "Steps are as follows:\n",
    "\n",
    "* Upload NIM container as a Vertex AI Model resource.\n",
    "* Create a Vertex AI Endpoint resource.\n",
    "* Deploy the Model resource to the Endpoint resource.\n",
    "* Generate raw prediction requests and get responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VADQagSOfSgN",
   "metadata": {
    "id": "VADQagSOfSgN"
   },
   "source": [
    "#### Upload NIM as a Vertex AI Model resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZR0xiIeGfQmY",
   "metadata": {
    "id": "ZR0xiIeGfQmY"
   },
   "source": [
    "First, we upload the NIM image as a Vertex AI model resource using the `upload()` method, with the following parameters:\n",
    "\n",
    "*  `display_name`: The human readable name for the Model resource.\n",
    "*  `artifact_uri`: The Cloud Storage location of the model artifacts. If the container image includes the model artifacts that you need to serve predictions, there is no need to load files from Cloud Storage.\n",
    "\n",
    "*  `serving_container_image`: The serving container image to use when the model is deployed to a Vertex AI\n",
    "\n",
    "*  `serving_container_command`: The serving binary (HTTP Server) to start up.\n",
    "\n",
    "*  `serving_container_shared_memory_size_mb`: The shared memory is an Inter-process communication (IPC) mechanism that allows multiple processes to access and manipulate a common block of memory. The default shared memory size is 64MB. Model servers such as vLLM or Nvidia Triton, use shared memory to cache internal data during model inferences. Also, because shared memory can be used for cross GPU communication, using more shared memory can improve performance for accelerators without NVLink capabilities (for example, L4), if the model container requires communication across GPUs. NIM generally requires a larger shared memory size than default. \n",
    "\n",
    "*  `serving_container_environment_variables`: The environment variables specify container required settings such as authentication key. \n",
    "\n",
    "*  `serving_container_args`: The arguments to pass to the serving binary. For example:\n",
    "\n",
    "      -- `model_name`: The human readable name to assign to the model.\n",
    "\n",
    "      -- `model_base_name`: Where to store the model artifacts in the container. The Vertex service sets the variable `AIP_STORAGE_URI` to where the service installed the model artifacts in the container.\n",
    "\n",
    "      -- `rest_api_port`: The port to which to send REST based prediction requests. NIM uses `8000`.\n",
    "\n",
    "      -- `port`: The port to which to send gRPC based prediction requests. NIM uses `8000`.\n",
    "\n",
    "*  `serving_container_health_route`: The URL for the service to periodically ping for a response to verify that the serving binary is running. For NIM, this will be `/v1/health/ready`.\n",
    "\n",
    "*  `serving_container_predict_route`: The URL for the service to route REST-based prediction requests to. For NIM, this will be `/v1/chat/completions` or `/v1/completions`.\n",
    "\n",
    "*  `serving_container_ports`: A list of ports for the HTTP server to listen for requests. \n",
    "\n",
    "*  `sync`: Whether to wait for the process to complete, or return immediately (async).\n",
    "\n",
    "Uploading a model into a Vertex Model resource may take a few moments. After completion, model will show up in Vertex AI Model Registry.\n",
    "\n",
    "Reference: [NIM API](https://docs.nvidia.com/nim/large-language-models/latest/api-reference.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HVFcXzZZs1Gp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1721244274689,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "HVFcXzZZs1Gp",
    "outputId": "064db104-e272-456e-d377-39ebbce664f7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core.future.polling import DEFAULT_POLLING\n",
    "from google.cloud.aiplatform import Endpoint, Model\n",
    "DEFAULT_POLLING._timeout = 360000\n",
    "\n",
    "models = Model.list(filter=f'displayName=\"{va_model_name}\"')\n",
    "\n",
    "if models:\n",
    "    model = models[0]\n",
    "else:\n",
    "    model = aiplatform.Model.upload(\n",
    "                display_name=va_model_name,\n",
    "                serving_container_image_uri=private_nim_image,\n",
    "                serving_container_predict_route=\"/v1/chat/completions\",\n",
    "                serving_container_health_route=\"/v1/health/ready\",\n",
    "                serving_container_environment_variables={\"NGC_API_KEY\": NGC_API_KEY, \"PORT\": \"8000\", \"shm-size\":\"16GB\"},\n",
    "                serving_container_shared_memory_size_mb=16000,\n",
    "                serving_container_ports=[8000],\n",
    "                sync=True,\n",
    "            )\n",
    "model.wait()\n",
    "\n",
    "print(\"Model:\")\n",
    "print(f\"\\tDisplay name: {model.display_name}\")\n",
    "print(f\"\\tResource name: {model.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a64dfb-010c-4967-8a10-561e166d5553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud ai models list --region=$region --filter=\"DISPLAY_NAME ~ .*nim.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5214c6-db87-4736-a076-94d6919ef5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = !gcloud ai models list --region=$region --filter=\"DISPLAY_NAME ~ .*nim.*\" | awk 'NR>1 {print $1}'\n",
    "MODEL_ID = MODEL_ID[1]\n",
    "MODEL_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b2276-e633-4d03-8fc0-6256083e9ccc",
   "metadata": {},
   "source": [
    "#### Create a Vertex AI Endpoint resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c60f1-5741-4d72-ba15-b38440ea5d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoints = Endpoint.list(filter=f'displayName=\"{endpoint_name}\"')\n",
    "if endpoints:\n",
    "    endpoint = endpoints[0]\n",
    "else:\n",
    "    print(f\"Endpoint {endpoint_name} doesn't exist, creating...\")\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=endpoint_name)\n",
    "print(\"Endpoint:\")\n",
    "print(f\"\\tDisplay name: {endpoint.display_name}\")\n",
    "print(f\"\\tResource name: {endpoint.resource_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162ce0f-4aa7-4117-b880-d712ba803546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud ai endpoints list --region=$region --filter=\"DISPLAY_NAME ~ .*nim.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682ab40a-3b35-4380-ad1e-5aa123bdbe84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENDPOINT_ID = !gcloud ai endpoints list --region=$region --filter=\"DISPLAY_NAME ~ .*nim.*\" | awk 'NR>1 {print $1}'\n",
    "ENDPOINT_ID = ENDPOINT_ID[1]\n",
    "ENDPOINT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45fc29c-6940-4cec-930e-f3303e5bc804",
   "metadata": {},
   "source": [
    "#### Deploy the Vertex AI model resource to a Vertex AI endpoint resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e8a803-a784-471a-86a0-d989df1ef85e",
   "metadata": {},
   "source": [
    "Next, deploy the Vertex AI model resource to the endpoint resource with the following parameters:\n",
    "\n",
    "* `deploy_model_display`: The human reable name for the deployed model.\n",
    "\n",
    "* `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "    * If only one model, then specify `{ \"0\": 100 }`, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "    * If there are existing models on the endpoint, for which the traffic is split, then use model_id to specify `{ \"0\": percent, model_id: percent, ... }`, where model_id is the ID of an existing deployed model on the endpoint. The percentages must add up to 100.\n",
    "\n",
    "* `machine_type`: The machine type for each VM node instance.\n",
    "\n",
    "* `min_replica_count`: The minimum number of nodes to provision for auto-scaling.\n",
    "\n",
    "* `max_replica_count`: The maximum number of nodes to provision for auto-scaling.\n",
    "\n",
    "* `accelerator_type`: The type, if any, of GPU accelators per provisioned node.\n",
    "\n",
    "* `accelrator_count`: The number, if any, of GPU accelators per provisioned node.\n",
    "\n",
    "After successful deployment, the endpoint and associated deloyed model will be available on Vertex AI Online Prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7j5UICB4DSXZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "executionInfo": {
     "elapsed": 1364148,
     "status": "error",
     "timestamp": 1721245643103,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": 240
    },
    "id": "7j5UICB4DSXZ",
    "outputId": "470f4ccc-2fb6-4c74-9141-21da5c2f56d8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=va_model_name,\n",
    "    traffic_percentage=100,\n",
    "    machine_type=machine_type,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    enable_access_logging=True,\n",
    "    sync=True,\n",
    ")\n",
    "print(f\"Model {model.display_name} deployed at endpoint {endpoint.display_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8rLOVGXHIVbF",
   "metadata": {
    "id": "8rLOVGXHIVbF"
   },
   "outputs": [],
   "source": [
    "print(endpoint.gca_resource)\n",
    "endpoint_name = endpoint.resource_name\n",
    "print(endpoint_name)\n",
    "print(endpoint.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac1e9d1-ddf0-4fdd-b0f0-66f67fa543fb",
   "metadata": {},
   "source": [
    "#### Endpoint Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ed41d-fa6a-4919-8625-a1a45fff2131",
   "metadata": {},
   "source": [
    "Use the Endpoint object's `rawPredict` function to get responses from the deployed model, which takes the following parameters:\n",
    "\n",
    "* `instances`: A list of messages or prompts instances. Each instance should be an array of strings. \n",
    "* `parameters`: A list of LLM model parameteres, e.g. temperature, max_tokens, top_p, stream.\n",
    "\n",
    "NIM on Vertex AI Workbench supports both OpenAI Python API and CLI. Streaming the response on/off option is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11e507-54f9-4204-9422-590117ca6ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU Computing.\"}\n",
    "]\n",
    "\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"temperature\": 0.2,  # Temperature controls the degree of randomness in token selection.\n",
    "  \"max_tokens\": 512,  # Token limit determines the maximum amount of text output.\n",
    "  \"top_p\": 0.8,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "}\n",
    "\n",
    "with open(\"request.json\", \"w\") as outfile: \n",
    "    json.dump(payload, outfile)\n",
    "\n",
    "# Streaming\n",
    "payload_s = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 512,\n",
    "  \"stream\": True\n",
    "}\n",
    "\n",
    "with open(\"request_stream.json\", \"w\") as outfile: \n",
    "    json.dump(payload_s, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be7abd1-1ee0-47a1-8a09-27ac071d7511",
   "metadata": {},
   "source": [
    "Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e331a4fc-988a-4907-9213-fffc4236e1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from google.api import httpbody_pb2\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "http_body = httpbody_pb2.HttpBody(\n",
    "    data=json.dumps(payload).encode(\"utf-8\"),\n",
    "    content_type=\"application/json\",\n",
    ")\n",
    "\n",
    "req = aiplatform_v1.RawPredictRequest(\n",
    "    http_body=http_body, endpoint=endpoint.resource_name\n",
    ")\n",
    "\n",
    "print('Request')\n",
    "print(req)\n",
    "pprint(json.loads(req.http_body.data))\n",
    "print()\n",
    "\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(region)\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "pred_client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "\n",
    "response = pred_client.raw_predict(req)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "print('Response')\n",
    "pprint(json.loads(response.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c6958-7fab-4335-a260-8edaa129ccb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Streaming\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "from google.api import httpbody_pb2\n",
    "from google.cloud import aiplatform_v1\n",
    "\n",
    "http_body = httpbody_pb2.HttpBody(\n",
    "    data=json.dumps(payload_s).encode(\"utf-8\"),\n",
    "    content_type=\"application/json\",\n",
    ")\n",
    "\n",
    "req = aiplatform_v1.RawPredictRequest(\n",
    "    http_body=http_body, endpoint=endpoint.resource_name\n",
    ")\n",
    "\n",
    "print('Request')\n",
    "print(req)\n",
    "pprint(json.loads(req.http_body.data))\n",
    "print()\n",
    "\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(region)\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "pred_client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "\n",
    "response = pred_client.raw_predict(req)\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "print('Response')\n",
    "print(response.data.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2784d83f-6daa-4b69-8602-5fe829960a3f",
   "metadata": {},
   "source": [
    "CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea7b8c-286f-4e1b-b810-431982e57ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! curl \\\n",
    "    --request POST \\\n",
    "    --header \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "    --header \"Content-Type: application/json\" \\\n",
    "    https://us-central1-prediction-aiplatform.googleapis.com/v1/projects/$project_id/locations/$region/endpoints/$ENDPOINT_ID:rawPredict \\\n",
    "    --data \"@request.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36215e80-0353-4744-8cb8-447380b10366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Streaming\n",
    "! curl \\\n",
    "    --request POST \\\n",
    "    --header \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "    --header \"Content-Type: application/json\" \\\n",
    "    https://us-central1-prediction-aiplatform.googleapis.com/v1/projects/$project_id/locations/$region/endpoints/$ENDPOINT_ID:rawPredict \\\n",
    "    --data \"@request_stream.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iMk921f1QKeK",
   "metadata": {
    "id": "iMk921f1QKeK"
   },
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y_YbwDwxQJ1G",
   "metadata": {
    "id": "Y_YbwDwxQJ1G"
   },
   "outputs": [],
   "source": [
    "delete_endpoint = True\n",
    "delete_model = True\n",
    "delete_image = True\n",
    "delete_art_repo = False\n",
    "delete_bucket = False\n",
    "\n",
    "# Undeploy model and delete endpoint\n",
    "try:\n",
    "    if delete_endpoint:\n",
    "        endpoint.undeploy_all(sync=True)\n",
    "        endpoint.delete()\n",
    "        print(f\"Deleted endpoint {endpoint.display_name}\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the model resource\n",
    "try:\n",
    "    if delete_model:\n",
    "        model.delete()\n",
    "        print(f\"Deleted model {model.display_name}\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the container image from Artifact Registry\n",
    "if delete_image:\n",
    "    !gcloud artifacts docker images delete --quiet --delete-tags {private_nim_image}\n",
    "\n",
    "# Delete the Artifact Repository\n",
    "if delete_art_repo:\n",
    "    ! gcloud artifacts repositories delete {private_repository} --location={region} -q\n",
    "\n",
    "# Delete the Cloud Storage bucket\n",
    "if delete_bucket:\n",
    "    ! gsutil rm -rf {bucket_url}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "VertexAI NIM Deployment",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
