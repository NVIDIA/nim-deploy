## @section Deployment parameters
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## @param affinity [object] [default: {}] Affinity settings for deployment. Allows to constraint pods to nodes.
affinity: {}

## @param containerSecurityContext [object] Specify privilege and access control settings for Container(Only affects the main container)
containerSecurityContext: {}
  # seLinuxOptions: null
  # runAsUser: 1001
  # runAsGroup: 1001
  # runAsNonRoot: true
  # privileged: false
  # readOnlyRootFilesystem: false
  # allowPrivilegeEscalation: false
  # capabilities:
  #   drop:
  #     - ALL
  # seccompProfile:
  #   type: "RuntimeDefault"

## @param env [array] Adds arbitrary environment variables to the main container
env: []
# name: OPENBLAS_NUM_THREADS
# value: "1"
# name: POD_NAME
# valueFrom:
#   fieldRef:
#     fieldPath: metadata.name
# name: SERVER_PORT_INTERNAL
# value: "9009"

## @param extraVolumes [object] Adds arbitrary additional volumes to the deployment set definition
extraVolumes: {}
  # my-volume-name:
  #   emptyDir: {}

## @param extraVolumeMounts [object] Specify volume mounts to the main container from extraVolumes
extraVolumeMounts: {}
  # my-volume-name:
  #   mountPath: /mnt/myvolume

## @param image.repository [string] NIM-LLM Image Repository
## @param image.tag [string] Image tag
## @param image.pullPolicy [string] Image pull policy
image:
  # repository: nvcr.io/nim/meta/llama3-8b-instruct
  repository: 
  pullPolicy: IfNotPresent
  # Tag overrides the image tag whose default is the chart appVersion.
  # tag: "1.0.0"

## @extra imagePullSecrets Specify list of secret names that are needed for the main container and any init containers.
## @skip imagePullSecrets[0].name
imagePullSecrets:
  - name: nvcrimagepullsecret  # change this to whatever your image pull secret should be

## @param nodeSelector [object] Specify labels to ensure that NeMo Inference is deployed only on certain nodes (likely best to set this to `nvidia.com/gpu.present: "true"` depending on cluster setup).
nodeSelector: {}  # likely best to set this to `nvidia.com/gpu.present: "true"` depending on cluster setup

## @param podAnnotations [object] Specify additional annotation to the main deployment pods
podAnnotations: {}

## @extra podSecurityContext Specify privilege and access control settings for pod (Only affects the main pod).
## @param podSecurityContext.runAsUser Specify user UID for pod.
## @param podSecurityContext.runAsGroup Specify group ID for pod.
## @param podSecurityContext.fsGroup Specify file system owner group id.
podSecurityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

## @param replicaCount Specify replica count for Job.
replicaCount: 1

## @extra resources [object] Specify resources limits and requests for the running service. 
## @param resources.limits.nvidia.com/gpu Specify number of GPUs to present to the running service.
resources:
  limits:
    nvidia.com/gpu: 1  # Number of GPUs to present to the running service

## @exta serviceAccount Options to specify service account for the deployment.
## @param serviceAccount.create Specifies whether a service account should be created.
## @param serviceAccount.annotations [object] Specifies annotations to be added to the service account.
## @param serviceAccount.name Specify name of the service account to use. If it is not set and create is true, a name is generated using a fullname template.
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

## @section Models parameters
## @param model.nimCache [string] Path to mount writeable storage or pre-filled model cache for the NIM
## @param model.name Specify name of the model in the API (name of the NIM). Mostly used for tests (optional otherwise). This must match the name from `/v1/models` to allow `helm test <release-name>` to work. In legacyCompat, this is required and sets the name of the model in /v1/models
## @param model.ngcAPISecret [string] Name of pre-existing secret with a key named NGC_CLI_API_KEY that contains an API key for NGC model downloads
## @param model.openaiPort Specify Open AI Port.
## @param model.labels [object] Specify extra labels to be added on deployed pods.
## @param model.jsonLogging Turn jsonl logging on or off. Defaults to true.
## @param model.logLevel log level of NIM services. Defaults to INFO
## @section Deprecated and Legacy Model parameters
## @param model.legacyCompat Set `true` to enable compatiblity with pre-release NIM versions prior to 1.0.0.
## @param model.numGpus (deprecated) Specify GPU requirements for the model.
## @param model.subPath (deprecated) Specify path within the model volume to mount if not the root -- default works with ngcInit and persistent volume. (legacyCompat only)
## @param model.modelStorePath (deprecated) Specify location of unpacked model.
model:  # most of these values only matter if not using customCommand
  nimCache: /model-store
  name: my-model # optionsl name of the model in the OpenAI API -- used in `helm test`
  ngcAPISecret: ngc-api
  openaiPort: 8000
  labels: {}  # any extra labels desired on deployed pods
  jsonLogging: true
  logLevel: INFO
  # All options below are deprecated
  legacyCompat: false # enable compatiblity with pre-release NIM versions prior to 24.05
  numGpus: 1 # GPU requirements for the model
  subPath: model-store # legacyCompat only -- path within the model volume to mount if not the root -- default works with ngcInit and persistent volume
  # openai_port: 8005 # deprecated for of openaiPort for backward legacy compatibility
  # nemo_port: 8006  # deprecated for backward legacy compatibility
  modelStorePath: ""  # location of unpacked model
  # numWorkers: 1
  # trtModelName: "trt_llm"  # is not required in most cases -- name of trtllm model
  # tritonModelName: "ensemble"  # not required for NVIDIA trtllm models
  # dataStoreURL: "gateway-api:9009"  # URL of data store service, if used
  # customizationSource: "NDS"  # Type of customization source, can be NDS, SERVICE or DIRECTORY -- optional

## @section Storage parameters
## @extra persistence Specify settings to modify the path `/model-store` if `model.legacyCompat` is enabled else `/.cache` volume where the model is served from.
## @param persistence.enabled Enable persistent volumes.
## @param persistence.existingClaim Secify existing claim. If using existingClaim, run only one replica or use a ReadWriteMany storage setup.
## @param persistence.storageClass [nullable] Specify persistent volume storage class. If set to "-", storageClassName: "", which disables dynamic provisioning. If undefined (the default) or set to null, no storageClassName spec is  set, choosing the default provisioner.
## @param persistence.accessMode Specify accessModes. If using an NFS or similar setup, you can use ReadWriteMany.
## @param persistence.stsPersistentVolumeClaimRetentionPolicy.whenDeleted Specify persistent volume claim retention policy when deleted. Only used with Stateful Set volume templates.
## @param persistence.stsPersistentVolumeClaimRetentionPolicy.whenScaled Specifypersistent volume claim retention policy when scaled. Only used with Stateful Set volume templates.
## @param persistence.size Specify size of claim (e.g. 8Gi).
## @param persistence.annotations [object] Specify annotations to be added to persistent volume.
## @param persistence.csi.driver Specify the CSI driver of the PersistentVolume.
## @param persistence.csi.volumeHandle Specify the volume handle of the PersistentVolume.
## @param persistence.csi.readOnly Specify if the volume should be mounted as readonly.
persistence:
  enabled: false
  existingClaim: ""  # if using existingClaim, run only one replica or use a ReadWriteMany storage setup
  # Persistent Volume Storage Class
  # If defined, storageClassName: <storageClass>
  # If set to "-", storageClassName: "", which disables dynamic provisioning.
  # If undefined (the default) or set to null, no storageClassName spec is
  #   set, choosing the default provisioner.
  storageClass: ""
  accessMode: ReadWriteOnce  # If using an NFS or similar setup, you can use ReadWriteMany
  stsPersistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  size: 50Gi  # size of claim in bytes (e.g. 8Gi)
  annotations: {}
  csi:
    driver: ""
    volumeHandle: ""
    readOnly: false

## @extra hostPath Configures model cache on local disk on the nodes using hostPath -- for special cases. One should investigate and understand the security implications before using this option.
## @param hostPath.enabled Enable hostPath.
## @param hostPath.path Specify path to the local model-store. 
hostPath:
  enabled: false
  path: /model-store  # path to the local model-store

## @extra nfs Configures model cache to sit on shared direct-mounted NFS. NOTE: you cannot set mount options using direct NFS mount to pods without a node-intalled nfsmount.conf. `csi-driver-nfs`` may be better in most cases.
## @param nfs.enabled Enable nfs mount
## @param nfs.path Specify path on NFS server to mount
## @param nfs.server Specify NFS server address
## @param nfs.readOnly Set to true to use readOnly
nfs:
  enabled: false
  server: nfs-server.example.com
  path: /exports
  readOnly: false
