nim-llm:
  image:
    repository: nvcr.io/nim/meta/llama-3.1-8b-instruct
    tag: 1.12.0
  llmModel: meta/llama-3.1-8b-instruct
  model:
    name: meta/llama-3.1-8b-instruct
  env:
  - name: NVIDIA_VISIBLE_DEVICES
    value: "0"
  - name: NIM_LOW_MEMORY_MODE
    value: "1"
  - name: NIM_RELAX_MEM_CONSTRAINTS
    value: "1"
  resources:
    limits:
      nvidia.com/gpu: 0    # no limit

vss:
  applicationSpecs:
    vss-deployment:
      containers:
        vss:
          image:
            pullPolicy: IfNotPresent
            repository: nvcr.io/nvidia/blueprint/vss-engine
            tag: 2.4.1
          env:
          - name: VLM_MODEL_TO_USE
            value: cosmos-reason2 # Or "vllm-compatible" or "cosmos-reason1" or "openai-compat" or "custom"
          - name: LLM_MODEL
            value: meta/llama-3.1-8b-instruct
          - name: MODEL_PATH
            value: "git:https://huggingface.co/nvidia/Cosmos-Reason2-8B"
          - name: NVIDIA_VISIBLE_DEVICES
            value: "0"
          - name: CA_RAG_EMBEDDINGS_DIMENSION
            value: "500"
          - name: VLM_BATCH_SIZE
            value: "256"
          - name: VLLM_GPU_MEMORY_UTILIZATION
            value: "0.4"
          - name: VLM_MAX_MODEL_LEN
            value: "10240"
          - name: DISABLE_GUARDRAILS
            value: "true"
      initContainers:
      - command:
        - sh
        - -c
        - until nc -z -w 2 milvus-milvus-deployment-milvus-service 19530; do echo
          waiting for milvus; sleep 2; done
        image: busybox:1.28
        imagePullPolicy: IfNotPresent
        name: check-milvus-up
      - command:
        - sh
        - -c
        - until nc -z -w 2 neo-4-j-service 7687; do echo waiting for neo4j; sleep
          2; done
        image: busybox:1.28
        imagePullPolicy: IfNotPresent
        name: check-neo4j-up
      - args:
        - "while ! curl -s -f -o /dev/null http://nemo-embedding-embedding-deployment-embedding-service:8000/v1/health/live;\
          \ do\n  echo \"Waiting for nemo-embedding...\"\n  sleep 2\ndone\n"
        command:
        - sh
        - -c
        image: curlimages/curl:latest
        imagePullPolicy: IfNotPresent
        name: check-nemo-embed-up
      - args:
        - "while ! curl -s -f -o /dev/null http://nemo-rerank-ranking-deployment-ranking-service:8000/v1/health/live;\
          \ do\n  echo \"Waiting for nemo-rerank...\"\n  sleep 2\ndone\n"
        command:
        - sh
        - -c
        image: curlimages/curl:latest
        imagePullPolicy: IfNotPresent
        name: check-nemo-rerank-up
      - args:
        - "while ! curl -s -f -o /dev/null http://llm-nim-svc:8000/v1/health/live;\
          \ do\n  echo \"Waiting for LLM...\"\n  sleep 2\ndone\n"
        command:
        - sh
        - -c
        image: curlimages/curl:latest
        name: check-llm-up
  llmModel: meta/llama-3.1-8b-instruct
  llmModelChat: meta/llama-3.1-8b-instruct
  configs:
    ca_rag_config.yaml:
      tools:
        summarization_llm:
          type: llm
          params:
            model: meta/llama-3.1-8b-instruct
        chat_llm:
          type: llm
          params:
            model: meta/llama-3.1-8b-instruct
        notification_llm:
          type: llm
          params:
            model: meta/llama-3.1-8b-instruct
    guardrails_config.yaml:
      models:
      - engine: nim
        model: meta/llama-3.1-8b-instruct
        parameters:
          base_url: http://llm-nim-svc:8000/v1
        type: main
      - engine: nim
        model: nvidia/llama-3.2-nv-embedqa-1b-v2
        parameters:
          base_url: http://nemo-embedding-embedding-deployment-embedding-service:8000/v1
        type: embeddings
  resources:
    limits:
      nvidia.com/gpu: 0    # no limit


nemo-embedding:
  applicationSpecs:
    embedding-deployment:
      containers:
        embedding-container:
          env:
          - name: NVIDIA_VISIBLE_DEVICES
            value: '0'
          - name: NIM_MODEL_PROFILE
            value: "f7391ddbcb95b2406853526b8e489fedf20083a2420563ca3e65358ff417b10f" # model profile: fp16-onnx-onnx
  resources:
    limits:
      nvidia.com/gpu: 0    # no limit

nemo-rerank:
  applicationSpecs:
    ranking-deployment:
      containers:
        ranking-container:
          env:
          - name: NVIDIA_VISIBLE_DEVICES
            value: '0'
          - name: NIM_MODEL_PROFILE
            value: "f7391ddbcb95b2406853526b8e489fedf20083a2420563ca3e65358ff417b10f" # model profile: fp16-onnx-onnx
  resources:
    limits:
      nvidia.com/gpu: 0    # no limit
