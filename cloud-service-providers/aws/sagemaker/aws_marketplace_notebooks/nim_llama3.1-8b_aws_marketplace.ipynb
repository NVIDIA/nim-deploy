{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy NVIDIA NIM from AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "NVIDIA NIM, a component of NVIDIA AI Enterprise, enhances your applications with the power of state-of-the-art large language models (LLMs), providing unmatched natural language processing and understanding capabilities. Whether you're developing chatbots, content analyzers, or any application that needs to understand and generate human language, NVIDIA NIM for LLMs has you covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy the Llama 3.1 8B-Instruct NIM from AWS Marketplace on Amazon SageMaker. The Llama 3.1 8B-Instruct NIM simplifies the deployment of the Llama 3.1 8B instruction tuned model which is optimized for language understanding, reasoning, and text generation use cases, and outperforms many of the available open source chat models on common industry benchmarks. The NIM is built on robust foundations including inference engines like Triton Inference Server, TensorRT, TensorRT-LLM, and PyTorch. NIM provides features like low latency, high throughput, function calling, metrics export, standard API, optimized profiles & enterprise support. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1f3df-a66e-490e-b4dc-7aa7b3a0ed6e",
   "metadata": {},
   "source": [
    "Please check out the [NIM LLM docs](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a3631-f207-4803-98ed-45d6895aa80a",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to one of the models listed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5004eb9a-5817-484c-a412-2ce80a2e2f36",
   "metadata": {},
   "source": [
    "## Subscribe to the model package\n",
    "To subscribe to the model package:\n",
    "1. Open the model package listing page\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fcdc9-3dbd-4bff-8e49-40783a5a2b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time, os\n",
    "from sagemaker import get_execution_role, ModelPackage\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82963-2d0b-4fb6-8ef4-a0e5f6a15c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the arn below with the model package arn you want to deploy\n",
    "nim_package = \"llama-v3-1-8b-instruct-nim-010de076f8a135988815e932e01b02f6\"\n",
    "\n",
    "# Mapping for Model Packages\n",
    "model_package_map = {\n",
    "    \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{nim_package}\",\n",
    "    \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{nim_package}\",\n",
    "    \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{nim_package}\",\n",
    "    \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{nim_package}\",\n",
    "    \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{nim_package}\",\n",
    "    \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{nim_package}\",\n",
    "    \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{nim_package}\",\n",
    "    \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{nim_package}\",\n",
    "    \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{nim_package}\",\n",
    "    \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{nim_package}\",\n",
    "    \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{nim_package}\",\n",
    "    \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{nim_package}\",\n",
    "    \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{nim_package}\",\n",
    "    \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{nim_package}\",\n",
    "    \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{nim_package}\",\n",
    "    \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{nim_package}\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "model_package_arn = model_package_map[region]\n",
    "model_package_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8132cd1-6055-4e45-9a92-1e74726ed61b",
   "metadata": {},
   "source": [
    "## Create the SageMaker Endpoint\n",
    "\n",
    "We first define SageMaker model using the specified ModelPackageArn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee9475-8611-41d1-8c29-5df33f66a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model details\n",
    "sm_model_name = \"nim-llama-8b\"\n",
    "\n",
    "# Create the SageMaker model\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name,\n",
    "    PrimaryContainer={\n",
    "        'ModelPackageName': model_package_arn\n",
    "    },\n",
    "    ExecutionRoleArn=role,\n",
    "    EnableNetworkIsolation=True\n",
    ")\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e096db-0e25-467b-aa9c-18a6a4f1ca60",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we create endpoint configuration specifying instance type, in this case it's g5.12xlarge or g5.24xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bb1b0-d5a6-439d-aa32-01f3fb0953a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the endpoint configuration\n",
    "endpoint_config_name = sm_model_name\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': sm_model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.g5.12xlarge', # or g5.24xlarge\n",
    "            'InferenceAmiVersion': 'al2-ami-sagemaker-inference-gpu-2',\n",
    "            'RoutingConfig': {'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'},\n",
    "            'ModelDataDownloadTimeoutInSeconds': 3600, # Specify the model download timeout in seconds.\n",
    "            'ContainerStartupHealthCheckTimeoutInSeconds': 3600, # Specify the health checkup timeout in seconds\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065575f-2ad7-438e-a5be-0e80c995414c",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28fcac3-4c9d-420e-87fe-7d170b752738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the endpoint\n",
    "endpoint_name = endpoint_config_name\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbe400-99f0-4888-a71e-00259a86fc17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the model deployed we can use a sample text to do a chat completion inference request using json as the payload format. For inference request format, currently NIM on SageMaker supports the OpenAI API chat completions inference protocol. For explanation of supported parameters please see [this link](https://platform.openai.com/docs/api-reference/chat). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload_model = \"meta/llama-3.1-8b-instruct\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU Computing.\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21b99b-ce34-4375-9bc7-d7e8ef80f262",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Try streaming inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b89469-1f88-45d4-b103-ce2df4232037",
   "metadata": {},
   "source": [
    "NIM on SageMaker also supports streaming inference and you can enable that by setting **`\"stream\"` as `True`** in the payload and by using [`invoke_endpoint_with_response_stream`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint_with_response_stream.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71632bf8-5297-45db-9a92-a4a371b7da26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU Computing.\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 100,\n",
    "  \"stream\": True\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9d24a-a23e-40aa-afdb-da2a63f624fe",
   "metadata": {},
   "source": [
    "We have some postprocessing code for the streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05742b6-f8e4-4116-92ed-e9b156b6cfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_stream = response['Body']\n",
    "accumulated_data = \"\"\n",
    "start_marker = 'data:'\n",
    "end_marker = '\"finish_reason\":null}]}'\n",
    "\n",
    "for event in event_stream:\n",
    "    try:\n",
    "        payload = event.get('PayloadPart', {}).get('Bytes', b'')\n",
    "        if payload:\n",
    "            data_str = payload.decode('utf-8')\n",
    "\n",
    "            accumulated_data += data_str\n",
    "\n",
    "            # Process accumulated data when a complete response is detected\n",
    "            while start_marker in accumulated_data and end_marker in accumulated_data:\n",
    "                start_idx = accumulated_data.find(start_marker)\n",
    "                end_idx = accumulated_data.find(end_marker) + len(end_marker)\n",
    "                full_response = accumulated_data[start_idx + len(start_marker):end_idx]\n",
    "                accumulated_data = accumulated_data[end_idx:]\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(full_response)\n",
    "                    content = data.get('choices', [{}])[0].get('delta', {}).get('content', \"\")\n",
    "                    if content:\n",
    "                        print(content, end='', flush=True)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing event: {e}\", flush=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
