{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy NVIDIA NIM from AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "NVIDIA NIM, a component of NVIDIA AI Enterprise, enhances your applications with the power of state-of-the-art large language models (LLMs), providing unmatched natural language processing and understanding capabilities. Whether you're developing chatbots, content analyzers, or any application that needs to understand and generate human language, NVIDIA NIM for LLMs has you covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy the NVIDIA Nemotron Parse VLM model from AWS Marketplace\n",
    "\n",
    "nemotron-parse is a general purpose text-extraction model, specifically designed to handle documents. Given an image, nemotron-parse is able to extract formatted-text, with bounding-boxes and the corresponding semantic class. This has downstream benefits for several tasks such as increasing the availability of training-data for Large Language Models (LLMs), improving the accuracy of retriever systems, and enhancing document understanding pipelines.\n",
    "\n",
    "#### Architecture Type :\n",
    "\n",
    "-   Transformer-based vision-encoder-decoder model\n",
    "\n",
    "\n",
    "#### Network Architecture :\n",
    "-   **Vision Encoder:** ViT-H model (https://huggingface.co/nvidia/C-RADIO)\n",
    "-   **Adapter Layer:** 1D convolutions & norms to compress dimensionality and sequence length of the latent space (1280 tokens to 320 tokens)\n",
    "-   **Decoder:** mBart [1] 10 blocks\n",
    "-   **Tokenizer:** Use of the tokenizer included in this model is governed by the CC-BY-4.0 license\n",
    "\n",
    "\n",
    "#### Input :\n",
    "\n",
    "<div align=\"left\">\n",
    "\n",
    "| Property | Value |\n",
    "| :--- | :--- |\n",
    "| **Input Type** | Image, Text |\n",
    "| **Input Type(s)** | Red, Green, Blue (RGB) + Prompt (String) |\n",
    "| **Input Parameters** | 2D, 1D |\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Other Properties Related to Input :\n",
    "\n",
    "<div align=\"left\">\n",
    "\n",
    "| Property | Value |\n",
    "| :--- | :--- |\n",
    "| **Max Input Resolution (W, H)** | 1648, 2048 |\n",
    "| **Min Input Resolution (W, H)** | 1024, 1280 |\n",
    "| **Channel Count** | 3 |\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Output :\n",
    "\n",
    "<div align=\"left\">\n",
    "\n",
    "| Property | Value |\n",
    "| :--- | :--- |\n",
    "| **Output Type** | Text |\n",
    "| **Output Format** | String |\n",
    "| **Output Parameters** | 1D |\n",
    "\n",
    "</div>\n",
    "\n",
    "nemotron-parse output format is a string which encodes text content (formatted or not) as well as bounding boxes and class attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1f3df-a66e-490e-b4dc-7aa7b3a0ed6e",
   "metadata": {},
   "source": [
    "Please check out the [NIM VLM docs](https://docs.nvidia.com/nim/vision-language-models/1.4.0/) and [nemotron parse docs](https://docs.nvidia.com/nim/vision-language-models/1.2.0/examples/retriever/api.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c8cfe",
   "metadata": {},
   "source": [
    "## ⚠️ Disclaimer\n",
    "\n",
    "When the request requires longer inference times, it may exceed the default 60-second timeout limit for [**AWS SageMaker's non-streaming endpoints**](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html). This notebook shows examples for both the streaming and non-streaming endpoints.\n",
    "\n",
    "To avoid inference failures due to timeout:\n",
    "- It is **recommended** to use the **SageMaker streaming endpoint** for this model as shown in the [Streaming inference](#Streaming-inference) section of this notebook.\n",
    "- If your use case **requires** using a **non-streaming endpoint** as shown in the [Run Inference](#Run-Inference) section of this notebook, you must first contact **AWS Support** to request an increased timeout limit for your **AWS Account and Region** to avoid unexpected errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a3631-f207-4803-98ed-45d6895aa80a",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to one of the models listed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5004eb9a-5817-484c-a412-2ce80a2e2f36",
   "metadata": {},
   "source": [
    "## Subscribe to the model package\n",
    "To subscribe to the model package:\n",
    "1. Open the model package listing page\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fcdc9-3dbd-4bff-8e49-40783a5a2b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time, os\n",
    "from sagemaker import get_execution_role, ModelPackage\n",
    "from botocore.config import Config\n",
    "\n",
    "config = Config(read_timeout=3600)\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\", config=config)\n",
    "region = sess.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82963-2d0b-4fb6-8ef4-a0e5f6a15c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the arn below with the model package arn you want to deploy\n",
    "nim_package = \"\"\n",
    "\n",
    "# Mapping for Model Packages\n",
    "model_package_map = {\n",
    "    \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{nim_package}\",\n",
    "    \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{nim_package}\",\n",
    "    \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{nim_package}\",\n",
    "    \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{nim_package}\",\n",
    "    \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{nim_package}\",\n",
    "    \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{nim_package}\",\n",
    "    \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{nim_package}\",\n",
    "    \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{nim_package}\",\n",
    "    \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{nim_package}\",\n",
    "    \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{nim_package}\",\n",
    "    \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{nim_package}\",\n",
    "    \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{nim_package}\",\n",
    "    \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{nim_package}\",\n",
    "    \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{nim_package}\",\n",
    "    \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{nim_package}\",\n",
    "    \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{nim_package}\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "model_package_arn = model_package_map[region]\n",
    "model_package_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8132cd1-6055-4e45-9a92-1e74726ed61b",
   "metadata": {},
   "source": [
    "## Create the SageMaker Endpoint\n",
    "\n",
    "We first define SageMaker model using the specified ModelPackageArn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee9475-8611-41d1-8c29-5df33f66a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model details\n",
    "sm_model_name = \"nemotron-parse\"\n",
    "\n",
    "# Create the SageMaker model\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name,\n",
    "    PrimaryContainer={\n",
    "        'ModelPackageName': model_package_arn\n",
    "    },\n",
    "    ExecutionRoleArn=role,\n",
    "    EnableNetworkIsolation=True\n",
    ")\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e096db-0e25-467b-aa9c-18a6a4f1ca60",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we create endpoint configuration specifying instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bb1b0-d5a6-439d-aa32-01f3fb0953a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the endpoint configuration\n",
    "endpoint_config_name = sm_model_name\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': sm_model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.g5.2xlarge', \n",
    "            'InferenceAmiVersion': 'al2-ami-sagemaker-inference-gpu-2',\n",
    "            'RoutingConfig': {'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'},\n",
    "            'ModelDataDownloadTimeoutInSeconds': 3600, # Specify the model download timeout in seconds.\n",
    "            'ContainerStartupHealthCheckTimeoutInSeconds': 3600, # Specify the health checkup timeout in seconds\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065575f-2ad7-438e-a5be-0e80c995414c",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28fcac3-4c9d-420e-87fe-7d170b752738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the endpoint\n",
    "endpoint_name = endpoint_config_name\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbe400-99f0-4888-a71e-00259a86fc17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the model deployed we can use a sample text to do an inference request. For inference request format, currently NIM on SageMaker supports the OpenAI API inference protocol. For explanation of supported parameters please see [this link](https://docs.api.nvidia.com/nim/reference/nvidia-llama-3_3-nemotron-super-49b-v1-infer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61480710-ba0f-4e39-8c41-8faaf83cf0a3",
   "metadata": {},
   "source": [
    "#### Streaming inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05852b",
   "metadata": {},
   "source": [
    "NIM on SageMaker also supports streaming inference and you can enable that by setting **`\"stream\"` as `True`** in the payload and by using [`invoke_endpoint_with_response_stream`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint_with_response_stream.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bbbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import mimetypes\n",
    "from pathlib import Path\n",
    "\n",
    "def encode_file_to_base64(file_path):\n",
    "    \"\"\"Encode a local media file to base64 for airgapped environment\"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Guess the MIME type based on the file extension\n",
    "    mime_type, _ = mimetypes.guess_type(file_path)\n",
    "    if mime_type is None:\n",
    "        raise ValueError(f\"Could not determine MIME type for file: {file_path}\")\n",
    "    \n",
    "    with open(file_path, \"rb\") as f:\n",
    "        encoded_string = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        return f\"data:{mime_type};base64,{encoded_string}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_filename = 'LibreOffice_Writer_6.3.png'  # Change to your local image file if run inference with local files\n",
    "image_data_url = encode_file_to_base64(image_filename)\n",
    "\n",
    "payload_model = \"nvidia/nemotron-parse\"\n",
    "messages = [\n",
    "    {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\n",
    "                      \"url\": image_data_url\n",
    "                  }\n",
    "              }\n",
    "          ]\n",
    "      }\n",
    "  ]\n",
    "\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 2000,\n",
    "  \"stream\": True\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0edce33",
   "metadata": {},
   "source": [
    "We have some postprocessing code for the streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stream = response['Body']\n",
    "accumulated_bytes = b''\n",
    "stream_finished = False # flag to track termination\n",
    "\n",
    "for event in event_stream:\n",
    "    if stream_finished:\n",
    "        break \n",
    "\n",
    "    try:\n",
    "        payload_part = event.get('PayloadPart', {})\n",
    "        if 'Bytes' in payload_part:\n",
    "            accumulated_bytes += payload_part['Bytes']\n",
    "        \n",
    "        decoded_data = accumulated_bytes.decode('utf-8-sig', errors='ignore')\n",
    "    \n",
    "        parts = decoded_data.rpartition('\\n')\n",
    "        lines_to_process = parts[0]\n",
    "        accumulated_bytes = parts[2].encode('utf-8', errors='ignore')\n",
    "        \n",
    "        for line in lines_to_process.split('\\n'):\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            if line == \"data: [DONE]\":\n",
    "                print(\"\\n\") \n",
    "                stream_finished = True \n",
    "                break\n",
    "            \n",
    "            if line.startswith('data:'):\n",
    "                json_str = line[len('data:'):].strip()\n",
    "                \n",
    "                if json_str:\n",
    "                    try:\n",
    "                        data = json.loads(json_str)\n",
    "                        \n",
    "                        content = data.get('choices', [{}])[0].get('delta', {}).get('content', \"\")\n",
    "                        \n",
    "                        # Print only the generated content token\n",
    "                        if content:\n",
    "                            print(content, end='', flush=True)\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR: CRITICAL] -> Error processing stream: {e}\", flush=True)\n",
    "        break\n",
    "\n",
    "print(\"\\n[STREAM END: LOOP COMPLETED]\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b543e96-7817-44d6-88cb-248a5d6af1e6",
   "metadata": {},
   "source": [
    "#### Non Streaming Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5b62a-4405-40d1-8c83-79673a54259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = 'LibreOffice_Writer_6.3.png'  # Change to your local image file if run inference with local files\n",
    "image_data_url = encode_file_to_base64(image_filename)\n",
    "\n",
    "payload_model = \"nvidia/nemotron-parse\"\n",
    "messages = [\n",
    "    {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\n",
    "                      \"url\": image_data_url\n",
    "                  }\n",
    "              }\n",
    "          ]\n",
    "      }\n",
    "  ]\n",
    "\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\", \n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
