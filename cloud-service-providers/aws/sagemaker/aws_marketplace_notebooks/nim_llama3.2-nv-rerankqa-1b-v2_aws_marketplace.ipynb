{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy NVIDIA NIM from AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "NVIDIA NIM, a component of NVIDIA AI Enterprise, enhances your applications with the power of state-of-the-art large language models (LLMs), providing unmatched natural language processing and understanding capabilities. Whether you're developing chatbots, content analyzers, or any application that needs to understand and generate human language, NVIDIA NIM for LLMs has you covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy the NVIDIA NeMo Retriever Llama3.2 reranking model from AWS Marketplace on Amazon SageMaker. The NVIDIA NeMo Retriever Llama3.2 reranking model is optimized for providing a logit score that represents how relevant a document(s) is to a given query. The model was fine-tuned for multilingual, cross-lingual text question-answering retrieval, with support for long documents (up to 8192 tokens). This model was evaluated on 26 languages: English, Arabic, Bengali, Chinese, Czech, Danish, Dutch, Finnish, French, German, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, and Turkish. The reranking model is a component in a text retrieval system to improve the overall accuracy. A text retrieval system often uses an embedding model (dense) or lexical search (sparse) index to return relevant text passages given the input. A reranking model can be used to rerank the potential candidate into a final order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1f3df-a66e-490e-b4dc-7aa7b3a0ed6e",
   "metadata": {},
   "source": [
    "Please check out the [NIM LLM docs](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a3631-f207-4803-98ed-45d6895aa80a",
   "metadata": {},
   "source": [
    "## Pre-requisites:\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to one of the models listed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5004eb9a-5817-484c-a412-2ce80a2e2f36",
   "metadata": {},
   "source": [
    "## Subscribe to the model package\n",
    "To subscribe to the model package:\n",
    "1. Open the model package listing page\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fcdc9-3dbd-4bff-8e49-40783a5a2b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time, os\n",
    "from sagemaker import get_execution_role, ModelPackage\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc82963-2d0b-4fb6-8ef4-a0e5f6a15c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the arn below with the model package arn you want to deploy\n",
    "nim_package = \"llama-3-2-nv-rerankqa-1b-v2-ni-0b2ddc90885430e08b9e11120d2522a0\"\n",
    "\n",
    "# Mapping for Model Packages\n",
    "model_package_map = {\n",
    "    \"us-east-1\": f\"arn:aws:sagemaker:us-east-1:865070037744:model-package/{nim_package}\",\n",
    "    \"us-east-2\": f\"arn:aws:sagemaker:us-east-2:057799348421:model-package/{nim_package}\",\n",
    "    \"us-west-1\": f\"arn:aws:sagemaker:us-west-1:382657785993:model-package/{nim_package}\",\n",
    "    \"us-west-2\": f\"arn:aws:sagemaker:us-west-2:594846645681:model-package/{nim_package}\",\n",
    "    \"ca-central-1\": f\"arn:aws:sagemaker:ca-central-1:470592106596:model-package/{nim_package}\",\n",
    "    \"eu-central-1\": f\"arn:aws:sagemaker:eu-central-1:446921602837:model-package/{nim_package}\",\n",
    "    \"eu-west-1\": f\"arn:aws:sagemaker:eu-west-1:985815980388:model-package/{nim_package}\",\n",
    "    \"eu-west-2\": f\"arn:aws:sagemaker:eu-west-2:856760150666:model-package/{nim_package}\",\n",
    "    \"eu-west-3\": f\"arn:aws:sagemaker:eu-west-3:843114510376:model-package/{nim_package}\",\n",
    "    \"eu-north-1\": f\"arn:aws:sagemaker:eu-north-1:136758871317:model-package/{nim_package}\",\n",
    "    \"ap-southeast-1\": f\"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/{nim_package}\",\n",
    "    \"ap-southeast-2\": f\"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/{nim_package}\",\n",
    "    \"ap-northeast-2\": f\"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/{nim_package}\",\n",
    "    \"ap-northeast-1\": f\"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/{nim_package}\",\n",
    "    \"ap-south-1\": f\"arn:aws:sagemaker:ap-south-1:077584701553:model-package/{nim_package}\",\n",
    "    \"sa-east-1\": f\"arn:aws:sagemaker:sa-east-1:270155090741:model-package/{nim_package}\",\n",
    "}\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise Exception(f\"Current boto3 session region {region} is not supported.\")\n",
    "\n",
    "model_package_arn = model_package_map[region]\n",
    "model_package_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8132cd1-6055-4e45-9a92-1e74726ed61b",
   "metadata": {},
   "source": [
    "## Create the SageMaker Endpoint\n",
    "\n",
    "We first define SageMaker model using the specified ModelPackageArn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee9475-8611-41d1-8c29-5df33f66a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model details\n",
    "sm_model_name = \"nim-llama-3-2-nv-rerankqa-1b-v2\"\n",
    "\n",
    "# Create the SageMaker model\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name,\n",
    "    PrimaryContainer={\n",
    "        'ModelPackageName': model_package_arn\n",
    "    },\n",
    "    ExecutionRoleArn=role,\n",
    "    EnableNetworkIsolation=True\n",
    ")\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e096db-0e25-467b-aa9c-18a6a4f1ca60",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we create endpoint configuration specifying instance type, in this case it's g5.12xlarge or g5.24xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002bb1b0-d5a6-439d-aa32-01f3fb0953a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the endpoint configuration\n",
    "endpoint_config_name = sm_model_name\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'AllTraffic',\n",
    "            'ModelName': sm_model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': 'ml.g5.12xlarge', # or g5.24xlarge\n",
    "            'InferenceAmiVersion': 'al2-ami-sagemaker-inference-gpu-2',\n",
    "            'RoutingConfig': {'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'},\n",
    "            'ModelDataDownloadTimeoutInSeconds': 3600, # Specify the model download timeout in seconds.\n",
    "            'ContainerStartupHealthCheckTimeoutInSeconds': 3600, # Specify the health checkup timeout in seconds\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065575f-2ad7-438e-a5be-0e80c995414c",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28fcac3-4c9d-420e-87fe-7d170b752738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the endpoint\n",
    "endpoint_name = endpoint_config_name\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbe400-99f0-4888-a71e-00259a86fc17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the model deployed we can use a sample text to do an inference request. For inference request format and supported parameters please see [this link](https://docs.api.nvidia.com/nim/reference/nvidia-llama-3_2-nv-rerankqa-1b-v1-infer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload_model = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n",
    "query = {\"text\": \"which way did the traveler go?\"}\n",
    "messages = [\n",
    "    {\"text\": \"two roads diverged in a yellow wood, and sorry i could not travel both and be one traveler, long i stood and looked down one as far as i could to where it bent in the undergrowth;\"},\n",
    "    {\"text\": \"then took the other, as just as fair, and having perhaps the better claim because it was grassy and wanted wear, though as for that the passing there had worn them really about the same,\"},\n",
    "    {\"text\": \"and both that morning equally lay in leaves no step had trodden black. oh, i marked the first for another day! yet knowing how way leads on to way i doubted if i should ever come back.\"},\n",
    "    {\"text\": \"i shall be telling this with a sigh somewhere ages and ages hense: two roads diverged in a wood, and i, i took the one less traveled by, and that has made all the difference.\"}\n",
    "  ]\n",
    "\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"query\": query,\n",
    "  \"passages\": messages,\n",
    "  \"truncate\": \"END\"\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
