{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy NVIDIA NIM on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "NVIDIA NIM, a component of NVIDIA AI Enterprise, enhances your applications with the power of state-of-the-art large language models (LLMs), providing unmatched natural language processing and understanding capabilities. Whether you're developing chatbots, content analyzers, or any application that needs to understand and generate human language, NVIDIA NIM for LLMs has you covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy `LLaMa-3 70B` on a p4d instance or `LLaMa-3 8B` on a g5 instance with NIM on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1599941-1c76-4352-b1c3-eca6f4a65aaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>IMPORTANT:</b> To run NIM on SageMaker you will need to have your NGC API KEY because it's required to access NGC resources. Check out <a href=\"https://build.nvidia.com/meta/llama3-70b?signin=true\"> this LINK</a> to learn how to get NGC API KEY. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1f3df-a66e-490e-b4dc-7aa7b3a0ed6e",
   "metadata": {},
   "source": [
    "Please check out the [NIM LLM docs](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383fca-0ffb-45f9-a6cf-1849d117a386",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a50f-24d5-4778-a02d-28efc31373b7",
   "metadata": {},
   "source": [
    "Installs the dependencies and setup roles required to package the model and create SageMaker endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578a7de-7ed3-4105-bec7-e5d3b04cd4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time, os\n",
    "from sagemaker import get_execution_role\n",
    "from pathlib import Path\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.region_name\n",
    "sts_client = sess.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7acbd-edad-4e3e-9386-1e587879b2a5",
   "metadata": {},
   "source": [
    "### Define Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcee757",
   "metadata": {},
   "source": [
    "Examples are provided below for both `llama3-70b-instruct` and `llama3-8b-instruct` NIMs. Remove the model you do **not** want to deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f64791-1c45-4b84-9b1a-1e3ebc60d2de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# llama3-70b-instruct\n",
    "public_nim_image = \"public.ecr.aws/nvidia/nim:llama3-70b-instruct-1.0.0\"\n",
    "nim_model = \"nim-llama3-70b-instruct\"\n",
    "sm_model_name = \"nim-llama3-70b-instruct\"\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "payload_model = \"meta/llama3-70b-instruct\"\n",
    "\n",
    "# llama3-8b-instruct\n",
    "public_nim_image = \"public.ecr.aws/nvidia/nim:llama3-8b-instruct-1.0.0\"\n",
    "nim_model = \"nim-llama3-8b-instruct\"\n",
    "sm_model_name = \"nim-llama3-8b-instruct\"\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "payload_model = \"meta/llama3-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb05462",
   "metadata": {},
   "source": [
    "### NIM Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851abe8-9ca4-403b-be7f-aef56dfa4b9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We first pull the NIM image from public ECR and then push it to private ECR repo within your account for deploying on SageMaker endpoint. Note:\n",
    "  - NIM ECR image is currently available only in `us-east-1` region\n",
    "  - You must have `ecr:CreateRepository` and appropriate push permissions associated with your execution role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944110e5-15bc-4a94-a731-7d4ea9344e9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get AWS account ID\n",
    "result = subprocess.run(['aws', 'sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error getting AWS account ID: {result.stderr}\")\n",
    "else:\n",
    "    account = result.stdout.strip()\n",
    "    print(f\"AWS account ID: {account}\")\n",
    "\n",
    "bash_script = f\"\"\"\n",
    "echo \"Public NIM Image: {public_nim_image}\"\n",
    "docker pull {public_nim_image}\n",
    "\n",
    "\n",
    "echo \"Resolved account: {account}\"\n",
    "echo \"Resolved region: {region}\"\n",
    "\n",
    "nim_image=\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "\n",
    "# Ensure the repository name adheres to AWS constraints\n",
    "repository_name=$(echo \"{nim_model}\" | tr '[:upper:]' '[:lower:]' | tr -cd '[:alnum:]._/-')\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"$repository_name\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"$repository_name\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin \"{account}.dkr.ecr.{region}.amazonaws.com\"\n",
    "\n",
    "docker tag {public_nim_image} $nim_image\n",
    "docker push $nim_image\n",
    "echo -n $nim_image\n",
    "\"\"\"\n",
    "nim_image=f\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "# Run the bash script and capture real-time output\n",
    "process = subprocess.Popen(bash_script, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == b'' and process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.decode().strip())\n",
    "\n",
    "stderr = process.stderr.read().decode()\n",
    "if stderr:\n",
    "    print(\"Errors:\", stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18863fd4-0893-4022-9ab0-38e1af1512d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "We print the private ECR NIM image in your account that we will be using for SageMaker deployment. \n",
    "- Should be similar to  `\"<ACCOUNT ID>.dkr.ecr.<REGION>.amazonaws.com/<NIM_MODEL>:latest\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb63ac-2a7a-4beb-b0dd-77a4473e1a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(nim_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518de4e-dcad-4944-9025-484878edb00b",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9efc86-0cf2-403b-9502-fd294acb4cb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Before proceeding further, please set your NGC API Key.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1f264-ebd8-4c6a-9926-7a21afd89ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SET ME\n",
    "NGC_API_KEY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9b93a-3fdf-49a4-8f89-494238008a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert NGC_API_KEY is not None, \"NGC API KEY is not set. Please set the NGC_API_KEY variable. It's required for running NIM.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5bce6-3807-43c3-866f-80543cfdedbf",
   "metadata": {},
   "source": [
    "We define sagemaker model from the NIM container making sure to pass in **NGC_API_KEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b784149-2ec3-4e29-a7cf-3636843dee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": nim_image,\n",
    "    \"Environment\": {\"NGC_API_KEY\": NGC_API_KEY}\n",
    "}\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2f0e6-0377-4a13-9cc3-c345adf08c86",
   "metadata": {},
   "source": [
    "Next we create endpoint configuration, here we are deploying the LLama3-70B model on the specified instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af8b7c-9347-4203-aea5-f44392449f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = sm_model_name\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 1800,\n",
    "            \"InferenceAmiVersion\": \"al2-ami-sagemaker-inference-gpu-2\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51121a-a662-4078-a0c6-b163cda0a718",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75add3d0-100f-4740-b326-6f54af7e9c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = sm_model_name\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d4bc4-b77b-4137-930e-7517295a041c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the endpoint's status as `InService` we can use a sample text to do a chat completion inference request using json as the payload format. For inference request format, currently NIM on SageMaker supports the OpenAI API chat completions inference protocol. For explanation of supported parameters please see [this link](https://platform.openai.com/docs/api-reference/chat). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d36583-d6b0-4fdf-a659-c088f913034a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>IMPORTANT:</b> Model name in inference request payload needs to be the name of NIM model. Please DON'T change it below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57265e9-98bb-4255-ad7d-143e3aeaf9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU Computing.\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21b99b-ce34-4375-9bc7-d7e8ef80f262",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Try streaming inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b89469-1f88-45d4-b103-ce2df4232037",
   "metadata": {},
   "source": [
    "NIM on SageMaker also supports streaming inference and you can enable that by setting **`\"stream\"` as `True`** in the payload and by using [`invoke_endpoint_with_response_stream`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint_with_response_stream.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71632bf8-5297-45db-9a92-a4a371b7da26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello! How are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi! I am quite well, how can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a short limerick about the wonders of GPU Computing.\"}\n",
    "]\n",
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": messages,\n",
    "  \"max_tokens\": 100,\n",
    "  \"stream\": True\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9d24a-a23e-40aa-afdb-da2a63f624fe",
   "metadata": {},
   "source": [
    "We have some postprocessing code for the streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05742b6-f8e4-4116-92ed-e9b156b6cfe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_stream = response['Body']\n",
    "accumulated_data = \"\"\n",
    "start_marker = 'data:'\n",
    "end_marker = '\"finish_reason\":null}]}'\n",
    "\n",
    "for event in event_stream:\n",
    "    try:\n",
    "        payload = event.get('PayloadPart', {}).get('Bytes', b'')\n",
    "        if payload:\n",
    "            data_str = payload.decode('utf-8')\n",
    "\n",
    "            accumulated_data += data_str\n",
    "\n",
    "            # Process accumulated data when a complete response is detected\n",
    "            while start_marker in accumulated_data and end_marker in accumulated_data:\n",
    "                start_idx = accumulated_data.find(start_marker)\n",
    "                end_idx = accumulated_data.find(end_marker) + len(end_marker)\n",
    "                full_response = accumulated_data[start_idx + len(start_marker):end_idx]\n",
    "                accumulated_data = accumulated_data[end_idx:]\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(full_response)\n",
    "                    content = data.get('choices', [{}])[0].get('delta', {}).get('content', \"\")\n",
    "                    if content:\n",
    "                        print(content, end='', flush=True)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing event: {e}\", flush=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "### Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
