{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0488cae-f2b2-4d0f-9e42-7cd4faae07d8",
   "metadata": {},
   "source": [
    "# Deploy NVIDIA NIM on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf191c-ac98-4d56-a6e5-a43e6de87b13",
   "metadata": {},
   "source": [
    "NVIDIA NIM, a component of NVIDIA AI Enterprise, enhances your applications with the power of state-of-the-art large language models (LLMs), providing unmatched natural language processing and understanding capabilities. Whether you're developing chatbots, content analyzers, or any application that needs to understand and generate human language, NVIDIA NIM for LLMs has you covered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1048b66-14f3-4f47-91fd-e653979c7cd5",
   "metadata": {},
   "source": [
    "In this example we show how to deploy `NVIDIA Nemotron Nano 9b v2` with NIM on Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1599941-1c76-4352-b1c3-eca6f4a65aaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>IMPORTANT:</b> To run NIM on SageMaker you will need to have your NGC API KEY because it's required to access NGC resources. Check out <a href=\"https://build.nvidia.com/meta/llama3-70b?signin=true\"> this LINK</a> to learn how to get NGC API KEY. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1f3df-a66e-490e-b4dc-7aa7b3a0ed6e",
   "metadata": {},
   "source": [
    "Please check out the [NIM LLM docs](https://docs.nvidia.com/nim/large-language-models/latest/introduction.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e8963-150a-4a65-b88a-55174a4e6db5",
   "metadata": {},
   "source": [
    "**⚠️ Disclaimer**\n",
    "\n",
    "Reasoning models often require longer inference times, which may exceed the default 60-second timeout limit for **AWS SageMaker's non-streaming endpoints**. This notebook shows examples for both the streaming and non-streaming endpoints \n",
    "\n",
    "To avoid inference failures due to timeout:\n",
    "- It is **recommended** to use a **SageMaker streaming endpoint** for this model.\n",
    "- If your use case **requires** using a **non-streaming endpoint**, you must first contact **AWS Support** to request an increased timeout limit for your **AWS Account and Region** to avoid unexpected errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa383fca-0ffb-45f9-a6cf-1849d117a386",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a50f-24d5-4778-a02d-28efc31373b7",
   "metadata": {},
   "source": [
    "Installs the dependencies and setup roles required to package the model and create SageMaker endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578a7de-7ed3-4105-bec7-e5d3b04cd4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json, sagemaker, time, os\n",
    "from sagemaker import get_execution_role\n",
    "from pathlib import Path\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n",
    "region = sess.region_name\n",
    "sts_client = sess.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7acbd-edad-4e3e-9386-1e587879b2a5",
   "metadata": {},
   "source": [
    "### Define Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f64791-1c45-4b84-9b1a-1e3ebc60d2de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "public_nim_image = \"public.ecr.aws/nvidia/nim:nvidia-nemotron-nano-9b-v2-1.12\"\n",
    "nim_model = \"nvidia-nemotron-nano-9b-v2\"\n",
    "sm_model_name = \"nvidia-nemotron-nano-9b-v2\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "payload_model = \"nvidia/nvidia-nemotron-nano-9b-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb05462",
   "metadata": {},
   "source": [
    "### NIM Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851abe8-9ca4-403b-be7f-aef56dfa4b9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We first pull the NIM image from public ECR and then push it to private ECR repo within your account for deploying on SageMaker endpoint. Note:\n",
    "  - NIM ECR image is currently available only in `us-east-1` region\n",
    "  - You must have `ecr:CreateRepository` and appropriate push permissions associated with your execution role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944110e5-15bc-4a94-a731-7d4ea9344e9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Get AWS account ID\n",
    "result = subprocess.run(['aws', 'sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error getting AWS account ID: {result.stderr}\")\n",
    "else:\n",
    "    account = result.stdout.strip()\n",
    "    print(f\"AWS account ID: {account}\")\n",
    "\n",
    "bash_script = f\"\"\"\n",
    "echo \"Public NIM Image: {public_nim_image}\"\n",
    "docker pull {public_nim_image}\n",
    "\n",
    "\n",
    "echo \"Resolved account: {account}\"\n",
    "echo \"Resolved region: {region}\"\n",
    "\n",
    "nim_image=\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "\n",
    "# Ensure the repository name adheres to AWS constraints\n",
    "repository_name=$(echo \"{nim_model}\" | tr '[:upper:]' '[:lower:]' | tr -cd '[:alnum:]._/-')\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"$repository_name\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"$repository_name\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin \"{account}.dkr.ecr.{region}.amazonaws.com\"\n",
    "\n",
    "docker tag {public_nim_image} $nim_image\n",
    "docker push $nim_image\n",
    "echo -n $nim_image\n",
    "\"\"\"\n",
    "nim_image=f\"{account}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
    "# Run the bash script and capture real-time output\n",
    "process = subprocess.Popen(bash_script, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == b'' and process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.decode().strip())\n",
    "\n",
    "stderr = process.stderr.read().decode()\n",
    "if stderr:\n",
    "    print(\"Errors:\", stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18863fd4-0893-4022-9ab0-38e1af1512d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "We print the private ECR NIM image in your account that we will be using for SageMaker deployment. \n",
    "- Should be similar to  `\"<ACCOUNT ID>.dkr.ecr.<REGION>.amazonaws.com/<NIM_MODEL>:latest\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb63ac-2a7a-4beb-b0dd-77a4473e1a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(nim_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518de4e-dcad-4944-9025-484878edb00b",
   "metadata": {},
   "source": [
    "### Create SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9efc86-0cf2-403b-9502-fd294acb4cb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Before proceeding further, please set your NGC API Key.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f1f264-ebd8-4c6a-9926-7a21afd89ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SET ME\n",
    "NGC_API_KEY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9b93a-3fdf-49a4-8f89-494238008a77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert NGC_API_KEY is not None, \"NGC API KEY is not set. Please set the NGC_API_KEY variable. It's required for running NIM.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5bce6-3807-43c3-866f-80543cfdedbf",
   "metadata": {},
   "source": [
    "We define sagemaker model from the NIM container making sure to pass in **NGC_API_KEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b784149-2ec3-4e29-a7cf-3636843dee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": nim_image,\n",
    "    \"Environment\": {\n",
    "        \"NGC_API_KEY\": NGC_API_KEY,\n",
    "        \"NIM_MAX_NUM_SEQS\": \"8\", # Set for smaller GPUs such as A10G (g5.xxxx instances) can remove for larger GPUs\n",
    "        \"NIM_MAMBA_SSM_CACHE_DTYPE\": \"auto\" # Set for smaller GPUs such as A10G (g5.xxxx instances) can remove for larger GPUs\n",
    "    }\n",
    "}\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2f0e6-0377-4a13-9cc3-c345adf08c86",
   "metadata": {},
   "source": [
    "Next we create endpoint configuration, here we are deploying the LLama3-70B model on the specified instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af8b7c-9347-4203-aea5-f44392449f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = sm_model_name\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 1800,\n",
    "            \"InferenceAmiVersion\": \"al2-ami-sagemaker-inference-gpu-2\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51121a-a662-4078-a0c6-b163cda0a718",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75add3d0-100f-4740-b326-6f54af7e9c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = sm_model_name\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d4bc4-b77b-4137-930e-7517295a041c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a97e4c-6dd8-4d9a-841c-e443b7c1583f",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9146f44-b85c-4125-b842-fceaf5c3cfa8",
   "metadata": {},
   "source": [
    "Once we have the endpoint's status as `InService` we can use a sample text to do a chat completion inference request using json as the payload format. For inference request format, currently NIM on SageMaker supports the OpenAI API chat completions inference protocol. For explanation of supported parameters please see [this link](https://platform.openai.com/docs/api-reference/chat). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d36583-d6b0-4fdf-a659-c088f913034a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>IMPORTANT:</b> Model name in inference request payload needs to be the name of NIM model. Please DON'T change it below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a046a9-40f3-4bda-a755-a032c44eb905",
   "metadata": {},
   "source": [
    "### Non Reasoning Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68589d-5ab5-4310-b598-ebedb1bdef49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": [\n",
    "    {   \n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"/no_think\"\n",
    "    },\n",
    "    {\n",
    "      \"role\":\"user\",\n",
    "      \"content\":\"Explain how a transformer neural network works.\"\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 3000\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f83033-33f0-407e-bd4c-d52b06d713a8",
   "metadata": {},
   "source": [
    "### Reasoning Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e1fbbd-60ff-40a3-8f10-f9062ea329f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": [\n",
    "    {   \n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"/think\"\n",
    "    },\n",
    "    {\n",
    "      \"role\":\"user\",\n",
    "      \"content\": \"Explain how a transformer neural network works\"\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 3000\n",
    "}\n",
    "\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "print(json.dumps(output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4dd197-4d0b-456a-90bc-f12758195ce9",
   "metadata": {},
   "source": [
    "## Streaming inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e76ad7-0869-47bc-99d8-b22ba143b160",
   "metadata": {},
   "source": [
    "NIM on SageMaker also supports streaming inference and you can enable that by setting **`\"stream\"` as `True`** in the payload and by using [`invoke_endpoint_with_response_stream`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint_with_response_stream.html) method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938337d-38c7-48e1-82fb-01f5cb183d27",
   "metadata": {},
   "source": [
    "### Non Reasoning Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb61a7d-0a58-44d8-93a4-79ea91534af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": [\n",
    "    {   \n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"/no_think\"\n",
    "    },\n",
    "    {\n",
    "      \"role\":\"user\",\n",
    "      \"content\":\"Explain how a transformer neural network works.\"\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 3000,\n",
    "  \"stream\": True\n",
    "\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f52c97-b4b2-4344-b85a-9d3934592e58",
   "metadata": {},
   "source": [
    "We have some postprocessing code for the streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479eba5-fbd2-4abc-8380-5661e629159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stream = response['Body']\n",
    "accumulated_data = \"\"\n",
    "start_marker = 'data:'\n",
    "end_marker = '\"finish_reason\":null}]}'\n",
    "\n",
    "for event in event_stream:\n",
    "    try:\n",
    "        payload = event.get('PayloadPart', {}).get('Bytes', b'')\n",
    "        if payload:\n",
    "            data_str = payload.decode('utf-8')\n",
    "\n",
    "            accumulated_data += data_str\n",
    "\n",
    "            # Process accumulated data when a complete response is detected\n",
    "            while start_marker in accumulated_data and end_marker in accumulated_data:\n",
    "                start_idx = accumulated_data.find(start_marker)\n",
    "                end_idx = accumulated_data.find(end_marker) + len(end_marker)\n",
    "                full_response = accumulated_data[start_idx + len(start_marker):end_idx]\n",
    "                accumulated_data = accumulated_data[end_idx:]\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(full_response)\n",
    "                    content = data.get('choices', [{}])[0].get('delta', {}).get('content', \"\")\n",
    "                    if content:\n",
    "                        print(content, end='', flush=True)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing event: {e}\", flush=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed8791-6fb8-4e91-a131-86b52bf3cfe8",
   "metadata": {},
   "source": [
    "### Reasoning Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34899db6-aa48-4282-843e-c5ea01f09dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "  \"model\": payload_model,\n",
    "  \"messages\": [\n",
    "    {   \n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"/think\"\n",
    "    },\n",
    "    {\n",
    "      \"role\":\"user\",\n",
    "      \"content\":\"Explain how a transformer neural network works.\"\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 3000,\n",
    "  \"stream\": True\n",
    "\n",
    "}\n",
    "\n",
    "response = client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(payload),\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/jsonlines\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105bd8ad-2206-4f65-88c4-779cbf7446e4",
   "metadata": {},
   "source": [
    "We have some postprocessing code for the streaming output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7112b-5611-464d-87c0-fdb885ac741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stream = response['Body']\n",
    "accumulated_data = \"\"\n",
    "start_marker = 'data:'\n",
    "end_marker = '\"finish_reason\":null}]}'\n",
    "\n",
    "for event in event_stream:\n",
    "    try:\n",
    "        payload = event.get('PayloadPart', {}).get('Bytes', b'')\n",
    "        if payload:\n",
    "            data_str = payload.decode('utf-8')\n",
    "\n",
    "            accumulated_data += data_str\n",
    "\n",
    "            # Process accumulated data when a complete response is detected\n",
    "            while start_marker in accumulated_data and end_marker in accumulated_data:\n",
    "                start_idx = accumulated_data.find(start_marker)\n",
    "                end_idx = accumulated_data.find(end_marker) + len(end_marker)\n",
    "                full_response = accumulated_data[start_idx + len(start_marker):end_idx]\n",
    "                accumulated_data = accumulated_data[end_idx:]\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(full_response)\n",
    "                    content = data.get('choices', [{}])[0].get('delta', {}).get('content', \"\")\n",
    "                    if content:\n",
    "                        print(content, end='', flush=True)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing event: {e}\", flush=True)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df78c1c-d725-4c61-885f-edb2dacb7859",
   "metadata": {},
   "source": [
    "## Agent implementation with Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c2b0d-d438-4cb3-89eb-cc3ce639895f",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Based on the user input, the agent invokes **a tool from the pool of available tools**. The agent will decide and invoke the required **tool(s)** to get the response back to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a7c00-2b1a-4ba3-a61c-5d83dfe8c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66481d-7fb0-4096-a411-407e759be678",
   "metadata": {},
   "source": [
    "### Define tools for the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fd111-7ba5-49ce-a2d7-a941f971da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(input) -> int:\n",
    "    \"\"\"Get the current temperature from a city, in Fahrenheit\"\"\"\n",
    "    \n",
    "    city = input[\"city\"].lower()\n",
    "    country = input[\"country\"].lower()\n",
    "    \n",
    "    # Hardcoded temperature data\n",
    "    weather = {\n",
    "        \"us\": {\n",
    "            \"new york\": 82,\n",
    "            \"los angeles\": 76,\n",
    "            \"chicago\": 70\n",
    "        },\n",
    "        \"gb\": {\n",
    "            \"london\": 68\n",
    "        },\n",
    "        \"ca\": {\n",
    "            \"toronto\": 65,\n",
    "            \"vancouver\": 60\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Look up the temperature\n",
    "    try:\n",
    "        return weather[country][city]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"No temperature found for {city.title()}, {country.upper()}\")\n",
    "\n",
    "def get_difference(input) -> int:\n",
    "    \"\"\"Get the difference between two numbers\"\"\"\n",
    "    \n",
    "    minuend = input[\"minuend\"]\n",
    "    subtrahend = input[\"subtrahend\"]\n",
    "    return minuend - subtrahend\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current temperature from a city, in Fahrenheit\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"City\"\n",
    "                    },\n",
    "                    \"country\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Country Code (e.g. US, GB, CA)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\", \"country\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_difference\",\n",
    "            \"description\": \"Get the difference between two numbers\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"minuend\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The number from which another number is to be subtracted\"\n",
    "                    },\n",
    "                    \"subtrahend\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The number to be subtracted\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"minuend\", \"subtrahend\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53dc473-c96a-48c6-bd79-59f1c0673721",
   "metadata": {},
   "source": [
    "### Define helper function to call SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e54b77-2db1-4395-8f11-285e8df7aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_sagemaker(messages, tools):\n",
    "    \"\"\"Call SageMaker endpoint with OpenAI API format\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": payload_model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0,\n",
    "        \"tool_choice\": \"auto\",\n",
    "        \"tools\": tools\n",
    "    }\n",
    "    \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    response_body = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ece5cc-694a-4dbf-90b2-01314077e153",
   "metadata": {},
   "source": [
    "### Define helper function to invoke a given tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7dfc9-7ebc-4416-8491-120d5c156e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool_result(tool_call):\n",
    "    \"\"\"Execute a tool call and return the result\"\"\"\n",
    "    tool_name = tool_call['function']['name']\n",
    "    tool_args = json.loads(tool_call['function']['arguments'])\n",
    "    \n",
    "    print(f\"Using tool `{tool_name}` with args `{tool_args}`\")\n",
    "    func = globals()[tool_name]\n",
    "    try:\n",
    "        return func(tool_args)  # Pass the full args dict, not just 'query'\n",
    "    except Exception as e:\n",
    "        raise ToolError(f\"Something went wrong: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113d5d8-907a-4259-8349-8d2c081871bf",
   "metadata": {},
   "source": [
    "### Define function to handle the raw responses from SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f0c325-09de-4710-b554-68aa829757e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_model_response(response):\n",
    "    \"\"\"Handle tool calls in the model response\"\"\"\n",
    "    \n",
    "    message = response['choices'][0]['message']\n",
    "    \n",
    "    # Check if there are tool calls\n",
    "    if not message.get('tool_calls'):\n",
    "        return None, message\n",
    "    \n",
    "    tool_messages = []\n",
    "    \n",
    "    for tool_call in message['tool_calls']:\n",
    "        try:\n",
    "            tool_result = get_tool_result(tool_call)\n",
    "            \n",
    "            tool_message = {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call['id'],\n",
    "                \"content\": json.dumps(tool_result)\n",
    "            }\n",
    "            tool_messages.append(tool_message)\n",
    "            \n",
    "        except ToolError as e:\n",
    "            tool_message = {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call['id'],\n",
    "                \"content\": f\"Error: {str(e)}\"\n",
    "            }\n",
    "            tool_messages.append(tool_message)\n",
    "    \n",
    "    return tool_messages, message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4c314-ad0d-4631-b6b6-5b97df07d093",
   "metadata": {},
   "source": [
    "### Define function that implements the Agent Loop till final response is received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ceaa23-2104-4365-b4d9-8cefceff3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(messages, tools):\n",
    "    \"\"\"Run the agent loop until completion\"\"\"\n",
    "    MAX_LOOPS = 10\n",
    "    loop_count = 0\n",
    "\n",
    "    while loop_count < MAX_LOOPS:\n",
    "        loop_count += 1\n",
    "        \n",
    "        # Call the model\n",
    "        response = call_sagemaker(messages, tools)\n",
    "        \n",
    "        # Handle the response\n",
    "        tool_messages, assistant_message = handle_model_response(response)\n",
    "        \n",
    "        # Add assistant message to conversation\n",
    "        messages.append(assistant_message)\n",
    "        \n",
    "        # If no tool calls, we're done\n",
    "        if tool_messages is None:\n",
    "            final_output = assistant_message.get('content', '')\n",
    "            break\n",
    "        \n",
    "        # Add tool results to conversation\n",
    "        messages.extend(tool_messages)\n",
    "    \n",
    "    else:\n",
    "        final_output = \"Maximum loops reached\"\n",
    "    \n",
    "    return messages, final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a26f3-1aa9-4f1c-ba49-fae91630629c",
   "metadata": {},
   "source": [
    "### Define agent executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03ddcd-9743-4694-9fc2-7062a7e422b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_agent_executor(input_prompt):\n",
    "    \n",
    "    system_prompt = system_prompt = \"\"\"You are a helpful weather assistant. You have access to tools that return:\n",
    "    - get_current_weather: Returns temperature in Fahrenheit for a given city\n",
    "    - get_difference: Returns the numerical difference between two numbers\n",
    "    \n",
    "    When you make tool calls and receive results, use those results directly in your answer. \n",
    "    The tool results correspond exactly to the tool calls you made in the same order.\n",
    "\n",
    "    Make the answer brief, do not mention tool calls in your answer\n",
    "    \"\"\"\n",
    "    detailed_thinking = \"/no_think\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": detailed_thinking\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    workflow, output = run_agent(messages, tools)\n",
    "    return workflow, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45e010-1248-4e0d-9d05-e2d31b3a66c9",
   "metadata": {},
   "source": [
    "### Test agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc757d2-7418-4ee4-bdf1-13e939ddddcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "workflow, output = weather_agent_executor(\"Where is it warmest: New york, london or Toronto? And by how much is it warmer than the other cities?\")\n",
    "print(\"\\n========Output=============\\n\")\n",
    "print(output)\n",
    "print(\"\\n========Tool calling details=============\\n\")\n",
    "print(workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19063f6-b6c0-4de2-a193-e482f26f7406",
   "metadata": {},
   "source": [
    "## Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db083f-4705-4c68-a488-f82da961be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
