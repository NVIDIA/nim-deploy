{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NVIDIA NIM TTS Model Deployment on Amazon SageMaker AI using BYOC (Bring Your Own Container)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook demonstrates how to deploy the **NVIDIA NIM TTS (Magpie TTS Multilingual)** model for Text-to-Speech (TTS) tasks using Amazon SageMaker with a custom container that supports both HTTP and gRPC protocols.\n",
        "\n",
        "### About NVIDIA NIM TTS (Magpie)\n",
        "\n",
        "The **NVIDIA NIM TTS Magpie Multilingual** provides a production-ready text-to-speech service:\n",
        "\n",
        "- **Architecture**: HTTP + gRPC routing to NVIDIA NIM TTS container\n",
        "- **Model**: Magpie TTS Multilingual optimized for high-quality speech synthesis\n",
        "- **Performance**: Low latency, high-quality audio output\n",
        "- **Features**: Multiple voices, languages, zero-shot voice cloning, custom dictionaries\n",
        "- **Deployment**: Ready for SageMaker real-time inference\n",
        "\n",
        "### Key Features\n",
        "\n",
        "1. **Dual Protocol Support**: HTTP for simple requests, gRPC for advanced features\n",
        "2. **Multilingual Support**: Multiple languages and voices available\n",
        "3. **Zero-Shot Voice Cloning**: Clone voices from audio prompts (gRPC)\n",
        "4. **Custom Dictionaries**: Define custom pronunciations (gRPC)\n",
        "5. **End-to-End Streaming**: True streaming via `InvokeEndpointWithResponseStream` API\n",
        "6. **Production Ready**: Built with NVIDIA NIM for enterprise deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites and Setup\n",
        "\n",
        "**‚ùó Important Notes:**\n",
        "- Docker is required to pull and push container images\n",
        "- You need an **NGC_API_KEY** from NVIDIA NGC ([Get one here](https://build.nvidia.com))\n",
        "- ECR permissions are required for pushing Docker images to your private ECR\n",
        "- NIM ECR image is currently available only in `us-east-1` region\n",
        "\n",
        "**Supported AWS Instances (Compute Capability >= 8.0):**\n",
        "\n",
        "| Instance Family | GPU | Examples |\n",
        "|-----------------|-----|----------|\n",
        "| ml.g6e.* | L40S | ml.g6e.xlarge, ml.g6e.2xlarge |\n",
        "| ml.p4d.* | A100 | ml.p4d.24xlarge |\n",
        "| ml.p5.* | H100 | ml.p5.48xlarge |\n",
        "\n",
        "> ‚ö†Ô∏è Other GPU instances (g4dn, g5, p3) are **not supported**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install sagemaker>=2.246.0 boto3 soundfile --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import boto3\n",
        "import json\n",
        "import sagemaker\n",
        "import time\n",
        "import os\n",
        "from sagemaker import get_execution_role\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup AWS clients and session\n",
        "sess = boto3.Session()\n",
        "sm = sess.client(\"sagemaker\")\n",
        "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
        "role = get_execution_role()\n",
        "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
        "region = sess.region_name\n",
        "sts_client = sess.client('sts')\n",
        "account_id = sts_client.get_caller_identity()['Account']\n",
        "\n",
        "print(f\"Region: {region}\")\n",
        "print(f\"Account ID: {account_id}\")\n",
        "print(f\"Role: {role}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define deployment arguments\n",
        "public_nim_image = \"public.ecr.aws/nvidia/nim:magpie-tts-multilingual-1.6.0\"\n",
        "nim_model = \"magpie-tts-multilingual\"\n",
        "sm_model_name = \"nim-tts-magpie-multilingual\"\n",
        "instance_type = \"ml.g6e.xlarge\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NIM Container Setup\n",
        "\n",
        "Pull the NIM image from public ECR and push to your private ECR repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull NIM image from public ECR and push to private ECR\n",
        "\n",
        "import subprocess\n",
        "\n",
        "print(f\"Public NIM Image: {public_nim_image}\")\n",
        "print(f\"Target model name: {nim_model}\")\n",
        "\n",
        "bash_script = f\"\"\"\n",
        "echo \"Public NIM Image: {public_nim_image}\"\n",
        "docker pull {public_nim_image}\n",
        "\n",
        "echo \"Resolved account: {account_id}\"\n",
        "echo \"Resolved region: {region}\"\n",
        "\n",
        "nim_image=\"{account_id}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
        "\n",
        "# Ensure the repository name adheres to AWS constraints\n",
        "repository_name=$(echo \"{nim_model}\" | tr '[:upper:]' '[:lower:]' | tr -cd '[:alnum:]._/-')\n",
        "\n",
        "# If the repository doesn't exist in ECR, create it.\n",
        "aws ecr describe-repositories --repository-names \"$repository_name\" > /dev/null 2>&1\n",
        "\n",
        "if [ $? -ne 0 ]\n",
        "then\n",
        "    aws ecr create-repository --repository-name \"$repository_name\" > /dev/null\n",
        "    echo \"‚úÖ Created ECR repository: $repository_name\"\n",
        "else\n",
        "    echo \"‚úÖ ECR repository already exists: $repository_name\"\n",
        "fi\n",
        "\n",
        "# Get the login command from ECR and execute it directly\n",
        "aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin \"{account_id}.dkr.ecr.{region}.amazonaws.com\"\n",
        "\n",
        "docker tag {public_nim_image} $nim_image\n",
        "docker push $nim_image\n",
        "echo \"‚úÖ Image pushed successfully\"\n",
        "echo -n $nim_image\n",
        "\"\"\"\n",
        "\n",
        "nim_image = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{nim_model}\"\n",
        "\n",
        "# Run the bash script and capture real-time output\n",
        "process = subprocess.Popen(bash_script, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "while True:\n",
        "    output = process.stdout.readline()\n",
        "    if output == b'' and process.poll() is not None:\n",
        "        break\n",
        "    if output:\n",
        "        print(output.decode().strip())\n",
        "\n",
        "stderr = process.stderr.read().decode()\n",
        "if stderr:\n",
        "    print(\"Errors:\", stderr)\n",
        "\n",
        "print(f\"\\nüéØ Private ECR Image: {nim_image}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the private ECR NIM image that will be used for SageMaker deployment\n",
        "print(f\"NIM Image URI: {nim_image}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create SageMaker Endpoint\n",
        "\n",
        "**Before proceeding further, please set your NGC API Key.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SET YOUR NGC API KEY HERE\n",
        "# Required for running NIM - get yours from https://build.nvidia.com\n",
        "NGC_API_KEY = None  # <-- SET ME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate NGC API Key\n",
        "assert NGC_API_KEY is not None, \"NGC API KEY is not set. Please set the NGC_API_KEY variable in the previous cell.\"\n",
        "print(\"‚úÖ NGC_API_KEY is set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create SageMaker model with NIM container\n",
        "container = {\n",
        "    \"Image\": nim_image,\n",
        "    \"Environment\": {\n",
        "        \"NGC_API_KEY\": NGC_API_KEY,\n",
        "    }\n",
        "}\n",
        "\n",
        "create_model_response = sm.create_model(\n",
        "    ModelName=sm_model_name, \n",
        "    ExecutionRoleArn=role, \n",
        "    PrimaryContainer=container\n",
        ")\n",
        "\n",
        "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create endpoint configuration\n",
        "endpoint_config_name = sm_model_name\n",
        "\n",
        "create_endpoint_config_response = sm.create_endpoint_config(\n",
        "    EndpointConfigName=endpoint_config_name,\n",
        "    ProductionVariants=[\n",
        "        {\n",
        "            \"InstanceType\": instance_type,\n",
        "            \"InitialVariantWeight\": 1,\n",
        "            \"InitialInstanceCount\": 1,\n",
        "            \"ModelName\": sm_model_name,\n",
        "            \"VariantName\": \"AllTraffic\",\n",
        "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 1800,\n",
        "            \"InferenceAmiVersion\": \"al2-ami-sagemaker-inference-gpu-2\"\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create endpoint\n",
        "endpoint_name = sm_model_name\n",
        "\n",
        "create_endpoint_response = sm.create_endpoint(\n",
        "    EndpointName=endpoint_name, \n",
        "    EndpointConfigName=endpoint_config_name\n",
        ")\n",
        "\n",
        "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for endpoint to be in service\n",
        "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
        "status = resp[\"EndpointStatus\"]\n",
        "print(\"Status: \" + status)\n",
        "\n",
        "while status == \"Creating\":\n",
        "    time.sleep(60)\n",
        "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
        "    status = resp[\"EndpointStatus\"]\n",
        "    print(\"Status: \" + status)\n",
        "\n",
        "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
        "print(\"Status: \" + status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Testing\n",
        "\n",
        "Test the deployed TTS endpoint with different transport options.\n",
        "\n",
        "### API Request Format\n",
        "\n",
        "The request body follows the [NVIDIA Riva TTS SynthesizeSpeechRequest proto](https://docs.nvidia.com/nim/riva/tts/1.6.0/protos.html#nvidia-riva-tts-synthesizespeechrequest):\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"text\": \"Text to synthesize\",\n",
        "    \"voice_name\": \"Magpie-Multilingual.EN-US.Aria\",\n",
        "    \"language_code\": \"en-US\",\n",
        "    \"sample_rate_hz\": 44100,\n",
        "    \"encoding\": \"LINEAR_PCM\",\n",
        "    \"zero_shot_data\": {\n",
        "        \"audio_prompt\": \"<base64-encoded-audio>\",\n",
        "        \"quality\": 20,\n",
        "        \"transcript\": \"optional transcript\"\n",
        "    },\n",
        "    \"custom_dictionary\": \"NVIDIA  en-vid-ee-ah,SageMaker  sage-may-ker\"\n",
        "}\n",
        "```\n",
        "\n",
        "| Field | Type | Description |\n",
        "|-------|------|-------------|\n",
        "| `text` | string | Text to synthesize (required) |\n",
        "| `voice_name` | string | Voice name ([available voices](https://docs.nvidia.com/nim/riva/tts/1.10.0/support-matrix.html#available-voices)) |\n",
        "| `language_code` | string | Language code: en-US, es-US, fr-FR, de-DE, zh-CN, vi-VN, it-IT ([docs](https://docs.nvidia.com/nim/riva/tts/1.10.0/support-matrix.html#magpie-tts-multilingual)) |\n",
        "| `sample_rate_hz` | int | Sample rate (default: 44100) |\n",
        "| `encoding` | string | `LINEAR_PCM` or `OGGOPUS` |\n",
        "| `zero_shot_data` | object | `{audio_prompt, quality, transcript}` for voice cloning |\n",
        "| `custom_dictionary` | string | `\"word1  pron1,word2  pron2\"` (double-space separator) |\n",
        "\n",
        "**Response:** JSON matching [NIM SynthesizeSpeechResponse](https://docs.nvidia.com/nim/riva/tts/1.6.0/protos.html#nvidia-riva-tts-synthesizespeechresponse):\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"audio\": \"<base64-encoded-audio>\",\n",
        "    \"meta\": {\n",
        "        \"text\": \"original input text\",\n",
        "        \"processed_text\": \"text after preprocessing\",\n",
        "        \"predicted_durations\": [0.1, 0.2, ...]\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "| Response Field | Type | Description |\n",
        "|----------------|------|-------------|\n",
        "| `audio` | string | Base64-encoded audio bytes (always present) |\n",
        "| `meta` | object | Optional - metadata from NIM (if returned) |\n",
        "\n",
        "**Streaming:** Use `CustomAttributes='/invocations/stream'` header with `invoke_endpoint_with_response_stream`\n",
        "\n",
        "### Transport Selection Options\n",
        "\n",
        "The TTS endpoint supports both HTTP and gRPC protocols internally:\n",
        "\n",
        "| Transport | Use Case | Features |\n",
        "|-----------|----------|----------|\n",
        "| **HTTP** (default) | Simple TTS requests | Basic synthesis, voice selection |\n",
        "| **gRPC** | Advanced features | Zero-shot cloning, streaming, custom dictionaries |\n",
        "| **Auto** | Automatic selection | HTTP for simple, gRPC for advanced features |\n",
        "\n",
        "### Transport Selection (via CustomAttributes Header)\n",
        "\n",
        "Use the `CustomAttributes` header to select transport:\n",
        "- `/invocations/http` - Force HTTP transport\n",
        "- `/invocations/grpc` - Force gRPC transport  \n",
        "- `/invocations/stream` - Enable streaming (uses gRPC)\n",
        "\n",
        "If not specified, auto-routing selects HTTP for simple requests, gRPC for advanced features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import IPython.display as ipd\n",
        "import base64\n",
        "\n",
        "def test_tts(text, voice_name=None, language_code=\"en-US\", sample_rate_hz=44100, \n",
        "             encoding=\"LINEAR_PCM\", zero_shot_data=None, custom_dictionary=None,\n",
        "             custom_attributes=None, output_file=\"tts_output.wav\"):\n",
        "    \"\"\"\n",
        "    Test TTS endpoint following NIM SynthesizeSpeechRequest proto format.\n",
        "    \n",
        "    Note: For streaming, use test_tts_streaming() with invoke_endpoint_with_response_stream.\n",
        "    \n",
        "    Args:\n",
        "        text: Text to synthesize (required)\n",
        "        voice_name: Voice name (NIM standard field)\n",
        "        language_code: Language code (default: en-US)\n",
        "        sample_rate_hz: Sample rate (default: 44100)\n",
        "        encoding: Audio encoding - LINEAR_PCM or OGGOPUS\n",
        "        zero_shot_data: Dict with {audio_prompt, quality, transcript} for voice cloning\n",
        "        custom_dictionary: NIM format string \"word1  pronunciation1,word2  pronunciation2\"\n",
        "        custom_attributes: SageMaker CustomAttributes header for transport selection\n",
        "        output_file: Output filename for the audio\n",
        "    \"\"\"\n",
        "    print(f\"Testing TTS with text: '{text[:50]}...'\")\n",
        "    \n",
        "    # Build payload following NIM SynthesizeSpeechRequest proto\n",
        "    payload = {\n",
        "        \"text\": text,\n",
        "        \"language_code\": language_code,\n",
        "        \"sample_rate_hz\": sample_rate_hz,\n",
        "        \"encoding\": encoding\n",
        "    }\n",
        "    \n",
        "    if voice_name:\n",
        "        payload[\"voice_name\"] = voice_name\n",
        "    if zero_shot_data:\n",
        "        payload[\"zero_shot_data\"] = zero_shot_data\n",
        "    if custom_dictionary:\n",
        "        payload[\"custom_dictionary\"] = custom_dictionary\n",
        "    \n",
        "    try:\n",
        "        # Build invoke_endpoint kwargs\n",
        "        invoke_kwargs = {\n",
        "            \"EndpointName\": endpoint_name,\n",
        "            \"ContentType\": \"application/json\",\n",
        "            \"Body\": json.dumps(payload)\n",
        "        }\n",
        "        \n",
        "        # Add custom attributes if specified (for transport selection)\n",
        "        if custom_attributes:\n",
        "            invoke_kwargs[\"CustomAttributes\"] = custom_attributes\n",
        "        \n",
        "        response = sm_runtime.invoke_endpoint(**invoke_kwargs)\n",
        "        \n",
        "        # Response is JSON matching NIM SynthesizeSpeechResponse proto\n",
        "        # https://docs.nvidia.com/nim/riva/tts/1.6.0/protos.html#nvidia-riva-tts-synthesizespeechresponse\n",
        "        response_body = response['Body'].read()\n",
        "        result = json.loads(response_body)\n",
        "        \n",
        "        print(f\"\\n‚úÖ TTS inference successful!\")\n",
        "        \n",
        "        # Extract audio from response (base64 encoded)\n",
        "        audio_bytes = base64.b64decode(result['audio'])\n",
        "        print(f\"Audio size: {len(audio_bytes):,} bytes\")\n",
        "        \n",
        "        # Print metadata if available\n",
        "        if 'meta' in result:\n",
        "            print(f\"Metadata: {result['meta']}\")\n",
        "        \n",
        "        # Save audio to WAV file\n",
        "        with open(output_file, 'wb') as f:\n",
        "            f.write(audio_bytes)\n",
        "        print(f\"Audio saved to: {output_file}\")\n",
        "        \n",
        "        # Play audio in notebook\n",
        "        return ipd.Audio(output_file)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå TTS test failed: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 1: Auto Transport (HTTP for simple requests)\n",
        "\n",
        "Uses HTTP by default for simple requests, automatically switches to gRPC for advanced features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Auto transport (HTTP for simple requests)\n",
        "# Simple requests use HTTP by default\n",
        "test_tts(\n",
        "    text=\"Hello! This is a test using automatic transport selection.\",\n",
        "    output_file=\"tts_auto.wav\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 2: Force HTTP Transport via Custom Attributes Header\n",
        "\n",
        "Use the `CustomAttributes` parameter in `invoke_endpoint` to force HTTP transport:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Force HTTP transport using CustomAttributes header\n",
        "# This is the recommended SageMaker-native way to select transport\n",
        "test_tts(\n",
        "    text=\"This audio is generated using HTTP transport via custom attributes header.\",\n",
        "    custom_attributes=\"/invocations/http\",\n",
        "    output_file=\"tts_http_header.wav\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 3: Force gRPC Transport via Custom Attributes Header\n",
        "\n",
        "Use gRPC for advanced features like streaming, zero-shot voice cloning, and custom dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Force gRPC transport using CustomAttributes header\n",
        "# Use this when you need advanced features\n",
        "test_tts(\n",
        "    text=\"This audio is generated using gRPC transport via custom attributes header.\",\n",
        "    custom_attributes=\"/invocations/grpc\",\n",
        "    output_file=\"tts_grpc_header.wav\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 4: Different Voice Selection\n",
        "\n",
        "Test with a specific voice name. Voice names follow the pattern: `Magpie-Multilingual.LANGUAGE.VoiceName`\n",
        "\n",
        "For a complete list of available voices, see the [NVIDIA NIM TTS Support Matrix](https://docs.nvidia.com/nim/riva/tts/1.10.0/support-matrix.html#available-voices).\n",
        "\n",
        "**Supported Languages:** English (en-US), Spanish (es-US), French (fr-FR), German (de-DE), Mandarin (zh-CN), Vietnamese (vi-VN), Italian (it-IT)\n",
        "\n",
        "**Example voices:**\n",
        "- `Magpie-Multilingual.EN-US.Aria` - English female\n",
        "- `Magpie-Multilingual.EN-US.Jason` - English male\n",
        "- `Magpie-Multilingual.ES-US.Diego` - Spanish male\n",
        "- `Magpie-Multilingual.FR-FR.Pascal` - French male"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4: Use a specific voice\n",
        "# Voice names follow the pattern: Magpie-Multilingual.LANGUAGE.VoiceName\n",
        "test_tts(\n",
        "    text=\"This audio demonstrates using a specific voice for synthesis.\",\n",
        "    voice_name=\"Magpie-Multilingual.EN-US.Aria\",\n",
        "    output_file=\"tts_voice.wav\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4b: Different sample rate\n",
        "# Try a lower sample rate (22050 Hz)\n",
        "test_tts(\n",
        "    text=\"This audio is generated with a lower sample rate for smaller file size.\",\n",
        "    sample_rate_hz=22050,\n",
        "    output_file=\"tts_low_sample_rate.wav\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 5: Force gRPC with Longer Text\n",
        "\n",
        "When processing longer texts, gRPC is recommended for better performance. Use `custom_attributes=\"/invocations/grpc\"` to force gRPC transport:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 5: Force gRPC for longer text\n",
        "# Use custom_attributes=\"/invocations/grpc\" for longer texts or gRPC-specific features\n",
        "test_tts(\n",
        "    text=\"This is a longer text that demonstrates gRPC transport. \"\n",
        "         \"gRPC is recommended for generating audio for longer texts \"\n",
        "         \"because it provides better performance and reliability.\",\n",
        "    custom_attributes=\"/invocations/grpc\",  # Force gRPC transport\n",
        "    output_file=\"tts_grpc_long.wav\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 6: Custom Dictionary (gRPC-only)\n",
        "\n",
        "Custom dictionaries allow you to define custom pronunciations for specific words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 6: Custom dictionary (gRPC-only feature)\n",
        "# NIM format: comma-separated key-value pairs with double-space separator\n",
        "# This automatically triggers gRPC in \"auto\" mode\n",
        "\n",
        "test_tts(\n",
        "    text=\"Welcome to NVIDIA and Amazon SageMaker integration.\",\n",
        "    custom_dictionary=\"NVIDIA  en-vid-ee-ah,SageMaker  sage-may-ker\",  # NIM format\n",
        "    output_file=\"tts_custom_dict.wav\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 7: Streaming with Audio Output\n",
        "\n",
        "Use SageMaker's `invoke_endpoint_with_response_stream` API for true end-to-end streaming with lower time-to-first-audio. This test shows timing metrics and outputs a playable audio file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 7: End-to-End Streaming with Audio Output\n",
        "# Collects streaming chunks, shows timing info, and outputs playable audio\n",
        "\n",
        "import wave\n",
        "import io\n",
        "import time\n",
        "\n",
        "def pcm_to_wav(pcm_data, sample_rate=44100, channels=1, bits_per_sample=16):\n",
        "    \"\"\"Convert raw PCM audio data to WAV format.\"\"\"\n",
        "    buffer = io.BytesIO()\n",
        "    with wave.open(buffer, 'wb') as wav_file:\n",
        "        wav_file.setnchannels(channels)\n",
        "        wav_file.setsampwidth(bits_per_sample // 8)\n",
        "        wav_file.setframerate(sample_rate)\n",
        "        wav_file.writeframesraw(pcm_data)\n",
        "    return buffer.getvalue()\n",
        "\n",
        "def test_tts_streaming(text, voice_name=None, language_code=\"en-US\", sample_rate_hz=44100, \n",
        "                       encoding=\"LINEAR_PCM\", output_file=\"tts_streaming.wav\"):\n",
        "    \"\"\"\n",
        "    Test TTS with end-to-end streaming using InvokeEndpointWithResponseStream.\n",
        "    \n",
        "    Shows timing metrics and outputs a playable audio file.\n",
        "    \"\"\"\n",
        "    print(f\"üé§ Streaming TTS Test\")\n",
        "    print(f\"Text: '{text[:60]}...'\" if len(text) > 60 else f\"Text: '{text}'\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    payload = {\n",
        "        \"text\": text,\n",
        "        \"language_code\": language_code,\n",
        "        \"sample_rate_hz\": sample_rate_hz,\n",
        "        \"encoding\": encoding\n",
        "    }\n",
        "    if voice_name:\n",
        "        payload[\"voice_name\"] = voice_name\n",
        "    \n",
        "    try:\n",
        "        response = sm_runtime.invoke_endpoint_with_response_stream(\n",
        "            EndpointName=endpoint_name,\n",
        "            ContentType='application/json',\n",
        "            CustomAttributes='/invocations/stream',\n",
        "            Body=json.dumps(payload)\n",
        "        )\n",
        "        \n",
        "        start_time = time.time()\n",
        "        first_chunk_time = None\n",
        "        chunks = []\n",
        "        \n",
        "        for event in response['Body']:\n",
        "            if 'PayloadPart' in event:\n",
        "                chunk = event['PayloadPart']['Bytes']\n",
        "                if chunk:\n",
        "                    if first_chunk_time is None:\n",
        "                        first_chunk_time = time.time() - start_time\n",
        "                    chunks.append(chunk)\n",
        "        \n",
        "        total_time = time.time() - start_time\n",
        "        raw_pcm = b''.join(chunks)\n",
        "        \n",
        "        print(f\"‚è±Ô∏è  Time to first chunk: {first_chunk_time:.3f}s\")\n",
        "        print(f\"üì¶ Chunks received: {len(chunks)}\")\n",
        "        print(f\"üìä Total PCM bytes: {len(raw_pcm):,}\")\n",
        "        print(f\"‚è±Ô∏è  Total time: {total_time:.3f}s\")\n",
        "        \n",
        "        # Convert to WAV and save\n",
        "        wav_data = pcm_to_wav(raw_pcm, sample_rate=sample_rate_hz)\n",
        "        with open(output_file, 'wb') as f:\n",
        "            f.write(wav_data)\n",
        "        print(f\"\\n‚úÖ Audio saved to: {output_file}\")\n",
        "        \n",
        "        return ipd.Audio(output_file)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Streaming failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Run streaming test with audio output\n",
        "test_tts_streaming(\n",
        "    text=\"Welcome to NVIDIA's text-to-speech streaming demonstration. \"\n",
        "         \"This test shows how audio chunks are delivered incrementally, \"\n",
        "         \"reducing time to first audio for real-time applications.\",\n",
        "    output_file=\"tts_streaming.wav\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test 8: Live Streaming Demo (Watch Chunks Arrive)\n",
        "\n",
        "This demonstrates real-time streaming - watch the audio chunks arrive live as they're generated by the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Live Streaming Demo - Watch chunks arrive in real-time like chat streaming\n",
        "# Each chunk is printed as it arrives from the model\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "import sys\n",
        "\n",
        "text = (\"This is a live streaming demonstration. Watch as each audio chunk arrives \"\n",
        "        \"from the NVIDIA NIM TTS model in real-time. This is similar to how chat \"\n",
        "        \"applications stream text tokens, but here we're streaming audio data.\")\n",
        "\n",
        "print(\"üé§ LIVE STREAMING DEMO\")\n",
        "print(f\"Text: '{text[:50]}...'\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "payload = {\n",
        "    \"text\": text,\n",
        "    \"language_code\": \"en-US\",\n",
        "    \"sample_rate_hz\": 44100,\n",
        "    \"encoding\": \"LINEAR_PCM\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    response = sm_runtime.invoke_endpoint_with_response_stream(\n",
        "        EndpointName=endpoint_name,\n",
        "        ContentType='application/json',\n",
        "        CustomAttributes='/invocations/stream',\n",
        "        Body=json.dumps(payload)\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    first_chunk_time = None\n",
        "    chunk_count = 0\n",
        "    total_bytes = 0\n",
        "    \n",
        "    for event in response['Body']:\n",
        "        if 'PayloadPart' in event:\n",
        "            chunk = event['PayloadPart']['Bytes']\n",
        "            if chunk:\n",
        "                chunk_count += 1\n",
        "                total_bytes += len(chunk)\n",
        "                \n",
        "                if first_chunk_time is None:\n",
        "                    first_chunk_time = time.time() - start_time\n",
        "                    print(f\"‚è±Ô∏è  First chunk arrived in {first_chunk_time:.3f}s!\")\n",
        "                    print(\"-\" * 70)\n",
        "                \n",
        "                elapsed = time.time() - start_time\n",
        "                # Print each chunk as it arrives - live streaming effect\n",
        "                preview = chunk[:16].hex()\n",
        "                print(f\"üì¶ Chunk {chunk_count:3d} | {len(chunk):6,} bytes | Total: {total_bytes:8,} | @{elapsed:.2f}s | {preview}...\")\n",
        "                sys.stdout.flush()  # Force immediate output\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"\\n‚úÖ Stream complete!\")\n",
        "    print(f\"   Total chunks: {chunk_count}\")\n",
        "    print(f\"   Total bytes: {total_bytes:,}\")\n",
        "    print(f\"   Total time: {total_time:.3f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Streaming failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transport Selection Summary\n",
        "\n",
        "| CustomAttributes Value | Description |\n",
        "|------------------------|-------------|\n",
        "| `/invocations/http` | Force HTTP transport |\n",
        "| `/invocations/grpc` | Force gRPC transport |\n",
        "| `/invocations/stream` | Enable streaming (uses gRPC) |\n",
        "| *(not set)* | Auto-routing: HTTP for simple, gRPC for advanced |\n",
        "\n",
        "**gRPC-only features** (auto-trigger gRPC in auto mode):\n",
        "- `zero_shot_data` - Voice cloning\n",
        "- `custom_dictionary` - Custom pronunciations\n",
        "\n",
        "### End-to-End Streaming\n",
        "\n",
        "For true end-to-end streaming:\n",
        "1. Use `CustomAttributes='/invocations/stream'` header\n",
        "2. Use SageMaker's `invoke_endpoint_with_response_stream` API (NOT regular `invoke_endpoint`)\n",
        "3. Streaming always uses gRPC internally\n",
        "\n",
        "```python\n",
        "# NIM SynthesizeSpeechRequest format (no stream field needed)\n",
        "payload = {\n",
        "    \"text\": \"Your text to synthesize\",\n",
        "    \"language_code\": \"en-US\",\n",
        "    \"sample_rate_hz\": 44100,\n",
        "    \"encoding\": \"LINEAR_PCM\"\n",
        "}\n",
        "\n",
        "# Use invoke_endpoint_with_response_stream with CustomAttributes\n",
        "response = sm_runtime.invoke_endpoint_with_response_stream(\n",
        "    EndpointName=endpoint_name,\n",
        "    ContentType='application/json',\n",
        "    CustomAttributes='/invocations/stream',\n",
        "    Body=json.dumps(payload)\n",
        ")\n",
        "```\n",
        "\n",
        "Streaming provides:\n",
        "- Lower time-to-first-audio (audio starts playing before full synthesis completes)\n",
        "- No message size limits (each chunk is small)\n",
        "- Better user experience for long texts\n",
        "- Returns raw PCM audio that needs WAV header conversion (see test_tts_streaming function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resource Cleanup\n",
        "\n",
        "**‚ö†Ô∏è Cost Warning**: Make sure to clean up resources when done testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup: Delete model, endpoint config, and endpoint\n",
        "sm.delete_model(ModelName=sm_model_name)\n",
        "print(f\"‚úÖ Model {sm_model_name} deleted\")\n",
        "\n",
        "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
        "print(f\"‚úÖ Endpoint config {endpoint_config_name} deleted\")\n",
        "\n",
        "sm.delete_endpoint(EndpointName=endpoint_name)\n",
        "print(f\"‚úÖ Endpoint {endpoint_name} deleted\")\n",
        "\n",
        "# Clean up generated audio files\n",
        "import glob\n",
        "audio_files = glob.glob(\"tts_*.wav\")\n",
        "for f in audio_files:\n",
        "    try:\n",
        "        os.remove(f)\n",
        "        print(f\"‚úÖ Removed {f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not remove {f}: {e}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
