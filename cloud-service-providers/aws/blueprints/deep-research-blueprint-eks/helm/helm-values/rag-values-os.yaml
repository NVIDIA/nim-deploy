# -- Global chart configuration
nameOverride: ""
fullnameOverride: "rag-server"
# subsection: rag-server
# RAG Orchestrator Service
# -- Kubernetes scheduling
nodeSelector: {}
affinity: {}
tolerations: []

# -- Common service account for rag-server
serviceAccount:
  create: true
  name: ""
  automount: true
  annotations: {}

# -- Replicas for rag-server
replicaCount: 1

# -- Namespace for documentation/reference; not actively used in templates
namespace: "nv-nvidia-blueprint-rag"

# -- Image pull secret for all images used by this chart
imagePullSecret:
  name: "ngc-secret"
  registry: "nvcr.io"
  username: "$oauthtoken"
  password: ""
  create: true

# -- Secret containing API keys for NVIDIA NGC model registry
ngcApiSecret:
  name: "ngc-api"
  password: ""
  create: true

# -- RAG server container image
image:
  repository: nvcr.io/nvidia/blueprint/rag-server
  tag: "2.3.0"
  pullPolicy: Always

# -- RAG server service configuration
service:
  type: ClusterIP
  port: 8081

# -- RAG server container resources
resources:
  limits:
    memory: "64Gi"
  requests:
    memory: "8Gi"

# -- Probes for rag-server (optional)
livenessProbe:
  httpGet:
    path: /health
    port: 8081
  initialDelaySeconds: 10
  periodSeconds: 15
  timeoutSeconds: 5
  failureThreshold: 3
readinessProbe:
  httpGet:
    path: /health
    port: 8081
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# -- RAG server runtime configuration
server:
  workers: 8

# -- Enable/disable creation of prompt ConfigMap
promptConfig:
  enabled: true

# -- Environment variables for rag-server
envVars:
  EXAMPLE_PATH: "./nvidia_rag/rag_server"
  PROMPT_CONFIG_FILE: "/prompt.yaml"
  PROMETHEUS_MULTIPROC_DIR: "/tmp-data/prom_data"

  ##===MINIO specific configurations used to store multimodal base64 content===
  MINIO_ENDPOINT: "rag-minio:9000"
  MINIO_ACCESSKEY: "minioadmin"
  MINIO_SECRETKEY: "minioadmin"

  ##===Vector DB specific configurations===
  # URL on which vectorstore is hosted
  APP_VECTORSTORE_URL: "https://your-opensearch-endpoint" # Use "http://elasticsearch:9200" for elasticsearch, "http://milvus:19530" for milvus
  # Type of vectordb used to store embedding - OpenSearch
  APP_VECTORSTORE_NAME: "opensearch" # supported values: "milvus", "elasticsearch", "opensearch"
  # Enable AWS SigV4 authentication for OpenSearch
  APP_VECTORSTORE_AWS_SIGV4: "true"
  # AWS service name for OpenSearch
  APP_VECTORSTORE_AWS_SERVICE: "aoss" # Use "aoss" for OpenSearch Serverless, "es" for OpenSearch Service
  # AWS region for OpenSearch
  APP_VECTORSTORE_AWS_REGION: "us-east-1"
  # Type of vectordb search to be used
  APP_VECTORSTORE_SEARCHTYPE: "dense"


  # vectorstore collection name to store embeddings
  COLLECTION_NAME: "multimodal_data"
  APP_RETRIEVER_SCORETHRESHOLD: "0.25"
  # Top K from vector DB, which goes as input to reranker model - not applicable if ENABLE_RERANKER is set to False
  VECTOR_DB_TOPK: "100"
  # Number of document chunks to insert in LLM prompt
  APP_RETRIEVER_TOPK: "10"

  ##===LLM Model specific configurations===
  APP_LLM_MODELNAME: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
  # URL on which LLM model is hosted. If "", Nvidia hosted API is used
  APP_LLM_SERVERURL: "nim-llm:8000"
  # LLM model parameters
  LLM_MAX_TOKENS: "32768"
  LLM_TEMPERATURE: "0"
  LLM_TOP_P: "1.0"

  ##===Query Rewriter Model specific configurations===
  APP_QUERYREWRITER_MODELNAME: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
  # URL on which query rewriter model is hosted. Uses main 49B model by default
  # Optional: Use AIRA's 8B model by setting: "nim-llm.nv-aira.svc.cluster.local:8000"
  APP_QUERYREWRITER_SERVERURL: "nim-llm:8000"

  ##===Filter Expression Generator Model specific configurations===
  APP_FILTEREXPRESSIONGENERATOR_MODELNAME: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
  # URL on which filter expression generator model is hosted. If "", Nvidia hosted API is used
  APP_FILTEREXPRESSIONGENERATOR_SERVERURL: "nim-llm:8000"
  # enable filter expression generator for natural language to filter expression conversion
  ENABLE_FILTER_GENERATOR: "False"

  ##===Embedding Model specific configurations===
  # URL on which embedding model is hosted. If "", Nvidia hosted API is used
  APP_EMBEDDINGS_SERVERURL: "nemoretriever-embedding-ms:8000"
  APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"

  ##===Reranking Model specific configurations===
  # URL on which ranking model is hosted. If "", Nvidia hosted API is used
  APP_RANKING_SERVERURL: "nemoretriever-ranking-ms:8000"
  APP_RANKING_MODELNAME: "nvidia/llama-3.2-nv-rerankqa-1b-v2"
  ENABLE_RERANKER: "True"
  # Default confidence threshold for filtering documents by reranker relevance scores (0.0 to 1.0)
  RERANKER_CONFIDENCE_THRESHOLD: "0.0"

  ##===VLM Model specific configurations===
  ENABLE_VLM_INFERENCE: "false"
  # Reasoning gate on VLM response
  ENABLE_VLM_RESPONSE_REASONING: "false"
  # Max images sent to VLM per request (query + context)
  APP_VLM_MAX_TOTAL_IMAGES: "4"
  # Use VLM for final response generation
  APP_VLM_RESPONSE_AS_FINAL_ANSWER: "true"
  # Max number of query images to include in VLM input
  APP_VLM_MAX_QUERY_IMAGES: "1"
  # Max number of context images to include in VLM input
  APP_VLM_MAX_CONTEXT_IMAGES: "1"
  APP_VLM_SERVERURL: "http://nim-vlm:8000/v1"
  APP_VLM_MODELNAME: "nvidia/llama-3.1-nemotron-nano-vl-8b-v1"

  # === Text Splitter ===
  APP_TEXTSPLITTER_CHUNKSIZE: "2000"
  APP_TEXTSPLITTER_CHUNKOVERLAP: "200"

  # === General ===
  # Choose whether to enable citations in the response
  ENABLE_CITATIONS: "True"
  # Choose whether to enable/disable guardrails
  ENABLE_GUARDRAILS: "False"
  # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
  LOGLEVEL: "INFO"
  # enable multi-turn conversation in the rag chain - this controls conversation history usage
  # while doing query rewriting and in LLM prompt
  ENABLE_MULTITURN: "True"
  # enable query rewriting for multiturn conversation in the rag chain.
  # This will improve accuracy of the retrieiver pipeline but increase latency due to an additional LLM call
  ENABLE_QUERYREWRITER: "False"
  # number of last n chat messages to consider from the provided conversation history
  CONVERSATION_HISTORY: "5"

  # === Tracing ===
  APP_TRACING_ENABLED: "False"
  # HTTP endpoint
  APP_TRACING_OTLPHTTPENDPOINT: "http://rag-opentelemetry-collector:4318/v1/traces"
  # GRPC endpoint
  APP_TRACING_OTLPGRPCENDPOINT: "grpc://rag-opentelemetry-collector:4317"

  # === Reflection ===
  # enable reflection (context relevance and response groundedness checking) in the rag chain
  ENABLE_REFLECTION: "false"
  # Maximum number of context relevance loop iterations
  MAX_REFLECTION_LOOP: "3"
  # Minimum relevance score threshold (0-2)
  CONTEXT_RELEVANCE_THRESHOLD: "1"
  # Minimum groundedness score threshold (0-2)
  RESPONSE_GROUNDEDNESS_THRESHOLD: "1"
  # reflection llm
  REFLECTION_LLM: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
  # reflection llm server url. If "", Nvidia hosted API is used
  REFLECTION_LLM_SERVERURL: "nim-llm:8000"

  # Choose whether to enable source metadata in document content during generation
  ENABLE_SOURCE_METADATA: "true"

  # Whether to filter content within <think></think> tags in model responses
  FILTER_THINK_TOKENS: "true"

  NEMO_GUARDRAILS_URL: "nemo-guardrails-microservice:7331"

  # enable iterative query decomposition
  ENABLE_QUERY_DECOMPOSITION: "false"
  # maximum recursion depth for iterative query decomposition
  MAX_RECURSION_DEPTH: "3"

# -- Ingestor Server
# subsection: ingestor-server
# Ingestor API Service
ingestor-server:
  enabled: true
  appName: ingestor-server

  # -- Pod scheduling
  nodeSelector: {}
  affinity: {}
  tolerations: []

  replicaCount: 1

  imagePullSecret:
    create: false
    name: "ngc-secret"
    registry: "nvcr.io"
    username: "$oauthtoken"
    password: ""

  image:
    repository: nvcr.io/nvidia/blueprint/ingestor-server
    tag: "2.3.0"
    pullPolicy: Always

  # -- Service config for ingestor-server
  service:
    type: ClusterIP
    port: 8082

  server:
    workers: 1

  # -- Probes for ingestor-server (optional)
  livenessProbe: {}
  readinessProbe: {}

  resources:
    limits:
      memory: "25Gi"
    requests:
      memory: "25Gi"

  envVars:
    # Path to example directory relative to repo root
    EXAMPLE_PATH: "src/nvidia_rag/ingestor_server"
    # Absolute path to custom prompt.yaml file
    PROMPT_CONFIG_FILE: "/prompt.yaml"
    # === Vector Store Configurations ===
    APP_VECTORSTORE_URL: "https://your-opensearch-endpoint" # Use "http://elasticsearch:9200" for elasticsearch, "http://milvus:19530" for milvus
    APP_VECTORSTORE_NAME: "opensearch" # supported values: "milvus", "elasticsearch", "opensearch"
    # Enable AWS SigV4 authentication for OpenSearch
    APP_VECTORSTORE_AWS_SIGV4: "true"
    # AWS service name for OpenSearch
    APP_VECTORSTORE_AWS_SERVICE: "aoss" # Use "aoss" for OpenSearch Serverless, "es" for OpenSearch Service
    # AWS region for OpenSearch
    APP_VECTORSTORE_AWS_REGION: "us-east-1"
    APP_VECTORSTORE_SEARCHTYPE: "dense"
    COLLECTION_NAME: "multimodal_data"


    # === MinIO Configurations ===
    MINIO_ENDPOINT: "rag-minio:9000"
    MINIO_ACCESSKEY: "minioadmin"
    MINIO_SECRETKEY: "minioadmin"

    # === Embeddings Configurations ===
    APP_EMBEDDINGS_SERVERURL: "nemoretriever-embedding-ms:8000"
    APP_EMBEDDINGS_MODELNAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"
    APP_EMBEDDINGS_DIMENSIONS: "2048"

    # === NV-Ingest Configurations ===
    APP_NVINGEST_MESSAGECLIENTHOSTNAME: "rag-nv-ingest"
    APP_NVINGEST_MESSAGECLIENTPORT: "7670"

    # === NV-Ingest extraction configurations ===
    APP_NVINGEST_PDFEXTRACTMETHOD: "None"  # Method used for text extraction from "None", "pdfium", "nemoretriever_parse"
    APP_NVINGEST_EXTRACTTEXT: "True"  # Enable text extraction
    APP_NVINGEST_EXTRACTINFOGRAPHICS: "False"  # Enable infographic extraction
    APP_NVINGEST_EXTRACTTABLES: "True"  # Enable table extraction
    APP_NVINGEST_EXTRACTCHARTS: "True"  # Enable chart extraction
    APP_NVINGEST_EXTRACTIMAGES: "False"  # Enable image extraction
    APP_NVINGEST_EXTRACTPAGEASIMAGE: "False"  # Extracts each page as image if enabled
    APP_NVINGEST_STRUCTURED_ELEMENTS_MODALITY: ""  # "image", "text_image"
    APP_NVINGEST_IMAGE_ELEMENTS_MODALITY: ""  # "image"
    APP_NVINGEST_TEXTDEPTH: "page"  # Extract text by "page" or "document"

    # === NV-Ingest caption configurations ===
    APP_NVINGEST_CAPTIONMODELNAME: "nvidia/llama-3.1-nemotron-nano-vl-8b-v1"  # Model name for captioning
    APP_NVINGEST_CAPTIONENDPOINTURL: ""  # Endpoint URL for captioning model

    # === NV-Ingest save to disk configurations ===
    APP_NVINGEST_SAVETODISK: "False"
    NVINGEST_MINIO_BUCKET: "nv-ingest"

    # === General ===
    # Summary Model Configurations
    SUMMARY_LLM: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
    SUMMARY_LLM_SERVERURL: "nim-llm:8000"
    SUMMARY_LLM_MAX_CHUNK_LENGTH: "50000"
    SUMMARY_CHUNK_OVERLAP: "200"

    # === General ===
    ENABLE_CITATIONS: "True"
    LOGLEVEL: "INFO"

    # === NV-Ingest splitting configurations ===
    APP_NVINGEST_CHUNKSIZE: "512"  # Size of chunks for splitting
    APP_NVINGEST_CHUNKOVERLAP: "150"  # Overlap size for chunks
    APP_NVINGEST_ENABLEPDFSPLITTER: "True"  # Enable PDF splitter
    APP_NVINGEST_SEGMENTAUDIO: "False"  # Enable audio segmentation for NV Ingest

    # === Redis configurations ===
    REDIS_HOST: "rag-redis-master"
    REDIS_PORT: "6379"
    REDIS_DB: "0"

    # === Bulk upload to MinIO ===
    ENABLE_MINIO_BULK_UPLOAD: "True"
    TEMP_DIR: "/tmp-data"
    INGESTOR_SERVER_DATA_DIR: "/data/"

    # === NV-Ingest Batch Mode Configurations ===
    NV_INGEST_FILES_PER_BATCH: "16"
    NV_INGEST_CONCURRENT_BATCHES: "4"

    # === Document Validation Configurations (Advanced - Optional Override) ===
    # Validation retries are automatically optimized based on APP_VECTORSTORE_AWS_SERVICE
    # Only override if you have specific timing requirements
    # VALIDATION_MAX_RETRIES: "10"  # Uncomment to override (AOSS default: 10, Regular: 5)

  # -- Persistent storage for ingestor-server data directory
  # If enabled, mounts a PVC at the path used by INGESTOR_SERVER_DATA_DIR
  persistence:
    enabled: true
    # If set, use an existing PVC by name; when empty and enabled, a PVC is created
    existingClaim: ""
    # StorageClass to use for the PVC; keep empty to use the cluster default
    storageClass: ""
    # Access modes for the PVC
    accessModes:
      - ReadWriteOnce
    # Requested size for the PVC
    size: 50Gi
    # Mount path inside the container; defaults to envVars.INGESTOR_SERVER_DATA_DIR if empty
    mountPath: "/data/"
    # Optional subPath within the PVC
    subPath: ""

# -- Frontend
# subsection: frontend
# RAG Playground Frontend
frontend:
  enabled: true
  appName: "rag-frontend"

  replicaCount: 1

  image:
    repository: nvcr.io/nvidia/blueprint/rag-frontend
    pullPolicy: IfNotPresent
    tag: "2.3.0"

  imagePullSecret:
    name: "ngc-secret"
    registry: "nvcr.io"
    username: "$oauthtoken"
    password: ""

  service:
    type: NodePort
    port: 3000

  # -- Probes for frontend (optional)
  livenessProbe: {}
  readinessProbe: {}

  envVars:
    # Runtime environment variables for Vite frontend
    # Note: Model names are now managed by frontend settings store
    - name: VITE_API_CHAT_URL
      value: "http://rag-server:8081/v1"
    - name: VITE_API_VDB_URL
      value: "http://ingestor-server:8082/v1"
    - name: VITE_MILVUS_URL
      value: "http://milvus:19530"

# -- Elasticsearch dependency toggle
# subsection: elasticsearch
elasticsearch:
  enabled: false
  fullnameOverride: elasticsearch
  security:
    enabled: false
    tls:
      restEncryption: false
  extraConfig:
    discovery.type: single-node
  master:
    masterOnly: false
    replicaCount: 1
  data:
    replicaCount: 0
  coordinating:
    replicaCount: 0
  ingest:
    enabled: false

# -- Observability
# subsection: serviceMonitor
serviceMonitor:
  enabled: false

# subsection: opentelemetry-collector
opentelemetry-collector:
  enabled: false
  mode: deployment
  image:
    repository: otel/opentelemetry-collector-contrib
    tag: "0.131.0"
  command:
    name: otelcol-contrib
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: '${env:MY_POD_IP}:4317'
          http:
            cors:
              allowed_origins:
                - "*"
    exporters:
      # NOTE: Prior to v0.86.0 use `logging` instead of `debug`.
      zipkin:
        endpoint: "http://rag-zipkin:9411/api/v2/spans"
      debug:
        verbosity: detailed
      prometheus:
        endpoint: ${env:MY_POD_IP}:8889
    extensions:
      health_check: {}
      zpages:
        endpoint: 0.0.0.0:55679
    processors:
      batch: {}
      tail_sampling:
        # filter out health checks
        # https://github.com/open-telemetry/opentelemetry-collector/issues/2310#issuecomment-1268157484
        policies:
          - name: drop_noisy_traces_url
            type: string_attribute
            string_attribute:
              key: http.target
              values:
                - \/health
              enabled_regex_matching: true
              invert_match: true
      transform:
        trace_statements:
          - context: span
            statements:
              - set(status.code, 1) where attributes["http.path"] == "/health"

              # after the http target has been anonymized, replace other aspects of the span
              - replace_match(attributes["http.route"], "/v1", attributes["http.target"]) where attributes["http.target"] != nil

              # replace the title of the span with the route to be more descriptive
              - replace_pattern(name, "/v1", attributes["http.route"]) where attributes["http.route"] != nil

              # set the route to equal the URL if it's nondescriptive (for the embedding case)
              - set(name, Concat([name, attributes["http.url"]], " ")) where name == "POST"
    service:
      extensions: [zpages, health_check]
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [debug, zipkin]
          processors: [tail_sampling, transform]
        metrics:
          exporters:
            - debug
            - prometheus
          processors:
            - memory_limiter
            - batch
          receivers:
            - otlp
            - prometheus
        logs:
          receivers: [otlp]
          exporters: [debug]
          processors: [batch]
  ports:
    metrics:
      enabled: true
      containerPort: 8889
      servicePort: 8889
      protocol: TCP

# subsection: zipkin
zipkin:
  enabled: false

# subsection: kube-prometheus-stack
kube-prometheus-stack:
  enabled: false
  prometheus:
    serviceMonitor:
      interval: "1s"
    prometheusSpec:
      scrapeInterval: "1s"
      evaluationInterval: "1s"
  grafana:
    adminUser: admin
    adminPassword: "admin"

# -- NIMs (dependencies) configuration
# subsection: nim-llm
# NIM LLM
nim-llm:
  enabled: true
  service:
    name: "nim-llm"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.3-nemotron-super-49b-v1.5
    pullPolicy: IfNotPresent
    tag: "1.13.1"
  resources:
    limits:
      nvidia.com/gpu: 8
    requests:
      nvidia.com/gpu: 8
  # Pin to Main Node 1 (all 8 GPUs for tensor parallelism)
  nodeSelector:
    kubernetes.io/hostname: ip-192-168-3-186.ec2.internal
  env:
    - name: NIM_MODEL_PROFILE
      value: "5f91fb3820f2f0b0b172f7832180b360248ae4e1f62bf15e0d2752935e33c4cf" # tensorrt_llm-a10g-bf16-tp8-pp1-latency For A10G latency optimized profile
    # - name: NIM_MODEL_PROFILE
    #   value: "16f1bbf92cfddc37e3c770c7efa4e9632ec4f694e677ae728d6b8cc3501c52e0" # tensorrt_llm-a10g-bf16-tp8-pp1-throughput For A10G throughput optimized profile
    - name: NIM_GUIDED_DECODING_BACKEND
      value: "outlines"
  model:
    ngcAPIKey: ""
    name: "nvidia/llama-3.3-nemotron-super-49b-v1.5"
    hfTokenSecret: ""


# subsection: nvidia-nim-llama-32-nv-embedqa-1b-v2
# NIM Text Embedding
nvidia-nim-llama-32-nv-embedqa-1b-v2:
  enabled: true
  service:
    name: "nemoretriever-embedding-ms"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-embedqa-1b-v2
    tag: "1.10.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  # Pin to Secondary Node
  nodeSelector:
    kubernetes.io/hostname: ip-192-168-19-220.ec2.internal
  nim:
    ngcAPIKey: ""

# subsection: nvidia-nim-llama-32-nemoretriever-1b-vlm-embed-v1
# NIM VLM Embedding
nvidia-nim-llama-32-nemoretriever-1b-vlm-embed-v1:
  enabled: false
  service:
    name: "nemoretriever-vlm-embedding-ms"
  image:
    repository: nvcr.io/nvidia/nemo-microservices/llama-3.2-nemoretriever-1b-vlm-embed-v1
    tag: "1.7.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# subsection: text-reranking-nim
# NIM Text Reranking
nvidia-nim-llama-32-nv-rerankqa-1b-v2:
  enabled: true
  service:
    name: "nemoretriever-ranking-ms"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.2-nv-rerankqa-1b-v2
    tag: "1.8.0"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  # Pin to Secondary Node
  nodeSelector:
    kubernetes.io/hostname: ip-192-168-19-220.ec2.internal
  nim:
    ngcAPIKey: ""

# subsection: nim-vlm
# NIM Vision-Language (VLM)
nim-vlm:
  enabled: false
  service:
    name: "nim-vlm"
  image:
    repository: nvcr.io/nim/nvidia/llama-3.1-nemotron-nano-vl-8b-v1
    tag: "1.3.1"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nim:
    ngcAPIKey: ""

# -- NV-Ingest dependency configuration
# subsection: nv-ingest
# NV-Ingest Service
nv-ingest:
  enabled: true
  imagePullSecrets:
    - name: "ngc-secret"
  ngcApiSecret:
    create: false
  ngcImagePullSecret:
    create: false
  image:
    repository: "nvcr.io/nvidia/nemo-microservices/nv-ingest"
    tag: "25.9.0"
  resources:
    limits:
      nvidia.com/gpu: 0
  envVars:
    ARROW_DEFAULT_MEMORY_POOL: "system"
    INGEST_LOG_LEVEL: DEFAULT
    INGEST_RAY_LOG_LEVEL: "PRODUCTION"
    NV_INGEST_MAX_UTIL: 48
    INGEST_EDGE_BUFFER_SIZE: 64
    INGEST_DYNAMIC_MEMORY_THRESHOLD: "0.80"
    INGEST_DISABLE_DYNAMIC_SCALING: "false"
    MRC_IGNORE_NUMA_CHECK: 1
    READY_CHECK_ALL_COMPONENTS: "true"
    COMPONENTS_TO_READY_CHECK: "ALL"
    REDIS_MORPHEUS_TASK_QUEUE: morpheus_task_queue
    REDIS_INGEST_TASK_QUEUE: "ingest_task_queue"
    NV_INGEST_DEFAULT_TIMEOUT_MS: "1234"
    MAX_INGEST_PROCESS_WORKERS: 16
    EMBEDDING_NIM_ENDPOINT: "http://nemoretriever-embedding-ms:8000/v1"
    EMBEDDING_NIM_MODEL_NAME: "nvidia/llama-3.2-nv-embedqa-1b-v2"
    MESSAGE_CLIENT_HOST: "rag-redis-master"
    MESSAGE_CLIENT_PORT: 6379
    MESSAGE_CLIENT_TYPE: "redis"
    MINIO_INTERNAL_ADDRESS: "rag-minio:9000"
    MINIO_PUBLIC_ADDRESS: "http://localhost:9000"
    MINIO_BUCKET: "nv-ingest"
    MILVUS_ENDPOINT: "http://milvus:19530"
    OTEL_EXPORTER_OTLP_ENDPOINT: "otel-collector:4317"
    MODEL_PREDOWNLOAD_PATH: "/workspace/models/"
    INSTALL_AUDIO_EXTRACTION_DEPS: "true"

    # Paddle OCR endpoints
    PADDLE_GRPC_ENDPOINT: nv-ingest-ocr:8001
    PADDLE_HTTP_ENDPOINT: http://nv-ingest-ocr:8000/v1/infer
    PADDLE_INFER_PROTOCOL: grpc

    # OCR routing (defaults to Paddle OCR service)
    OCR_GRPC_ENDPOINT: nv-ingest-ocr:8001
    OCR_HTTP_ENDPOINT: http://nv-ingest-ocr:8000/v1/infer
    OCR_INFER_PROTOCOL: grpc
    OCR_MODEL_NAME: paddle

    # NeMo Retriever Parse (VLM text extraction)
    NEMORETRIEVER_PARSE_HTTP_ENDPOINT: http://nim-vlm-text-extraction-nemoretriever-parse:8000/v1/chat/completions
    NEMORETRIEVER_PARSE_INFER_PROTOCOL: http
    NEMORETRIEVER_PARSE_MODEL_NAME: nvidia/nemoretriever-parse

    # YOLOX endpoints
    YOLOX_GRPC_ENDPOINT: nemoretriever-page-elements-v2:8001
    YOLOX_HTTP_ENDPOINT: http://nemoretriever-page-elements-v2:8000/v1/infer
    YOLOX_INFER_PROTOCOL: grpc
    YOLOX_GRAPHIC_ELEMENTS_GRPC_ENDPOINT: nemoretriever-graphic-elements-v1:8001
    YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT: http://nemoretriever-graphic-elements-v1:8000/v1/infer
    YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL: grpc
    YOLOX_TABLE_STRUCTURE_GRPC_ENDPOINT: nemoretriever-table-structure-v1:8001
    YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT: http://nemoretriever-table-structure-v1:8000/v1/infer
    YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL: grpc

    # Captioning
    VLM_CAPTION_MODEL_NAME: nvidia/llama-3.1-nemotron-nano-vl-8b-v1
    VLM_CAPTION_ENDPOINT: http://nim-vlm:8000/v1/chat/completions

    # Audio service
    AUDIO_GRPC_ENDPOINT: nv-ingest-riva-nim:50051
    AUDIO_INFER_PROTOCOL: grpc

  # Expose internal Milvus/MinIO config managed by nv-ingest subchart
  milvusDeployed: true
  milvus:
    image:
      all:
        repository: milvusdb/milvus
        tag: v2.5.17
    etcd:
      image:
        repository: "milvusdb/etcd"
        tag: "3.5.22-r1"
    standalone:
      resources:
        limits:
          nvidia.com/gpu: 0
      # Pin to Secondary Node
      nodeSelector:
        kubernetes.io/hostname: ip-192-168-19-220.ec2.internal
    minio:
      image:
        repository: minio/minio
        tag: "RELEASE.2025-09-07T16-13-09Z"
      accessKey: minioadmin
      secretKey: minioadmin
      bucketName: nv-ingest
    fullnameOverride: milvus

  # Ensure nv-ingest does not deploy its own embedding NIM
  nvidia-nim-llama-32-nv-embedqa-1b-v2:
    deployed: false

  # Redis Master
  redis:
    image:
      repository: redis
      tag: 8.2.1

  # Ensure nv-ingest does not deploy its own observability components
  otelDeployed: false
  zipkinDeployed: false

  # Sub-NIMs deployed by NV-Ingest
  # NIM OCR (PaddleOCR)
  paddleocr-nim:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/baidu/paddleocr
      tag: "1.5.0"
    imagePullSecrets:
      - name: ngc-secret
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_RATE_LIMIT
        value: "3"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "3072"
      - name: OMP_NUM_THREADS
        value: "8"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
    # Pin to Data Ingestion Node
    nodeSelector:
      kubernetes.io/hostname: ip-192-168-39-59.ec2.internal

  # Nemo Retriever OCR
  nemoretriever-ocr:
    deployed: false
    replicaCount: 1
    image:
      repository: nvcr.io/nvidia/nemo-microservices/nemoretriever-ocr-v1
      tag: "1.1.0"
    imagePullSecrets:
      - name: ngc-secret
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: NIM_TRITON_RATE_LIMIT
        value: "1"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_MAX_BATCH_SIZE
        value: "32"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "2048"
      - name: NIM_TRITON_CPU_THREADS_PRE_PROCESSOR
        value: "2"
      - name: NIM_TRITON_CPU_THREADS_POST_PROCESSOR
        value: "1"
      - name: OMP_NUM_THREADS
        value: "1"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1

  # NIM Graphic Elements
  nemoretriever-graphic-elements-v1:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-graphic-elements-v1
      tag: "1.5.0"
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: NIM_TRITON_RATE_LIMIT
        value: "3"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_MAX_BATCH_SIZE
        value: "32"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "2048"
      - name: OMP_NUM_THREADS
        value: "1"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
    # Pin to Data Ingestion Node
    nodeSelector:
      kubernetes.io/hostname: ip-192-168-39-59.ec2.internal

  # NIM Page Elements
  nemoretriever-page-elements-v2:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-page-elements-v2
      tag: "1.5.0"
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: NIM_TRITON_RATE_LIMIT
        value: "3"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_MAX_BATCH_SIZE
        value: "32"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "2048"
      - name: NIM_TRITON_CPU_THREADS_PRE_PROCESSOR
        value: "2"
      - name: NIM_TRITON_CPU_THREADS_POST_PROCESSOR
        value: "1"
      - name: OMP_NUM_THREADS
        value: "2"
      - name: NIM_ENABLE_OTEL
        value: "true"
      - name: NIM_OTEL_SERVICE_NAME
        value: "page-elements"
      - name: NIM_OTEL_TRACES_EXPORTER
        value: "otlp"
      - name: NIM_OTEL_METRICS_EXPORTER
        value: "console"
      - name: NIM_OTEL_EXPORTER_OTLP_ENDPOINT
        value: "http://otel-collector:4318"
      - name: TRITON_OTEL_URL
        value: "http://otel-collector:4318/v1/traces"
      - name: TRITON_OTEL_RATE
        value: "1"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
    # Pin to Data Ingestion Node
    nodeSelector:
      kubernetes.io/hostname: ip-192-168-39-59.ec2.internal

  # NIM Table Structure
  nemoretriever-table-structure-v1:
    deployed: true
    replicaCount: 1
    image:
      repository: nvcr.io/nim/nvidia/nemoretriever-table-structure-v1
      tag: "1.5.0"
    env:
      - name: NIM_HTTP_API_PORT
        value: "8000"
      - name: NIM_TRITON_LOG_VERBOSE
        value: "1"
      - name: NIM_TRITON_RATE_LIMIT
        value: "3"
      - name: CUDA_VISIBLE_DEVICES
        value: "0"
      - name: NIM_TRITON_MAX_BATCH_SIZE
        value: "32"
      - name: NIM_TRITON_CUDA_MEMORY_POOL_MB
        value: "2048"
      - name: OMP_NUM_THREADS
        value: "1"
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
    # Pin to Data Ingestion Node
    nodeSelector:
      kubernetes.io/hostname: ip-192-168-39-59.ec2.internal
