## @section Deployment parameters
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
## @param affinity [object] [default: {}] Affinity settings for deployment. Allows to constraint pods to nodes.
affinity: {}

## @param containerSecurityContext [object] Specify privilege and access control settings for Container(Only affects the main container)
containerSecurityContext: {}
  # seLinuxOptions: null
  # runAsUser: 1001
  # runAsGroup: 1001
  # runAsNonRoot: true
  # privileged: false
  # readOnlyRootFilesystem: false
  # allowPrivilegeEscalation: false
  # capabilities:
  #   drop:
  #     - ALL
  # seccompProfile:
  #   type: "RuntimeDefault"

## @param customCommand [array] overrides command line options sent to the NeMo Inference service with the array listed here. 
## For andvanced use if necessary only
customCommand: []

## @param env [array] Adds arbitrary environment variables to the main container
env: []
# name: OPENBLAS_NUM_THREADS
# value: "1"
# name: POD_NAME
# valueFrom:
#   fieldRef:
#     fieldPath: metadata.name

## @param extraVolumes [object] Adds arbitrary additional volumes to the deployment set definition
extraVolumes: {}
  # my-volume-name:
  #   emptyDir: {}

## @param extraVolumeMounts [object] Specify volume mounts to the main container from extraVolumes
extraVolumeMounts: {}
  # my-volume-name:
  #   mountPath: /mnt/myvolume

## @param image.repository [string] NIM-LLM Image Repository
## @param image.tag [string] Image tag
## @param image.pullPolicy [string] Image pull policy
image:
  repository: nvcr.io/nim/meta/llama3-8b-instruct
  pullPolicy: IfNotPresent
  # Tag overrides the image tag whose default is the chart appVersion.
  tag: ""

## @extra imagePullSecrets Specify list of secret names that are needed for the main container and any init containers.
## @skip imagePullSecrets[0].name
imagePullSecrets:
  - name: nvcrimagepullsecret  # change this to whatever your image pull secret should be

## @extra initContainers [object] Specify model init containers, select only one, if needed.
## @param initContainers.ngcInit [object] Legacy containers only. Specify NGC init container. It should either have ngc cli pre-installed or wget + unzip pre-installed -- must not be musl-based (alpine).
## @param initContainers.extraInit [object] Specify any additional init containers your use case requires.
initContainers:
  ngcInit: {} # disabled by default
  #  imageName: mydocker-registry/myngcimage # should either have ngc cli pre-installed or wget + unzip pre-installed -- must not be musl-based (alpine)
  #  imageTag: 0.1
  #  secretName: # name of kube secret for ngc keys named NGC_CLI_API_KEY (required) and NGC_DECRYPT_KEY (optional)
  #  containerSecurityContext: # ONLY include if you are setting this for the main container
  #    seLinuxOptions: null
  #    runAsUser: 1001  # may not be able to do an NGC CLI install without root if you choose this
  #    runAsGroup: 1001
  #    runAsNonRoot: true
  #    privileged: false
  #    readOnlyRootFilesystem: false
  #    allowPrivilegeEscalation: false
  #    capabilities:
  #      drop:
  #        - ALL
  #    seccompProfile:
  #      type: "RuntimeDefault"
  #  env:
  #    STORE_MOUNT_PATH: /nvme/model-store
  #    NGC_CLI_ORG: # ngc org where model lives
  #    NGC_CLI_TEAM: # ngc team where model lives
  #    NGC_MODEL_NAME: # model name in ngc
  #    NGC_MODEL_VERSION: # model version in ngc
  #    NGC_EXE: ngc  # path to ngc cli, if pre-installed in container
  #    DOWNLOAD_NGC_CLI: "false"  # set to string 'true' if container should download and install ngc cli
  #    NGC_CLI_VERSION: "3.34.1"  # version of ngc cli to download (only matters if downloading)
  #    TARFILE: yes  # tells the script to untar the model. Set to "" to turn off
  #    MODEL_NAME: # actual model name, once downloaded
  extraInit: [] # Add any additional init containers your use case requires.
  # -  # full init container definition here

## @param healthPort [string] Specify health check port. -- for use with models.legacyCompat only since current NIMs have no separate port
healthPort: 8000 # Set this to the required healthcheck port

## @param nodeSelector [object] Specify labels to ensure that NeMo Inference is deployed only on certain nodes (likely best to set this to `nvidia.com/gpu.present: "true"` depending on cluster setup).
nodeSelector: {}  # likely best to set this to `nvidia.com/gpu.present: "true"` depending on cluster setup

## @param podAnnotations [object] Specify additional annotation to the main deployment pods
podAnnotations: {}

## @extra podSecurityContext Specify privilege and access control settings for pod (Only affects the main pod).
## @param podSecurityContext.runAsUser Specify user UID for pod.
## @param podSecurityContext.runAsGroup Specify group ID for pod.
## @param podSecurityContext.fsGroup Specify file system owner group id.
podSecurityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

## @param replicaCount Specify replica count for deployment.
replicaCount: 1

## @extra resources [object] Specify resources limits and requests for the running service. 
## @param resources.limits.nvidia.com/gpu Specify number of GPUs to present to the running service.
resources:
  limits:
    nvidia.com/gpu: 1  # Number of GPUs to present to the running service

## @exta serviceAccount Options to specify service account for the deployment.
## @param serviceAccount.create Specifies whether a service account should be created.
## @param serviceAccount.annotations [object] Specifies annotations to be added to the service account.
## @param serviceAccount.name Specify name of the service account to use. If it is not set and create is true, a name is generated using a fullname template.
serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

## @param statefulSet.enabled Enables statefulset deployment. Enabling statefulSet allows PVC templates for scaling. If using central PVC with RWX accessMode, this isn't needed.
statefulSet:
  enabled: true

## @extra tolerations Specify tolerations for pod assignment. Allows the scheduler to schedule pods with matching taints.
## @skip tolerations[0].key
## @skip tolerations[0].operator
## @skip tolerations[0].effect
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"


## @section Autoscaling parameters
## @descriptionStart
## Values used for autoscaling. If autoscaling is not enabled, these are ignored.
## They should be overridden on a per-model basis based on quality-of-service metrics as well as cost metrics.
## This isn't recommended except with usage of the custom metrics API using something like the prometheus-adapter.
## Standard metrics of CPU and memory are of limited use in scaling NIM
## @descriptionEnd
## @param autoscaling.enabled Enable horizontal pod autoscaler.
## @param autoscaling.minReplicas Specify minimum replicas for autoscaling.
## @param autoscaling.maxReplicas Specify maximum replicas for autoscaling.
## @param autoscaling.metrics Array of metrics for autoscaling.
autoscaling:
  enabled: false 
  minReplicas: 1
  maxReplicas: 10
  metrics: []


## @section Ingress parameters
## @param ingress.enabled Enables ingress.
## @param ingress.className Specify class name for Ingress.
## @param ingress.annotations Specify additional annotations for ingress.
## @extra ingress.hosts Specify list of hosts each containing lists of paths.
## @param ingress.hosts[0].host Specify name of host.
## @param ingress.hosts[0].paths[0].path Specify ingress path. 
## @param ingress.hosts[0].paths[0].pathType Specify path type.
## @param ingress.hosts[0].paths[0].serviceType Specify service type. It can be can be nemo or openai -- make sure your model serves the appropriate port(s).
## @param ingress.tls Specify list of pairs of TLS secretName and hosts.
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
          serviceType: openai  # can be nemo or openai -- make sure your model serves the appropriate port(s)

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local


## @section Probe parameters
## @param livenessProbe.enabled Enable livenessProbe
## @param livenessProbe.method LivenessProbe http or script, but no script is currently provided
## @param livenessProbe.command LivenessProbe script command to use (unsupported at this time)
## @param livenessProbe.path LivenessProbe endpoint path
## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
## @param livenessProbe.periodSeconds Period seconds for livenessProbe
## @param livenessProbe.successThreshold Success threshold for livenessProbe
## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
livenessProbe:
  enabled: true
  method: http
  command:
    - myscript.sh
  path: /v1/health/live  # correct for LLM container
  initialDelaySeconds: 15
  timeoutSeconds: 1
  periodSeconds: 10
  successThreshold: 1
  failureThreshold: 3

## @param readinessProbe.enabled Enable readinessProbe
## @param readinessProbe.path Readiness Endpoint Path
## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
## @param readinessProbe.periodSeconds Period seconds for readinessProbe
## @param readinessProbe.successThreshold Success threshold for readinessProbe
## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
readinessProbe:
  enabled: true
  path: /v1/health/ready  # correct for LLM container
  initialDelaySeconds: 15
  timeoutSeconds: 1
  periodSeconds: 10
  successThreshold: 1
  failureThreshold: 3

## @param startupProbe.enabled Enable startupProbe
## @param startupProbe.path StartupProbe Endpoint Path
## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
## @param startupProbe.periodSeconds Period seconds for startupProbe
## @param startupProbe.successThreshold Success threshold for startupProbe
## @param startupProbe.failureThreshold Failure threshold for startupProbe
startupProbe:
  enabled: true
  path: /v1/health/ready  # correct for LLM container
  initialDelaySeconds: 40
  timeoutSeconds: 1
  periodSeconds: 10
  successThreshold: 1
  failureThreshold: 180


## @section Metrics parameters
## @extra metrics Opens the metrics port for the triton inference server on port 8002.
## @param metrics.enabled Enables metrics endpoint -- for legacyCompat only since current NIMs serve metrics on the OpenAI API port always
## @extra serviceMonitor Options for serviceMonitor to use the Prometheus Operator and the primary service object.
## @param metrics.serviceMonitor.enabled Enables serviceMonitor creation.
## @param metrics.serviceMonitor.additionalLabels [object] Specify additional labels for ServiceMonitor.
metrics:
  enabled: true
  serviceMonitor:  # for use with the Prometheus Operator and the primary service object
    enabled: false
    additionalLabels: {}


## @section Models parameters
## @param model.nimCache [string] Path to mount writeable storage or pre-filled model cache for the NIM
## @param model.name Specify name of the model in the API (name of the NIM). Mostly used for tests (optional otherwise). This must match the name from `/v1/models` to allow `helm test <release-name>` to work. In legacyCompat, this is required and sets the name of the model in /v1/models
## @param model.ngcAPISecret [string] Name of pre-existing secret with a key named NGC_CLI_API_KEY that contains an API key for NGC model downloads
## @param model.ngcAPIKey [string] NGC API key literal to use as the API secret and image pull secret when set
## @param model.openaiPort Specify Open AI Port.
## @param model.labels [object] Specify extra labels to be added on deployed pods.
## @param model.jsonLogging Turn jsonl logging on or off. Defaults to true.
## @param model.logLevel Log level of NIM service. Possible values of the variable are TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL.
## @section Deprecated and Legacy Model parameters
## @param model.legacyCompat Set `true` to enable compatiblity with pre-release NIM versions prior to 1.0.0.
## @param model.numGpus (deprecated) Specify GPU requirements for the model.
## @param model.subPath (deprecated) Specify path within the model volume to mount if not the root -- default works with ngcInit and persistent volume. (legacyCompat only)
## @param model.modelStorePath (deprecated) Specify location of unpacked model.
model:
  nimCache: /model-store
  name: meta/llama3-8b-instruct # optional name of the model in the OpenAI API -- used in `helm test`
  ngcAPISecret: "ngc-api"
  ngcAPIKey: ""
  openaiPort: 8000
  labels: {}  # any extra labels desired on deployed pods
  jsonLogging: true
  logLevel: INFO
  # All options below are deprecated
  legacyCompat: false # enable compatiblity with pre-release NIM versions prior to 1.0.0
  numGpus: 1 # GPU requirements for the model
  subPath: model-store # legacyCompat only -- path within the model volume to mount if not the root -- default works with ngcInit and persistent volume
  # openai_port: 8005 # deprecated for of openaiPort for backward legacy compatibility
  # nemo_port: 8006  # deprecated for backward legacy compatibility
  modelStorePath: ""  # location of unpacked model
  # numWorkers: 1
  # trtModelName: "trt_llm"  # is not required in most cases -- name of trtllm model
  # tritonModelName: "ensemble"  # not required for NVIDIA trtllm models
  # dataStoreURL: "gateway-api:9009"  # URL of data store service, if used
  # customizationSource: "NDS"  # Type of customization source, can be NDS, SERVICE or DIRECTORY -- optional


## @section Storage parameters
## @extra persistence Specify settings to modify the path `/model-store` if `model.legacyCompat` is enabled else `/.cache` volume where the model is served from.
## @param persistence.enabled Enable persistent volumes.
## @param persistence.existingClaim Secify existing claim. If using existingClaim, run only one replica or use a ReadWriteMany storage setup.
## @param persistence.storageClass [nullable] Specify persistent volume storage class. If set to "-", storageClassName: "", which disables dynamic provisioning. If undefined (the default) or set to null, no storageClassName spec is  set, choosing the default provisioner.
## @param persistence.accessMode Specify accessModes. If using an NFS or similar setup, you can use ReadWriteMany.
## @param persistence.stsPersistentVolumeClaimRetentionPolicy.whenDeleted Specify persistent volume claim retention policy when deleted. Only used with Stateful Set volume templates.
## @param persistence.stsPersistentVolumeClaimRetentionPolicy.whenScaled Specifypersistent volume claim retention policy when scaled. Only used with Stateful Set volume templates.
## @param persistence.size Specify size of claim (e.g. 8Gi).
## @param persistence.annotations [object] Specify annotations to be added to persistent volume.
persistence:
  enabled: false
  existingClaim: ""  # if using existingClaim, run only one replica or use a ReadWriteMany storage setup
  # Persistent Volume Storage Class
  # If defined, storageClassName: <storageClass>
  # If set to "-", storageClassName: "", which disables dynamic provisioning.
  # If undefined (the default) or set to null, no storageClassName spec is
  #   set, choosing the default provisioner.
  storageClass: ""
  accessMode: ReadWriteOnce  # If using an NFS or similar setup, you can use ReadWriteMany
  stsPersistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  size: 50Gi  # size of claim in bytes (e.g. 8Gi)
  annotations: {}

## @extra hostPath Configures model cache on local disk on the nodes using hostPath -- for special cases. One should investigate and understand the security implications before using this option.
## @param hostPath.enabled Enable hostPath.
## @param hostPath.path Specify path to the local model-store. 
hostPath:
  enabled: false
  path: /model-store  # path to the local model-store

## @extra nfs Configures model cache to sit on shared direct-mounted NFS. NOTE: you cannot set mount options using direct NFS mount to pods without a node-intalled nfsmount.conf. `csi-driver-nfs`` may be better in most cases.
## @param nfs.enabled Enable nfs mount
## @param nfs.path Specify path on NFS server to mount
## @param nfs.server Specify NFS server address
## @param nfs.readOnly Set to true to use readOnly
nfs:
  enabled: false
  server: nfs-server.example.com
  path: /exports
  readOnly: false

## @section Service parameters
## @param service.type Specify service type for the deployment.
## @param service.name Override the default service name
## @param service.openaiPort Specify Open AI Port for the service.
## @param service.annotations [object] Specify additional annotations to be added to service.
## @param service.labels Specify additional labels to be added to service.
service:
  type: ClusterIP
  openaiPort: 8000
  annotations: {}
  labels: {}
  name: ""  # override the default service name 
  # below options are deprecated
  # http_port: 8000  # exposes http interface used in healthchecks to the service
  # grpc_port: 8001  # exposes the triton grpc interface
  # metrics_port: 8002  # expose metrics through the main service
  # openai_port: 8005
  # nemo_port: 8006

